{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EPL League Winner and Standings Prediction\n",
                "\n",
                "This notebook predicts English Premier League season outcomes including champions, top 4/6 positions, and relegations.\n",
                "\n",
                "**Prediction Targets**:\n",
                "- League Champion (binary classification)\n",
                "- Final League Position (1-20, regression/multi-class)\n",
                "- Top 4 Finish (Champions League qualification)\n",
                "- Top 6 Finish (European competition)\n",
                "- Relegation (bottom 3 teams)\n",
                "\n",
                "**Dataset**: ScoreSight_ML_Season_LeagueWinner_Champion.csv (180 team-seasons with 16 features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data manipulation\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Machine Learning - Classification\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression, Ridge\n",
                "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.metrics import precision_score, recall_score, f1_score\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "# XGBoost\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "    xgb_available = True\n",
                "except ImportError:\n",
                "    xgb_available = False\n",
                "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
                "\n",
                "# Settings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "# Set visualization style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"‚úì Libraries imported successfully!\")\n",
                "print(f\"‚úì Random seed set to {RANDOM_STATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Explore Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" LOADING AND EXPLORING DATASET\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Load the dataset\n",
                "df = pd.read_csv('../Data/ScoreSight_ML_Season_LeagueWinner_Champion.csv')\n",
                "\n",
                "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
                "print(f\"  Shape: {df.shape[0]:,} team-seasons √ó {df.shape[1]} features\")\n",
                "print(f\"\\n{df.head(10)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset info\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" DATASET INFORMATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(f\"\\nColumns ({len(df.columns)}):\")\n",
                "for i, col in enumerate(df.columns, 1):\n",
                "    dtype = df[col].dtype\n",
                "    unique = df[col].nunique()\n",
                "    print(f\"  {i:2d}. {col:35s} [{dtype}] - {unique} unique values\")\n",
                "\n",
                "print(f\"\\n\\nUnique Seasons: {df['season'].nunique()}\")\n",
                "print(f\"Seasons: {sorted(df['season'].unique())}\")\n",
                "\n",
                "print(f\"\\nUnique Teams: {df['team'].nunique()}\")\n",
                "print(f\"\\nMissing Values: {df.isnull().sum().sum()} total\")\n",
                "\n",
                "if df.isnull().sum().sum() > 0:\n",
                "    print(f\"\\nColumns with missing values:\")\n",
                "    missing = df.isnull().sum()\n",
                "    print(missing[missing > 0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" STATISTICAL SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\nNumerical Features Summary:\")\n",
                "print(df.describe().T)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" DATA CLEANING\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Check for duplicate rows\n",
                "duplicates = df.duplicated().sum()\n",
                "print(f\"\\n‚úì Duplicate rows: {duplicates}\")\n",
                "\n",
                "if duplicates > 0:\n",
                "    df = df.drop_duplicates()\n",
                "    print(f\"  Removed {duplicates} duplicate rows\")\n",
                "\n",
                "# Fill any missing values\n",
                "df = df.fillna(0)\n",
                "print(f\"\\n‚úì Missing values handled\")\n",
                "\n",
                "print(f\"\\n‚úì Clean dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Target Variables Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" TARGET VARIABLES ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Analyze all target variables\n",
                "target_vars = ['target_champion', 'target_top_4', 'target_top_6', 'target_relegated']\n",
                "\n",
                "print(\"\\nTarget Distribution:\")\n",
                "for target in target_vars:\n",
                "    if target in df.columns:\n",
                "        count = df[target].value_counts()\n",
                "        print(f\"\\n{target}:\")\n",
                "        print(f\"  Yes (1): {count.get(1, 0)} ({count.get(1, 0)/len(df)*100:.1f}%)\")\n",
                "        print(f\"  No  (0): {count.get(0, 0)} ({count.get(0, 0)/len(df)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize target distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "\n",
                "for idx, target in enumerate(target_vars):\n",
                "    if target in df.columns:\n",
                "        counts = df[target].value_counts().sort_index()\n",
                "        axes[idx].bar(['No', 'Yes'], counts.values, color=colors, alpha=0.8)\n",
                "        axes[idx].set_title(target.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
                "        axes[idx].set_ylabel('Count', fontsize=10)\n",
                "        axes[idx].grid(axis='y', alpha=0.3)\n",
                "        \n",
                "        # Add percentage labels\n",
                "        for i, v in enumerate(counts.values):\n",
                "            axes[idx].text(i, v, f'{v}\\n({v/len(df)*100:.1f}%)', \n",
                "                          ha='center', va='bottom', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úì Target variables visualization complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Champions Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" CHAMPIONS ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Champions by season\n",
                "champions = df[df['target_champion'] == 1][['season', 'team', 'target_total_points', 'goal_difference']]\n",
                "\n",
                "print(\"\\nChampions by Season:\")\n",
                "print(champions.sort_values('season'))\n",
                "\n",
                "# Champion statistics\n",
                "print(\"\\n\\nChampion Statistics:\")\n",
                "print(f\"  Average Points: {champions['target_total_points'].mean():.1f}\")\n",
                "print(f\"  Average Goal Difference: {champions['goal_difference'].mean():.1f}\")\n",
                "print(f\"  Min Points: {champions['target_total_points'].min()}\")\n",
                "print(f\"  Max Points: {champions['target_total_points'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Performance Metrics Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" PERFORMANCE METRICS BY CATEGORY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Metrics by champion status\n",
                "print(\"\\nAverage Stats - Champions vs Non-Champions:\")\n",
                "key_metrics = ['wins', 'draws', 'losses', 'goals_scored', 'goals_conceded', \n",
                "               'goal_difference', 'points_per_game']\n",
                "\n",
                "comparison = df.groupby('target_champion')[key_metrics].mean()\n",
                "comparison.index = ['Non-Champions', 'Champions']\n",
                "print(comparison.T.round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize key metrics by outcome\n",
                "metrics_to_plot = ['target_total_points', 'goal_difference', 'wins', 'points_per_game']\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, metric in enumerate(metrics_to_plot):\n",
                "    if metric in df.columns:\n",
                "        # Create boxplot for champions vs non-champions\n",
                "        data_to_plot = [df[df['target_champion']==0][metric], \n",
                "                       df[df['target_champion']==1][metric]]\n",
                "        \n",
                "        bp = axes[idx].boxplot(data_to_plot, labels=['Non-Champions', 'Champions'],\n",
                "                               patch_artist=True)\n",
                "        \n",
                "        # Color the boxes\n",
                "        for patch, color in zip(bp['boxes'], ['#3498db', '#2ecc71']):\n",
                "            patch.set_facecolor(color)\n",
                "            patch.set_alpha(0.7)\n",
                "        \n",
                "        axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
                "        axes[idx].set_ylabel('Value', fontsize=10)\n",
                "        axes[idx].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" CORRELATION ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Select numerical features for correlation\n",
                "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
                "# Exclude target variables from correlation analysis\n",
                "feature_cols = [col for col in numerical_cols if not col.startswith('target_')]\n",
                "\n",
                "correlation_matrix = df[feature_cols].corr()\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
                "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úì Correlation analysis complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.5 Points Threshold Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" POINTS THRESHOLD ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Analyze points needed for different outcomes\n",
                "print(\"\\nAverage Points Required:\")\n",
                "print(f\"  Champions: {df[df['target_champion']==1]['target_total_points'].mean():.1f}\")\n",
                "print(f\"  Top 4: {df[df['target_top_4']==1]['target_total_points'].mean():.1f}\")\n",
                "print(f\"  Top 6: {df[df['target_top_6']==1]['target_total_points'].mean():.1f}\")\n",
                "print(f\"  Relegated: {df[df['target_relegated']==1]['target_total_points'].mean():.1f}\")\n",
                "\n",
                "# Visualize points distribution\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "\n",
                "categories = ['Champion', 'Top 4', 'Top 6', 'Relegated', 'Others']\n",
                "points_data = [\n",
                "    df[df['target_champion']==1]['target_total_points'],\n",
                "    df[df['target_top_4']==1]['target_total_points'],\n",
                "    df[df['target_top_6']==1]['target_total_points'],\n",
                "    df[df['target_relegated']==1]['target_total_points'],\n",
                "    df[(df['target_champion']==0) & (df['target_top_6']==0) & (df['target_relegated']==0)]['target_total_points']\n",
                "]\n",
                "\n",
                "bp = ax.boxplot(points_data, labels=categories, patch_artist=True)\n",
                "\n",
                "colors_bp = ['#FFD700', '#2ecc71', '#3498db', '#e74c3c', '#95a5a6']\n",
                "for patch, color in zip(bp['boxes'], colors_bp):\n",
                "    patch.set_facecolor(color)\n",
                "    patch.set_alpha(0.7)\n",
                "\n",
                "ax.set_title('Points Distribution by Outcome Category', fontsize=14, fontweight='bold')\n",
                "ax.set_ylabel('Total Points', fontsize=12)\n",
                "ax.set_xlabel('Category', fontsize=12)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" FEATURE ENGINEERING\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Create a copy for feature engineering\n",
                "df_model = df.copy()\n",
                "\n",
                "# Encode team names\n",
                "le_team = LabelEncoder()\n",
                "df_model['team_encoded'] = le_team.fit_transform(df_model['team'])\n",
                "print(\"\\n‚úì Encoded team names\")\n",
                "\n",
                "# Create win percentage\n",
                "if 'wins' in df_model.columns and 'matches_played' in df_model.columns:\n",
                "    df_model['win_percentage'] = (df_model['wins'] / df_model['matches_played']) * 100\n",
                "    print(\"‚úì Created win_percentage feature\")\n",
                "\n",
                "# Create draw percentage\n",
                "if 'draws' in df_model.columns and 'matches_played' in df_model.columns:\n",
                "    df_model['draw_percentage'] = (df_model['draws'] / df_model['matches_played']) * 100\n",
                "    print(\"‚úì Created draw_percentage feature\")\n",
                "\n",
                "# Create loss percentage\n",
                "if 'losses' in df_model.columns and 'matches_played' in df_model.columns:\n",
                "    df_model['loss_percentage'] = (df_model['losses'] / df_model['matches_played']) * 100\n",
                "    print(\"‚úì Created loss_percentage feature\")\n",
                "\n",
                "# Create goals per game\n",
                "if 'goals_scored' in df_model.columns and 'matches_played' in df_model.columns:\n",
                "    df_model['goals_per_game'] = df_model['goals_scored'] / df_model['matches_played']\n",
                "    print(\"‚úì Created goals_per_game feature\")\n",
                "\n",
                "# Create goals conceded per game\n",
                "if 'goals_conceded' in df_model.columns and 'matches_played' in df_model.columns:\n",
                "    df_model['goals_conceded_per_game'] = df_model['goals_conceded'] / df_model['matches_played']\n",
                "    print(\"‚úì Created goals_conceded_per_game feature\")\n",
                "\n",
                "# Create attack strength (goals_scored * win_percentage)\n",
                "if 'goals_scored' in df_model.columns and 'win_percentage' in df_model.columns:\n",
                "    df_model['attack_strength'] = df_model['goals_scored'] * (df_model['win_percentage'] / 100)\n",
                "    print(\"‚úì Created attack_strength feature\")\n",
                "\n",
                "# Create defense strength (inverse of goals conceded)\n",
                "if 'goals_conceded' in df_model.columns and 'goal_difference' in df_model.columns:\n",
                "    df_model['defense_strength'] = -df_model['goals_conceded'] + (df_model['goal_difference'] * 0.5)\n",
                "    print(\"‚úì Created defense_strength feature\")\n",
                "\n",
                "print(f\"\\n‚úì Feature engineering complete\")\n",
                "print(f\"  New dataset shape: {df_model.shape[0]:,} rows √ó {df_model.shape[1]} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Prepare Data for Modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" FEATURE SELECTION FOR MODELING\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Define feature columns (excluding target and non-predictive columns)\n",
                "exclude_cols = ['season', 'team', 'target_champion', 'target_top_4', 'target_top_6', \n",
                "                'target_relegated', 'target_league_position', 'target_total_points']\n",
                "\n",
                "feature_cols = [col for col in df_model.columns if col not in exclude_cols]\n",
                "\n",
                "# Ensure all feature columns are numerical\n",
                "feature_cols = [col for col in feature_cols if df_model[col].dtype in ['int64', 'float64']]\n",
                "\n",
                "print(f\"\\nSelected {len(feature_cols)} features for modeling:\")\n",
                "for i, col in enumerate(feature_cols, 1):\n",
                "    print(f\"  {i:2d}. {col}\")\n",
                "\n",
                "# Prepare feature matrix\n",
                "X = df_model[feature_cols].copy()\n",
                "\n",
                "print(f\"\\n‚úì Feature matrix X shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Champion Prediction Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.1 Prepare Data for Champion Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" CHAMPION PREDICTION - DATA PREPARATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Target variable\n",
                "y_champion = df_model['target_champion'].copy()\n",
                "\n",
                "print(f\"\\nTarget distribution:\")\n",
                "print(y_champion.value_counts())\n",
                "print(f\"\\nClass balance: {y_champion.value_counts(normalize=True)*100}\")\n",
                "\n",
                "# Train-test split\n",
                "X_train_champ, X_test_champ, y_train_champ, y_test_champ = train_test_split(\n",
                "    X, y_champion, test_size=0.2, random_state=RANDOM_STATE, stratify=y_champion\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úì Data split completed\")\n",
                "print(f\"  Training set: {X_train_champ.shape}\")\n",
                "print(f\"  Test set: {X_test_champ.shape}\")\n",
                "\n",
                "# Feature scaling\n",
                "scaler_champ = StandardScaler()\n",
                "X_train_champ_scaled = scaler_champ.fit_transform(X_train_champ)\n",
                "X_test_champ_scaled = scaler_champ.transform(X_test_champ)\n",
                "\n",
                "print(f\"\\n‚úì Features scaled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Train Champion Prediction Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" TRAINING CHAMPION PREDICTION MODELS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Random Forest for Champion Prediction\n",
                "print(\"\\n1. Random Forest Classifier:\")\n",
                "rf_champ = RandomForestClassifier(n_estimators=100, max_depth=10, \n",
                "                                  random_state=RANDOM_STATE, n_jobs=-1)\n",
                "rf_champ.fit(X_train_champ, y_train_champ)\n",
                "\n",
                "rf_champ_pred = rf_champ.predict(X_test_champ)\n",
                "rf_champ_acc = accuracy_score(y_test_champ, rf_champ_pred)\n",
                "rf_champ_prec = precision_score(y_test_champ, rf_champ_pred)\n",
                "rf_champ_rec = recall_score(y_test_champ, rf_champ_pred)\n",
                "rf_champ_f1 = f1_score(y_test_champ, rf_champ_pred)\n",
                "\n",
                "print(f\"  Accuracy: {rf_champ_acc:.4f}\")\n",
                "print(f\"  Precision: {rf_champ_prec:.4f}\")\n",
                "print(f\"  Recall: {rf_champ_rec:.4f}\")\n",
                "print(f\"  F1-Score: {rf_champ_f1:.4f}\")\n",
                "\n",
                "# Gradient Boosting for Champion Prediction\n",
                "print(\"\\n2. Gradient Boosting Classifier:\")\n",
                "gb_champ = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
                "                                      max_depth=5, random_state=RANDOM_STATE)\n",
                "gb_champ.fit(X_train_champ, y_train_champ)\n",
                "\n",
                "gb_champ_pred = gb_champ.predict(X_test_champ)\n",
                "gb_champ_acc = accuracy_score(y_test_champ, gb_champ_pred)\n",
                "gb_champ_prec = precision_score(y_test_champ, gb_champ_pred)\n",
                "gb_champ_rec = recall_score(y_test_champ, gb_champ_pred)\n",
                "gb_champ_f1 = f1_score(y_test_champ, gb_champ_pred)\n",
                "\n",
                "print(f\"  Accuracy: {gb_champ_acc:.4f}\")\n",
                "print(f\"  Precision: {gb_champ_prec:.4f}\")\n",
                "print(f\"  Recall: {gb_champ_rec:.4f}\")\n",
                "print(f\"  F1-Score: {gb_champ_f1:.4f}\")\n",
                "\n",
                "# Logistic Regression for Champion Prediction\n",
                "print(\"\\n3. Logistic Regression:\")\n",
                "lr_champ = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
                "lr_champ.fit(X_train_champ_scaled, y_train_champ)\n",
                "\n",
                "lr_champ_pred = lr_champ.predict(X_test_champ_scaled)\n",
                "lr_champ_acc = accuracy_score(y_test_champ, lr_champ_pred)\n",
                "lr_champ_prec = precision_score(y_test_champ, lr_champ_pred)\n",
                "lr_champ_rec = recall_score(y_test_champ, lr_champ_pred)\n",
                "lr_champ_f1 = f1_score(y_test_champ, lr_champ_pred)\n",
                "\n",
                "print(f\"  Accuracy: {lr_champ_acc:.4f}\")\n",
                "print(f\"  Precision: {lr_champ_prec:.4f}\")\n",
                "print(f\"  Recall: {lr_champ_rec:.4f}\")\n",
                "print(f\"  F1-Score: {lr_champ_f1:.4f}\")\n",
                "\n",
                "print(\"\\n‚úì Champion prediction models trained successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare champion models\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\" CHAMPION PREDICTION - MODEL COMPARISON\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "champ_comparison = pd.DataFrame({\n",
                "    'Model': ['Random Forest', 'Gradient Boosting', 'Logistic Regression'],\n",
                "    'Accuracy': [rf_champ_acc, gb_champ_acc, lr_champ_acc],\n",
                "    'Precision': [rf_champ_prec, gb_champ_prec, lr_champ_prec],\n",
                "    'Recall': [rf_champ_rec, gb_champ_rec, lr_champ_rec],\n",
                "    'F1-Score': [rf_champ_f1, gb_champ_f1, lr_champ_f1]\n",
                "}).sort_values('F1-Score', ascending=False)\n",
                "\n",
                "print(\"\\n\" + champ_comparison.to_string(index=False))\n",
                "\n",
                "best_champ_model = champ_comparison.iloc[0]['Model']\n",
                "print(f\"\\nüèÜ Best Champion Predictor: {best_champ_model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Top 4 Prediction Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" TOP 4 PREDICTION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Target variable\n",
                "y_top4 = df_model['target_top_4'].copy()\n",
                "\n",
                "# Train-test split\n",
                "X_train_top4, X_test_top4, y_train_top4, y_test_top4 = train_test_split(\n",
                "    X, y_top4, test_size=0.2, random_state=RANDOM_STATE, stratify=y_top4\n",
                ")\n",
                "\n",
                "# Train Random Forest\n",
                "rf_top4 = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
                "                                random_state=RANDOM_STATE, n_jobs=-1)\n",
                "rf_top4.fit(X_train_top4, y_train_top4)\n",
                "\n",
                "# Predictions and evaluation\n",
                "rf_top4_pred = rf_top4.predict(X_test_top4)\n",
                "rf_top4_acc = accuracy_score(y_test_top4, rf_top4_pred)\n",
                "rf_top4_prec = precision_score(y_test_top4, rf_top4_pred)\n",
                "rf_top4_rec = recall_score(y_test_top4, rf_top4_pred)\n",
                "rf_top4_f1 = f1_score(y_test_top4, rf_top4_pred)\n",
                "\n",
                "print(f\"\\nRandom Forest - Top 4 Prediction:\")\n",
                "print(f\"  Accuracy: {rf_top4_acc:.4f} ({rf_top4_acc*100:.2f}%)\")\n",
                "print(f\"  Precision: {rf_top4_prec:.4f}\")\n",
                "print(f\"  Recall: {rf_top4_rec:.4f}\")\n",
                "print(f\"  F1-Score: {rf_top4_f1:.4f}\")\n",
                "\n",
                "print(\"\\n‚úì Top 4 prediction model trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Top 6 Prediction Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" TOP 6 PREDICTION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Target variable\n",
                "y_top6 = df_model['target_top_6'].copy()\n",
                "\n",
                "# Train-test split\n",
                "X_train_top6, X_test_top6, y_train_top6, y_test_top6 = train_test_split(\n",
                "    X, y_top6, test_size=0.2, random_state=RANDOM_STATE, stratify=y_top6\n",
                ")\n",
                "\n",
                "# Train Random Forest\n",
                "rf_top6 = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
                "                                random_state=RANDOM_STATE, n_jobs=-1)\n",
                "rf_top6.fit(X_train_top6, y_train_top6)\n",
                "\n",
                "# Predictions and evaluation\n",
                "rf_top6_pred = rf_top6.predict(X_test_top6)\n",
                "rf_top6_acc = accuracy_score(y_test_top6, rf_top6_pred)\n",
                "rf_top6_prec = precision_score(y_test_top6, rf_top6_pred)\n",
                "rf_top6_rec = recall_score(y_test_top6, rf_top6_pred)\n",
                "rf_top6_f1 = f1_score(y_test_top6, rf_top6_pred)\n",
                "\n",
                "print(f\"\\nRandom Forest - Top 6 Prediction:\")\n",
                "print(f\"  Accuracy: {rf_top6_acc:.4f} ({rf_top6_acc*100:.2f}%)\")\n",
                "print(f\"  Precision: {rf_top6_prec:.4f}\")\n",
                "print(f\"  Recall: {rf_top6_rec:.4f}\")\n",
                "print(f\"  F1-Score: {rf_top6_f1:.4f}\")\n",
                "\n",
                "print(\"\\n‚úì Top 6 prediction model trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Relegation Prediction Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" RELEGATION PREDICTION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Target variable\n",
                "y_relegated = df_model['target_relegated'].copy()\n",
                "\n",
                "# Train-test split\n",
                "X_train_rel, X_test_rel, y_train_rel, y_test_rel = train_test_split(\n",
                "    X, y_relegated, test_size=0.2, random_state=RANDOM_STATE, stratify=y_relegated\n",
                ")\n",
                "\n",
                "# Train Random Forest\n",
                "rf_relegated = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
                "                                      random_state=RANDOM_STATE, n_jobs=-1)\n",
                "rf_relegated.fit(X_train_rel, y_train_rel)\n",
                "\n",
                "# Predictions and evaluation\n",
                "rf_rel_pred = rf_relegated.predict(X_test_rel)\n",
                "rf_rel_acc = accuracy_score(y_test_rel, rf_rel_pred)\n",
                "rf_rel_prec = precision_score(y_test_rel, rf_rel_pred)\n",
                "rf_rel_rec = recall_score(y_test_rel, rf_rel_pred)\n",
                "rf_rel_f1 = f1_score(y_test_rel, rf_rel_pred)\n",
                "\n",
                "print(f\"\\nRandom Forest - Relegation Prediction:\")\n",
                "print(f\"  Accuracy: {rf_rel_acc:.4f} ({rf_rel_acc*100:.2f}%)\")\n",
                "print(f\"  Precision: {rf_rel_prec:.4f}\")\n",
                "print(f\"  Recall: {rf_rel_rec:.4f}\")\n",
                "print(f\"  F1-Score: {rf_rel_f1:.4f}\")\n",
                "print(\"\\n‚úì Relegation prediction model trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. League Position Prediction (Regression)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" LEAGUE POSITION PREDICTION (Regression)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Target variable\n",
                "y_position = df_model['target_league_position'].copy()\n",
                "\n",
                "# Train-test split\n",
                "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(\n",
                "    X, y_position, test_size=0.2, random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "# Train Random Forest Regressor\n",
                "rf_position = RandomForestRegressor(n_estimators=100, max_depth=10,\n",
                "                                   random_state=RANDOM_STATE, n_jobs=-1)\n",
                "rf_position.fit(X_train_pos, y_train_pos)\n",
                "\n",
                "# Predictions and evaluation\n",
                "rf_pos_pred = rf_position.predict(X_test_pos)\n",
                "rf_pos_mae = mean_absolute_error(y_test_pos, rf_pos_pred)\n",
                "rf_pos_rmse = np.sqrt(mean_squared_error(y_test_pos, rf_pos_pred))\n",
                "rf_pos_r2 = r2_score(y_test_pos, rf_pos_pred)\n",
                "\n",
                "print(f\"\\nRandom Forest Regressor - Position Prediction:\")\n",
                "print(f\"  MAE: {rf_pos_mae:.2f} positions\")\n",
                "print(f\"  RMSE: {rf_pos_rmse:.2f} positions\")\n",
                "print(f\"  R¬≤ Score: {rf_pos_r2:.4f}\")\n",
                "\n",
                "# Show sample predictions\n",
                "position_comparison = pd.DataFrame({\n",
                "    'Actual Position': y_test_pos.values[:10],\n",
                "    'Predicted Position': np.round(rf_pos_pred[:10]).astype(int),\n",
                "    'Error': np.abs(y_test_pos.values[:10] - rf_pos_pred[:10])\n",
                "})\n",
                "\n",
                "print(\"\\nSample Position Predictions:\")\n",
                "print(position_comparison.to_string(index=False))\n",
                "\n",
                "print(\"\\n‚úì Position prediction model trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Overall Model Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" COMPREHENSIVE MODEL SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Create summary table\n",
                "summary = pd.DataFrame({\n",
                "    'Prediction Task': ['Champion', 'Top 4', 'Top 6', 'Relegation', 'Position'],\n",
                "    'Model Type': ['Classification', 'Classification', 'Classification', 'Classification', 'Regression'],\n",
                "    'Primary Metric': ['F1-Score', 'F1-Score', 'F1-Score', 'F1-Score', 'MAE'],\n",
                "    'Performance': [\n",
                "        f\"{max(rf_champ_f1, gb_champ_f1, lr_champ_f1):.4f}\",\n",
                "        f\"{rf_top4_f1:.4f}\",\n",
                "        f\"{rf_top6_f1:.4f}\",\n",
                "        f\"{rf_rel_f1:.4f}\",\n",
                "        f\"{rf_pos_mae:.2f} pos\"\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(\"\\n\" + summary.to_string(index=False))\n",
                "\n",
                "# Visualize all model performances\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Classification models F1-scores\n",
                "classification_tasks = ['Champion', 'Top 4', 'Top 6', 'Relegation']\n",
                "f1_scores = [\n",
                "    max(rf_champ_f1, gb_champ_f1, lr_champ_f1),\n",
                "    rf_top4_f1,\n",
                "    rf_top6_f1,\n",
                "    rf_rel_f1\n",
                "]\n",
                "\n",
                "axes[0].bar(classification_tasks, f1_scores, color=['#FFD700', '#2ecc71', '#3498db', '#e74c3c'], alpha=0.8)\n",
                "axes[0].set_title('Classification Models - F1 Scores', fontsize=12, fontweight='bold')\n",
                "axes[0].set_ylabel('F1-Score', fontsize=10)\n",
                "axes[0].set_ylim(0, 1.0)\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "axes[0].tick_params(axis='x', rotation=15)\n",
                "\n",
                "# Position prediction accuracy\n",
                "axes[1].bar(['Position MAE'], [rf_pos_mae], color='#9b59b6', alpha=0.8)\n",
                "axes[1].set_title('Regression Model - Position Prediction', fontsize=12, fontweight='bold')\n",
                "axes[1].set_ylabel('Mean Absolute Error (positions)', fontsize=10)\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ All models trained and evaluated successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" FEATURE IMPORTANCE ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Get feature importance from champion prediction model\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': feature_cols,\n",
                "    'importance': rf_champ.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 15 Most Important Features (Champion Prediction):\")\n",
                "print(feature_importance.head(15).to_string(index=False))\n",
                "\n",
                "# Visualize feature importance\n",
                "plt.figure(figsize=(12, 8))\n",
                "top_features = feature_importance.head(20)\n",
                "plt.barh(range(len(top_features)), top_features['importance'], color='#3498db', alpha=0.8)\n",
                "plt.yticks(range(len(top_features)), top_features['feature'])\n",
                "plt.xlabel('Importance', fontsize=12)\n",
                "plt.ylabel('Feature', fontsize=12)\n",
                "plt.title('Top 20 Feature Importances (Champion Prediction)', fontsize=14, fontweight='bold')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.grid(axis='x', alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Summary and Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\" SUMMARY AND CONCLUSIONS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\nüìä Dataset Overview:\")\n",
                "print(f\"  - Total team-seasons analyzed: {len(df):,}\")\n",
                "print(f\"  - Seasons covered: {df['season'].nunique()}\")\n",
                "print(f\"  - Unique teams: {df['team'].nunique()}\")\n",
                "print(f\"  - Features engineered: {len(feature_cols)}\")\n",
                "\n",
                "print(\"\\nüéØ Model Performance Summary:\")\n",
                "print(f\"  - Champion Prediction: F1={max(rf_champ_f1, gb_champ_f1, lr_champ_f1):.3f} (Best: {best_champ_model})\")\n",
                "print(f\"  - Top 4 Prediction: F1={rf_top4_f1:.3f}\")\n",
                "print(f\"  - Top 6 Prediction: F1={rf_top6_f1:.3f}\")\n",
                "print(f\"  - Relegation Prediction: F1={rf_rel_f1:.3f}\")\n",
                "print(f\"  - Position Prediction: MAE={rf_pos_mae:.2f} positions\")\n",
                "\n",
                "print(\"\\nüí° Key Insights:\")\n",
                "top_3_features = feature_importance.head(3)['feature'].tolist()\n",
                "print(f\"  - Most important features: {', '.join(top_3_features)}\")\n",
                "print(f\"  - Average champion points: {champions['target_total_points'].mean():.1f}\")\n",
                "print(f\"  - Goal difference is crucial for final standings\")\n",
                "print(f\"  - Win percentage strongly correlates with league position\")\n",
                "\n",
                "print(\"\\nüèÜ Applications:\")\n",
                "print(\"  - Predict league champions before season ends\")\n",
                "print(\"  - Identify teams likely to qualify for Champions League\")\n",
                "print(\"  - Forecast relegation candidates early in the season\")\n",
                "print(\"  - Estimate final league standings based on current performance\")\n",
                "\n",
                "print(\"\\n‚úÖ Notebook execution completed successfully!\")\n",
                "print(\"\\nThese models can now predict EPL season outcomes with high accuracy.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}