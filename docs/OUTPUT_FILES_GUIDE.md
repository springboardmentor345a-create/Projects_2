# Feature Engineering v3.0 - Output Files Guide

**Created:** November 5, 2025  
**Files Generated By:** `notebooks/07_Feature_Engineering_v3_Advanced.ipynb`

---

## üìã Output Files Overview

### 1. Main Dataset with New Features
**File:** `data/engineered/data_engineered_match_v3.csv`

**What it contains:**
- Original match data (date, teams, scores, etc.)
- All original engineered features from v2.0
- **52 NEW features from v3.0 (Tiers 1-3)**

**Size:** ~6,800+ rows √ó 92+ columns

**How to use:**
```python
import pandas as pd

# Load the enhanced dataset
df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')

# Use new features in your model
new_features = [col for col in df.columns if 'percentile' in col 
                or 'strength' in col or 'efficiency' in col]
print(f"Available new features: {len(new_features)}")
```

**Expected columns:**
- Original: `date`, `hometeam`, `awayteam`, `fthg`, `ftag`, `ftr`, etc.
- Tier 1: `home_goals_percentile`, `home_scoring_cv`, `home_goals_skewness`, etc.
- Tier 2: `home_team_tier`, `rest_advantage`, `home_winning_momentum`, etc.
- Tier 3: `home_offensive_efficiency`, `home_strength_index`, `matchup_threat_score`, etc.

---

### 2. Feature Descriptions Reference
**File:** `data/engineered/feature_descriptions_v3.json`

**What it contains:**
- Name and description of all 52 new features
- One-line explanations for each feature

**Format:**
```json
{
  "home_goals_percentile": "Home team's goal scoring relative to all teams (0-100 scale)",
  "home_strength_index": "Composite home team strength index [0-1] (form, offense, defense, consistency)",
  ...
}
```

**How to use:**
```python
import json

# Load feature descriptions
with open('data/engineered/feature_descriptions_v3.json', 'r') as f:
    descriptions = json.load(f)

# Look up a feature
print(descriptions['home_strength_index'])

# Iterate through all features
for feature_name, description in descriptions.items():
    print(f"{feature_name}: {description}")
```

**Use Case:**
- Quick reference while building models
- Feature interpretation for stakeholders
- Documentation generation
- Model explainability

---

### 3. Visualization: Feature Distributions
**File:** `visualizations/tier1_3_feature_distributions.png`

**What it shows:**
6-panel histogram visualization of key engineered features:
1. `home_goals_percentile` - Goal scoring distribution
2. `home_scoring_cv` - Consistency variation
3. `home_strength_index` - Strength composite
4. `home_offensive_efficiency` - Offensive metric
5. `home_team_tier` - Team tier distribution
6. `home_quality_score` - Team quality

**How to interpret:**
- **Normal distribution** = Well-distributed feature
- **Right skew** = Some extreme high values
- **Left skew** = Some extreme low values
- **Bimodal** = Two distinct clusters (possibly elite vs struggling)

**Example interpretation:**
```
home_goals_percentile histogram:
- If bell-shaped ‚Üí Most teams score ~50th percentile
- If uniform ‚Üí No difference between teams (bad feature)
- If bimodal ‚Üí Clear elite vs average separation (good feature)
```

---

## üìä How to Use Output Files

### Scenario 1: Train a Predictive Model

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# 1. Load enhanced dataset
df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')

# 2. Select features (including new ones)
feature_cols = [col for col in df.columns 
                if col not in ['date', 'hometeam', 'awayteam', 'fthg', 'ftag', 'ftr']]

X = df[feature_cols].fillna(df[feature_cols].mean())
y = df['fthg']  # Target: Full-Time Home Goals

# 3. Train model
model = RandomForestRegressor(n_estimators=100)
model.fit(X, y)

# 4. Analyze feature importance
importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(importance.head(20))
```

---

### Scenario 2: Feature Analysis & Selection

```python
import json

# 1. Load descriptions
with open('data/engineered/feature_descriptions_v3.json', 'r') as f:
    descriptions = json.load(f)

# 2. Load dataset
df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')

# 3. Analyze each tier
tier1_features = [f for f in descriptions.keys() 
                  if 'percentile' in f or 'skewness' in f or 'iqr' in f]
tier2_features = [f for f in descriptions.keys() 
                  if 'tier' in f or 'rest' in f or 'momentum' in f]
tier3_features = [f for f in descriptions.keys() 
                  if 'efficiency' in f or 'strength' in f or 'index' in f]

print(f"Tier 1 Statistical: {len(tier1_features)} features")
print(f"Tier 2 Market: {len(tier2_features)} features")
print(f"Tier 3 Interactions: {len(tier3_features)} features")

# 4. Check correlations with target
correlations = df[tier1_features].corrwith(df['fthg']).sort_values(ascending=False)
print("\nTier 1 Feature Correlations with Target:")
print(correlations)
```

---

### Scenario 3: Feature Interpretation for Reports

```python
import json

with open('data/engineered/feature_descriptions_v3.json', 'r') as f:
    descriptions = json.load(f)

# Create feature documentation table
report = """
# Feature Engineering v3.0 - Generated Features

## Tier 1: Statistical Features (Robustness & Consistency)
"""

tier1_features = [f for f in descriptions.keys() if 'percentile' in f or 'zscore' in f]
for feat in sorted(tier1_features):
    report += f"\n- **{feat}**: {descriptions[feat]}"

print(report)

# Can be saved to file
with open('Feature_Documentation.md', 'w') as f:
    f.write(report)
```

---

## üé® Visualization Details

### PNG File: `tier1_3_feature_distributions.png`

**Properties:**
- Resolution: 300 DPI (publication quality)
- Size: ~2000√ó1200 pixels
- Format: PNG (lossless)
- Title: "Distribution of Key Engineered Features (Tier 1-3)"

**Panels:**
```
Row 1: [home_goals_percentile] [home_scoring_cv] [home_strength_index]
Row 2: [home_offensive_efficiency] [home_team_tier] [home_quality_score]
```

**How to view:**
```python
from PIL import Image
import matplotlib.pyplot as plt

img = Image.open('visualizations/tier1_3_feature_distributions.png')
plt.figure(figsize=(16, 10))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

---

## üìà Expected File Sizes

| File | Approximate Size |
|---|---|
| `data_engineered_match_v3.csv` | 5-10 MB |
| `feature_descriptions_v3.json` | 10-20 KB |
| `tier1_3_feature_distributions.png` | 200-400 KB |
| **Total** | **5-10 MB** |

---

## ‚úÖ Data Quality Checks

### CSV File Checks
```python
import pandas as pd

df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')

# Check dimensions
print(f"Shape: {df.shape}")  # Should be (N_rows, 92+)

# Check data types
print(df.dtypes)

# Check missing values
print(df.isnull().sum())  # Most features should have <10% missing

# Check numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
print(f"Numeric features: {len(numeric_cols)}")

# Verify new features exist
tier1_check = 'home_goals_percentile' in df.columns
tier2_check = 'home_team_tier' in df.columns
tier3_check = 'home_strength_index' in df.columns
print(f"All tiers present: {tier1_check and tier2_check and tier3_check}")
```

### JSON File Validation
```python
import json

with open('data/engineered/feature_descriptions_v3.json', 'r') as f:
    descriptions = json.load(f)

print(f"Total features documented: {len(descriptions)}")
print(f"All strings: {all(isinstance(v, str) for v in descriptions.values())}")

# Should have ~52 features
assert len(descriptions) >= 50, "Feature count too low!"
```

---

## üîÑ Integration with ML Pipeline

### Step 1: Load Enhanced Data
```python
df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')
```

### Step 2: Feature Engineering Validation
```python
# Check for data leakage
assert df['fthg'].isnull().sum() == 0, "Target has NaN values!"

# Check temporal ordering
df['date'] = pd.to_datetime(df['date'])
assert (df['date'].diff() >= pd.Timedelta(0)).all() or True  # Might not be sorted
```

### Step 3: Feature Selection
```python
# Keep only new v3.0 features
v3_features = df.columns.difference(df_old.columns)  # New vs old

# Or manually select
selected = [col for col in df.columns 
            if any(keyword in col for keyword in ['percentile', 'efficiency', 'strength'])]
```

### Step 4: Model Training
```python
X = df[selected].fillna(df[selected].mean())
y = df['fthg']

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X, y)
```

---

## üêõ Troubleshooting Output Files

### Problem: CSV file not created
**Solution:** Check notebook execution completed Cell 10 without errors

### Problem: JSON file is empty or malformed
**Solution:** Ensure Cell 10 executed successfully

### Problem: PNG visualization missing
**Solution:** Run Cell 9 again, check `visualizations/` directory exists

### Problem: Wrong data format
**Solution:** Verify data type of features
```python
df['home_goals_percentile'].dtype  # Should be float64
df['home_team_tier'].dtype  # Should be int64
```

---

## üìö Related Documentation

1. **Feature Descriptions:** See `feature_descriptions_v3.json` for all details
2. **Technical Guide:** See `FEATURE_ENGINEERING_V3_GUIDE.md` for full documentation
3. **Quick Reference:** See `FEATURE_ENGINEERING_QUICK_REFERENCE.md` for quick lookup
4. **Configuration:** See `feature_engineering_config.py` for parameter options

---

## üéØ Using Outputs in Production

### For Inference/Prediction:
```python
# 1. Load trained model
model = load_trained_model()

# 2. Load new match data (with v3.0 features)
new_matches = pd.read_csv('data/engineered/data_engineered_match_v3.csv')

# 3. Make predictions
predictions = model.predict(new_matches[v3_features])

# 4. Output predictions
results = pd.DataFrame({
    'match': new_matches['hometeam'] + ' vs ' + new_matches['awayteam'],
    'predicted_goals': predictions
})
```

### For Reporting:
```python
# 1. Load descriptions
with open('data/engineered/feature_descriptions_v3.json', 'r') as f:
    descriptions = json.load(f)

# 2. Load feature importance from model
importances = model.feature_importances_

# 3. Generate report
report = f"Top 10 Features:\n"
for feat, imp in sorted(zip(v3_features, importances), 
                        key=lambda x: x[1], reverse=True)[:10]:
    report += f"\n{feat} ({imp:.4f}): {descriptions[feat]}"
```

---

## üìû Support

If outputs look unexpected:
1. Check notebook ran completely (all cells green ‚úì)
2. Verify data paths are correct
3. Check for errors in notebook execution
4. Review troubleshooting section above
5. Check documentation files

---

**Last Updated:** November 5, 2025  
**Notebook Version:** 07_Feature_Engineering_v3_Advanced.ipynb  
**Phase:** 1 of 4 Complete
