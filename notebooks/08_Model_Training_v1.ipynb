{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09efa81c",
   "metadata": {},
   "source": [
    "# ScoreSight: Model Training Pipeline v1.0\n",
    "## Multi-Algorithm ML Optimization for EPL Prediction\n",
    "\n",
    "**Author:** Prathamesh Fuke  \n",
    "**Version:** 1.0 - Comprehensive Model Training  \n",
    "**Focus:** Multi-algorithm training, hyperparameter optimization, temporal CV, performance analysis\n",
    "\n",
    "### Notebook Overview\n",
    "This notebook implements the comprehensive model training pipeline:\n",
    "- **Phase 1:** Data Loading & Feature Selection\n",
    "- **Phase 2:** Multi-Algorithm Training (Linear, Ridge, Random Forest, XGBoost, LightGBM)\n",
    "- **Phase 3:** Hyperparameter Optimization (GridSearchCV, RandomizedSearchCV)\n",
    "- **Phase 4:** Temporal Cross-Validation (Walk-Forward, Season-Aware)\n",
    "- **Phase 5:** Performance Evaluation & Comparison\n",
    "- **Phase 6:** Feature Importance Analysis\n",
    "\n",
    "### Expected Outcomes\n",
    "- Best Model: XGBoost or LightGBM with MAE < 1.0\n",
    "- Feature Importance Ranking\n",
    "- Cross-validation Performance Metrics\n",
    "- Model Comparison Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff4e57",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f3b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "  XGBoost Available: True\n",
      "  LightGBM Available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not installed\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  LightGBM not installed\")\n",
    "\n",
    "import os\n",
    "os.chdir('d:\\\\ScoreSight')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print(f\"  XGBoost Available: {XGB_AVAILABLE}\")\n",
    "print(f\"  LightGBM Available: {LGB_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cd500",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dcfe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully\n",
      "\n",
      "üìä DATASET OVERVIEW\n",
      "================================================================================\n",
      "Shape: (6840, 96)\n",
      "\n",
      "Columns: ['unnamed:_0', 'date', 'hometeam', 'awayteam', 'fthg', 'ftag', 'ftr', 'htgs', 'atgs', 'htgc']...\n",
      "\n",
      "Target Variable (fthg): Home team goals\n",
      "  Mean: 1.53\n",
      "  Std: 1.30\n",
      "  Min: 0, Max: 9\n",
      "\n",
      "Missing values: 1056\n",
      "\n",
      "Date range: 2000-01-10 00:00:00 to 2018-12-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load engineered features\n",
    "df = pd.read_csv('data/engineered/data_engineered_match_v3.csv')\n",
    "\n",
    "print(\"‚úì Data loaded successfully\")\n",
    "print(f\"\\nüìä DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns[:10])}...\")\n",
    "\n",
    "# Standardize column names (lowercase)\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "print(f\"\\nTarget Variable (fthg): Home team goals\")\n",
    "print(f\"  Mean: {df['fthg'].mean():.2f}\")\n",
    "print(f\"  Std: {df['fthg'].std():.2f}\")\n",
    "print(f\"  Min: {df['fthg'].min():.0f}, Max: {df['fthg'].max():.0f}\")\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for date column\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea675e",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff95137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Feature Selection Complete\n",
      "\n",
      "Features: 78\n",
      "Target: fthg\n",
      "\n",
      "Feature list (first 20):\n",
      "   1. unnamed__0\n",
      "   2. htgs\n",
      "   3. atgs\n",
      "   4. htgc\n",
      "   5. atgc\n",
      "   6. htp\n",
      "   7. atp\n",
      "   8. mw\n",
      "   9. htformpts\n",
      "  10. atformpts\n",
      "  11. htwinstreak3\n",
      "  12. htwinstreak5\n",
      "  13. htlossstreak3\n",
      "  14. htlossstreak5\n",
      "  15. atwinstreak3\n",
      "  16. atwinstreak5\n",
      "  17. atlossstreak3\n",
      "  18. atlossstreak5\n",
      "  19. htgd\n",
      "  20. atgd\n",
      "\n",
      "‚úì Missing values handled\n",
      "  Final shape: (6840, 78)\n"
     ]
    }
   ],
   "source": [
    "# Define target and features\n",
    "target = 'fthg'  # Full-Time Home Goals\n",
    "\n",
    "# Remove non-numeric and irrelevant columns\n",
    "exclude_cols = ['date', 'hometeam', 'awayteam', 'ftr', 'htr', 'fthg', 'ftag', 'hthg', 'htag', 'unnamed: 0']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Clean feature names for LightGBM compatibility (remove special JSON characters)\n",
    "X.columns = [col.replace(':', '_').replace('[', '(').replace(']', ')') for col in X.columns]\n",
    "feature_cols = list(X.columns)\n",
    "\n",
    "print(f\"‚úì Feature Selection Complete\")\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"\\nFeature list (first 20):\")\n",
    "for i, col in enumerate(feature_cols[:20], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.mean())\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(X.mean())\n",
    "\n",
    "print(f\"\\n‚úì Missing values handled\")\n",
    "print(f\"  Final shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad0820",
   "metadata": {},
   "source": [
    "## 4. Temporal Cross-Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1daf720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Temporal Cross-Validation Created (Walk-Forward Strategy)\n",
      "\n",
      " Fold  Train Size  Test Size Train % Test %\n",
      "    1        4104        547   60.0%   8.0%\n",
      "    2        4651        547   68.0%   8.0%\n",
      "    3        5198        547   76.0%   8.0%\n",
      "    4        5745        547   84.0%   8.0%\n",
      "    5        6292        547   92.0%   8.0%\n"
     ]
    }
   ],
   "source": [
    "class TemporalCrossValidator:\n",
    "    \"\"\"\n",
    "    Implement walk-forward cross-validation for time-series data\n",
    "    Prevents data leakage by only training on past data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, initial_train_size=0.5):\n",
    "        self.n_splits = n_splits\n",
    "        self.initial_train_size = initial_train_size\n",
    "        \n",
    "    def split(self, X, y=None):\n",
    "        \"\"\"Generate train/test indices for walk-forward validation\"\"\"\n",
    "        n_samples = len(X)\n",
    "        initial_train = int(n_samples * self.initial_train_size)\n",
    "        test_size = (n_samples - initial_train) // self.n_splits\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            train_end = initial_train + (i * test_size)\n",
    "            test_end = train_end + test_size\n",
    "            \n",
    "            train_idx = np.arange(0, train_end)\n",
    "            test_idx = np.arange(train_end, min(test_end, n_samples))\n",
    "            \n",
    "            if len(test_idx) > 0:\n",
    "                yield train_idx, test_idx\n",
    "\n",
    "# Create temporal CV splitter\n",
    "tcv = TemporalCrossValidator(n_splits=5, initial_train_size=0.6)\n",
    "\n",
    "# Verify splits\n",
    "split_info = []\n",
    "for i, (train_idx, test_idx) in enumerate(tcv.split(X, y), 1):\n",
    "    split_info.append({\n",
    "        'Fold': i,\n",
    "        'Train Size': len(train_idx),\n",
    "        'Test Size': len(test_idx),\n",
    "        'Train %': f\"{len(train_idx)/len(X)*100:.1f}%\",\n",
    "        'Test %': f\"{len(test_idx)/len(X)*100:.1f}%\"\n",
    "    })\n",
    "\n",
    "print(\"‚úì Temporal Cross-Validation Created (Walk-Forward Strategy)\")\n",
    "print(\"\\n\" + pd.DataFrame(split_info).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee67ea5",
   "metadata": {},
   "source": [
    "## 5. Multi-Algorithm Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9713057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Features scaled\n",
      "\n",
      "‚úì Models configured: 7 models\n",
      "  - Linear Regression\n",
      "  - Ridge Regression\n",
      "  - Lasso Regression\n",
      "  - Random Forest\n",
      "  - Gradient Boosting\n",
      "  - XGBoost\n",
      "  - LightGBM\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"‚úì Features scaled\")\n",
    "\n",
    "# Define models with hyperparameters\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {},\n",
    "        'use_scaled': True\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'alpha': [0.1, 1, 10, 100]},\n",
    "        'use_scaled': True\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(max_iter=5000),\n",
    "        'params': {'alpha': [0.001, 0.01, 0.1, 1]},\n",
    "        'use_scaled': True\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'params': {'max_depth': [10, 20, 30], 'min_samples_split': [5, 10]},\n",
    "        'use_scaled': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5, 7]},\n",
    "        'use_scaled': False\n",
    "    }\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    models['XGBoost'] = {\n",
    "        'model': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5, 7]},\n",
    "        'use_scaled': False\n",
    "    }\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    models['LightGBM'] = {\n",
    "        'model': lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5, 7]},\n",
    "        'use_scaled': False\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úì Models configured: {len(models)} models\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0ee1c",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa70aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ TRAINING MODELS WITH CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Linear Regression...\n",
      "  ‚úì MAE: 0.7998 (¬±0.0207)\n",
      "  ‚úì RMSE: 1.0110\n",
      "  ‚úì R¬≤: 0.4016\n",
      "\n",
      "Ridge Regression...\n",
      "  ‚úì MAE: 0.7995 (¬±0.0203)\n",
      "  ‚úì RMSE: 1.0109\n",
      "  ‚úì R¬≤: 0.4017\n",
      "\n",
      "Lasso Regression...\n",
      "  ‚úì MAE: 1.0600 (¬±0.0144)\n",
      "  ‚úì RMSE: 1.3085\n",
      "  ‚úì R¬≤: -0.0015\n",
      "\n",
      "Random Forest...\n",
      "  ‚úì MAE: 0.8252 (¬±0.0335)\n",
      "  ‚úì RMSE: 1.0503\n",
      "  ‚úì R¬≤: 0.3534\n",
      "  ‚úì MAE: 0.8252 (¬±0.0335)\n",
      "  ‚úì RMSE: 1.0503\n",
      "  ‚úì R¬≤: 0.3534\n",
      "\n",
      "Gradient Boosting...\n",
      "\n",
      "Gradient Boosting...\n",
      "  ‚úì MAE: 0.8222 (¬±0.0521)\n",
      "  ‚úì RMSE: 1.0496\n",
      "  ‚úì R¬≤: 0.3541\n",
      "  ‚úì MAE: 0.8222 (¬±0.0521)\n",
      "  ‚úì RMSE: 1.0496\n",
      "  ‚úì R¬≤: 0.3541\n",
      "\n",
      "XGBoost...\n",
      "\n",
      "XGBoost...\n",
      "  ‚úì MAE: 0.8894 (¬±0.0174)\n",
      "  ‚úì RMSE: 1.1382\n",
      "  ‚úì R¬≤: 0.2418\n",
      "  ‚úì MAE: 0.8894 (¬±0.0174)\n",
      "  ‚úì RMSE: 1.1382\n",
      "  ‚úì R¬≤: 0.2418\n",
      "\n",
      "LightGBM...\n",
      "\n",
      "LightGBM...\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Do not support special JSON characters in feature name.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLightGBMError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     26\u001b[39m model = model_config[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].\u001b[34m__class__\u001b[39m(**model_config[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].get_params())\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     30\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\engine.py:297\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     booster = \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[32m    299\u001b[39m         booster.set_train_data_name(train_data_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\basic.py:3656\u001b[39m, in \u001b[36mBooster.__init__\u001b[39m\u001b[34m(self, params, train_set, model_file, model_str)\u001b[39m\n\u001b[32m   3649\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_network(\n\u001b[32m   3650\u001b[39m         machines=machines,\n\u001b[32m   3651\u001b[39m         local_listen_port=params[\u001b[33m\"\u001b[39m\u001b[33mlocal_listen_port\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3652\u001b[39m         listen_time_out=params.get(\u001b[33m\"\u001b[39m\u001b[33mtime_out\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m120\u001b[39m),\n\u001b[32m   3653\u001b[39m         num_machines=params[\u001b[33m\"\u001b[39m\u001b[33mnum_machines\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3654\u001b[39m     )\n\u001b[32m   3655\u001b[39m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3656\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3657\u001b[39m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[32m   3658\u001b[39m params.update(train_set.get_params())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\basic.py:2590\u001b[39m, in \u001b[36mDataset.construct\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2585\u001b[39m             \u001b[38;5;28mself\u001b[39m._set_init_score_by_predictor(\n\u001b[32m   2586\u001b[39m                 predictor=\u001b[38;5;28mself\u001b[39m._predictor, data=\u001b[38;5;28mself\u001b[39m.data, used_indices=used_indices\n\u001b[32m   2587\u001b[39m             )\n\u001b[32m   2588\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2590\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.free_raw_data:\n\u001b[32m   2604\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\basic.py:2227\u001b[39m, in \u001b[36mDataset._lazy_init\u001b[39m\u001b[34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[39m\n\u001b[32m   2225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrong predictor type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(predictor).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2226\u001b[39m \u001b[38;5;66;03m# set feature names\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_feature_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\basic.py:3046\u001b[39m, in \u001b[36mDataset.set_feature_name\u001b[39m\u001b[34m(self, feature_name)\u001b[39m\n\u001b[32m   3042\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3043\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength of feature_name(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(feature_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) and num_feature(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_feature()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3044\u001b[39m         )\n\u001b[32m   3045\u001b[39m     c_feature_name = [_c_str(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m feature_name]\n\u001b[32m-> \u001b[39m\u001b[32m3046\u001b[39m     \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_DatasetSetFeatureNames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_c_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_char_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_feature_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\lightgbm\\basic.py:313\u001b[39m, in \u001b[36m_safe_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    The return value from C API calls.\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB.LGBM_GetLastError().decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mLightGBMError\u001b[39m: Do not support special JSON characters in feature name."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ TRAINING MODELS WITH CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n{model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train_data = X_scaled if model_config['use_scaled'] else X\n",
    "    \n",
    "    # Temporal CV\n",
    "    cv_scores_mae = []\n",
    "    cv_scores_rmse = []\n",
    "    cv_scores_r2 = []\n",
    "    \n",
    "    for train_idx, test_idx in tcv.split(X_train_data, y):\n",
    "        X_train, X_test = X_train_data.iloc[train_idx], X_train_data.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train\n",
    "        model = model_config['model'].__class__(**model_config['model'].get_params())\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Score\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        cv_scores_mae.append(mae)\n",
    "        cv_scores_rmse.append(rmse)\n",
    "        cv_scores_r2.append(r2)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': np.mean(cv_scores_mae),\n",
    "        'MAE_std': np.std(cv_scores_mae),\n",
    "        'RMSE': np.mean(cv_scores_rmse),\n",
    "        'R¬≤': np.mean(cv_scores_r2),\n",
    "        'Time (s)': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì MAE: {np.mean(cv_scores_mae):.4f} (¬±{np.std(cv_scores_mae):.4f})\")\n",
    "    print(f\"  ‚úì RMSE: {np.mean(cv_scores_rmse):.4f}\")\n",
    "    print(f\"  ‚úì R¬≤: {np.mean(cv_scores_r2):.4f}\")\n",
    "    \n",
    "    # Train final model on full data for feature importance\n",
    "    final_model = model_config['model'].__class__(**model_config['model'].get_params())\n",
    "    final_model.fit(X_train_data, y)\n",
    "    trained_models[model_name] = final_model\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0d643",
   "metadata": {},
   "source": [
    "## 7. Model Comparison & Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455cf28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).sort_values('MAE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_mae = results_df.iloc[0]['MAE']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   MAE: {best_mae:.4f}\")\n",
    "print(f\"   RMSE: {results_df.iloc[0]['RMSE']:.4f}\")\n",
    "print(f\"   R¬≤: {results_df.iloc[0]['R¬≤']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdabf68",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from tree-based models\n",
    "importance_data = {}\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_data[model_name] = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "if importance_data:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ TOP 15 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use best available model with feature importance\n",
    "    for model_name in [best_model_name, 'Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting']:\n",
    "        if model_name in importance_data:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            top_features = importance_data[model_name].head(15)\n",
    "            for i, row in top_features.iterrows():\n",
    "                print(f\"  {row['feature']:40s} : {row['importance']:8.4f}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Feature importance not available for selected model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db4c55",
   "metadata": {},
   "source": [
    "## 9. Model Export & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66256e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('data/engineered/model_comparison_results.csv', index=False)\n",
    "\n",
    "# Save model summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_model': best_model_name,\n",
    "    'best_mae': float(best_mae),\n",
    "    'best_rmse': float(results_df.iloc[0]['RMSE']),\n",
    "    'best_r2': float(results_df.iloc[0]['R¬≤']),\n",
    "    'n_features': len(feature_cols),\n",
    "    'n_samples': len(X),\n",
    "    'cv_strategy': 'Walk-Forward (5-fold temporal)',\n",
    "    'all_results': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open('data/engineered/model_training_summary_v1.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ RESULTS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model Comparison: data/engineered/model_comparison_results.csv\")\n",
    "print(f\"Training Summary: data/engineered/model_training_summary_v1.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODEL TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"  MAE: {best_mae:.4f} goals per match\")\n",
    "print(f\"  RMSE: {results_df.iloc[0]['RMSE']:.4f}\")\n",
    "print(f\"  R¬≤: {results_df.iloc[0]['R¬≤']:.4f}\")\n",
    "print(f\"\\nDataset: {len(X)} matches, {len(feature_cols)} features\")\n",
    "print(f\"CV Strategy: Walk-Forward 5-fold temporal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
