{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918e1b9c",
   "metadata": {},
   "source": [
    "# PS5: Match Result Prediction (Retrained with Temporal Validation)\n",
    "\n",
    "This notebook retrains the model for **Problem Statement 5: Predicting the match result (Home Win, Draw, Away Win)**.\n",
    "\n",
    "**Correction Log:**\n",
    "1.  **Data Source**: Switched to `data/raw/data_raw_match.csv` which contains the necessary `Date` and `FTR` columns for a proper temporal analysis.\n",
    "2.  **Target Engineering**: The multi-class target `Result` is engineered from the `FTR` column ('H', 'D', 'A').\n",
    "3.  **Temporal Splitting**: Replaced the incorrect `train_test_split` with a strict chronological split. The first 80% of the data is used for training and the final 20% for testing, preserving the timeline.\n",
    "4.  **Cross-Validation**: Implemented `TimeSeriesSplit` for all cross-validation procedures to prevent data leakage during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a3157",
   "metadata": {},
   "source": [
    "# Problem Statement 5: Match Result Prediction\n",
    "## Predict Match Result (Home/Draw/Away)\n",
    "\n",
    "**Author:** ScoreSight ML Team  \n",
    "**Date:** 2025-11-12  \n",
    "**Problem Type:** Multi-class Classification (H/D/A)\n",
    "\n",
    "### Dataset\n",
    "- **File:** `data/match_prediction_corrected.csv`\n",
    "- **Task:** Predict match result (Home/Draw/Away)\n",
    "- **Features:** Match data from corrected dataset\n",
    "- **Target:** Result (H/D/A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341ee15",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf2fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All libraries imported and paths configured.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define file paths\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATA_PATH = Path('../data/raw/data_raw_match.csv')\n",
    "\n",
    "print(\"[OK] All libraries imported and paths configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417ee48",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84749567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading and Preparation ---\n",
      "Data loaded from: ..\\data\\raw\\data_raw_match.csv\n",
      "Shape after sorting: (6840, 42)\n",
      "\n",
      "--- Target Variable ---\n",
      "Engineered 'result' column from goal difference.\n",
      "result\n",
      "H    3176\n",
      "A    1913\n",
      "D    1751\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded 'target' column (0=H, 1=D, 2=A):\n",
      "target\n",
      "0    3176\n",
      "2    1913\n",
      "1    1751\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Features ---\n",
      "Total features selected: 31\n",
      "Numeric features (21): ['htgs', 'atgs', 'htgc', 'atgc', 'htp']...\n",
      "Categorical features (10): ['hm1', 'hm2', 'hm3', 'hm4', 'hm5', 'am1', 'am2', 'am3', 'am4', 'am5']\n",
      "\n",
      "[OK] Data prepared for temporal splitting.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Data Loading and Initial Preprocessing ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Convert date and sort\n",
    "# Use format='mixed' to handle potential inconsistencies (e.g., yy vs yyyy)\n",
    "df['date'] = pd.to_datetime(df['date'], format='mixed')\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- 2. Target Engineering ---\n",
    "# The 'ftr' column contains 'H' (Home Win), 'A' (Away Win), and 'D' (Draw).\n",
    "# We will map these to numerical values for a multi-class classification problem.\n",
    "# H -> 0, D -> 1, A -> 2\n",
    "# Note: The original raw data had 'NH' for non-home wins. Let's create a proper 3-class target.\n",
    "def create_target(row):\n",
    "    if row['fthg'] > row['ftag']:\n",
    "        return 'H'\n",
    "    elif row['fthg'] == row['ftag']:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'A'\n",
    "\n",
    "df['result'] = df.apply(create_target, axis=1)\n",
    "target_map = {'H': 0, 'D': 1, 'A': 2}\n",
    "df['target'] = df['result'].map(target_map)\n",
    "\n",
    "\n",
    "# --- 3. Feature Selection ---\n",
    "# Exclude identifiers, date, target, and other non-feature columns\n",
    "exclude_cols = [\n",
    "    'unnamed: 0', 'date', 'hometeam', 'awayteam', \n",
    "    'fthg', 'ftag', 'ftr', 'result', 'target'\n",
    "]\n",
    "\n",
    "# Select numeric and categorical features\n",
    "numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in exclude_cols]\n",
    "\n",
    "categorical_features = [\n",
    "    'hm1', 'hm2', 'hm3', 'hm4', 'hm5', \n",
    "    'am1', 'am2', 'am3', 'am4', 'am5'\n",
    "]\n",
    "\n",
    "# Final feature list\n",
    "features = numeric_features + categorical_features\n",
    "X = df[features].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "print(\"--- Data Loading and Preparation ---\")\n",
    "print(f\"Data loaded from: {DATA_PATH}\")\n",
    "print(f\"Shape after sorting: {df.shape}\")\n",
    "print(\"\\n--- Target Variable ---\")\n",
    "print(\"Engineered 'result' column from goal difference.\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\nEncoded 'target' column (0=H, 1=D, 2=A):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\n--- Features ---\")\n",
    "print(f\"Total features selected: {len(features)}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\n[OK] Data prepared for temporal splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518147da",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8a2736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temporal Splitting ---\n",
      "Training data: 5472 samples (from 2000-01-10 to 2014-12-13)\n",
      "Testing data:  1368 samples (from 2014-12-13 to 2018-12-03)\n",
      "\n",
      "--- Training Set Target Distribution ---\n",
      "target\n",
      "0    0.466557\n",
      "2    0.275585\n",
      "1    0.257858\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Test Set Target Distribution ---\n",
      "target\n",
      "0    0.455409\n",
      "2    0.296053\n",
      "1    0.248538\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "[OK] Temporal split completed.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Temporal Train-Test Split ---\n",
    "# Split the data chronologically: 80% for training, 20% for testing.\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "train_dates = df['date'].iloc[:split_index]\n",
    "test_dates = df['date'].iloc[split_index:]\n",
    "\n",
    "print(\"--- Temporal Splitting ---\")\n",
    "print(f\"Training data: {len(X_train)} samples (from {train_dates.min().date()} to {train_dates.max().date()})\")\n",
    "print(f\"Testing data:  {len(X_test)} samples (from {test_dates.min().date()} to {test_dates.max().date()})\")\n",
    "\n",
    "print(\"\\n--- Training Set Target Distribution ---\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n--- Test Set Target Distribution ---\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print(\"\\n[OK] Temporal split completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315680ed",
   "metadata": {},
   "source": [
    "## 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a48728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training with Temporal CV ---\n",
      "\n",
      "[TRAINING] ==> LogisticRegression\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[RESULT] Best F1-Macro (avg over 5 splits): 0.4385\n",
      "[RESULT] Best Parameters: {'classifier__C': 0.1}\n",
      "\n",
      "[TRAINING] ==> RandomForestClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[RESULT] Best F1-Macro (avg over 5 splits): 0.4385\n",
      "[RESULT] Best Parameters: {'classifier__C': 0.1}\n",
      "\n",
      "[TRAINING] ==> RandomForestClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[RESULT] Best F1-Macro (avg over 5 splits): 0.4250\n",
      "[RESULT] Best Parameters: {'classifier__n_estimators': 200, 'classifier__min_samples_split': 5, 'classifier__max_depth': 20}\n",
      "\n",
      "[TRAINING] ==> GradientBoostingClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[RESULT] Best F1-Macro (avg over 5 splits): 0.4250\n",
      "[RESULT] Best Parameters: {'classifier__n_estimators': 200, 'classifier__min_samples_split': 5, 'classifier__max_depth': 20}\n",
      "\n",
      "[TRAINING] ==> GradientBoostingClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[RESULT] Best F1-Macro (avg over 5 splits): 0.4327\n",
      "[RESULT] Best Parameters: {'classifier__n_estimators': 100, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.1}\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "                           best_score_f1_macro  \\\n",
      "LogisticRegression                    0.438473   \n",
      "GradientBoostingClassifier            0.432731   \n",
      "RandomForestClassifier                0.424998   \n",
      "\n",
      "                                                                  best_params  \n",
      "LogisticRegression                                     {'classifier__C': 0.1}  \n",
      "GradientBoostingClassifier  {'classifier__n_estimators': 100, 'classifier_...  \n",
      "RandomForestClassifier      {'classifier__n_estimators': 200, 'classifier_...  \n",
      "\n",
      "üèÜ Best performing model from CV: 'LogisticRegression' with F1-Macro: 0.4385\n",
      "\n",
      "[OK] Model training and cross-validation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Model Training with Temporal Cross-Validation ---\n",
    "\n",
    "# Define preprocessing steps for different column types\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a preprocessor object using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define models and their hyperparameter grids\n",
    "models_to_train = {\n",
    "    'LogisticRegression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [10, 20, 30, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Store results\n",
    "trained_models = {}\n",
    "results_summary = {}\n",
    "\n",
    "print(\"--- Starting Model Training with Temporal CV ---\")\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"\\n[TRAINING] ==> {model_name}\")\n",
    "\n",
    "    # Create the full pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Randomized Search with Temporal CV\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_grids[model_name],\n",
    "        n_iter=10,\n",
    "        scoring='f1_macro',\n",
    "        cv=tscv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Store results\n",
    "    trained_models[model_name] = search.best_estimator_\n",
    "    results_summary[model_name] = {\n",
    "        'best_score_f1_macro': search.best_score_,\n",
    "        'best_params': search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"[RESULT] Best F1-Macro (avg over {n_splits} splits): {search.best_score_:.4f}\")\n",
    "    print(f\"[RESULT] Best Parameters: {search.best_params_}\")\n",
    "\n",
    "# --- 6. Identify the Best Overall Model ---\n",
    "best_model_name = max(results_summary, key=lambda k: results_summary[k]['best_score_f1_macro'])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_model_score = results_summary[best_model_name]['best_score_f1_macro']\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "summary_df = pd.DataFrame(results_summary).T\n",
    "summary_df = summary_df.sort_values('best_score_f1_macro', ascending=False)\n",
    "print(summary_df)\n",
    "\n",
    "print(f\"\\nüèÜ Best performing model from CV: '{best_model_name}' with F1-Macro: {best_model_score:.4f}\")\n",
    "print(\"\\n[OK] Model training and cross-validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a832a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Best Model: LogisticRegression ---\n",
      "Test Set Accuracy: 0.5197\n",
      "Test Set F1-Macro: 0.4190\n",
      "Test Set Precision-Macro: 0.4499\n",
      "Test Set Recall-Macro: 0.4453\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Home Win       0.55      0.80      0.65       623\n",
      "        Draw       0.29      0.08      0.13       340\n",
      "    Away Win       0.51      0.45      0.48       405\n",
      "\n",
      "    accuracy                           0.52      1368\n",
      "   macro avg       0.45      0.45      0.42      1368\n",
      "weighted avg       0.47      0.52      0.47      1368\n",
      "\n",
      "\n",
      "[OK] Evaluation on test set complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Evaluate Best Model on Temporal Test Set ---\n",
    "print(f\"--- Evaluating Best Model: {best_model_name} ---\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Store final metrics\n",
    "final_metrics = {\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': accuracy,\n",
    "    'f1_macro': f1,\n",
    "    'precision_macro': precision,\n",
    "    'recall_macro': recall,\n",
    "    'best_cv_params': results_summary[best_model_name]['best_params']\n",
    "}\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Set F1-Macro: {f1:.4f}\")\n",
    "print(f\"Test Set Precision-Macro: {precision:.4f}\")\n",
    "print(f\"Test Set Recall-Macro: {recall:.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Home Win', 'Draw', 'Away Win']))\n",
    "\n",
    "print(\"\\n[OK] Evaluation on test set complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2183d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Artifacts Saved ---\n",
      "‚úÖ Best Model saved to: models\\ps5_match_result_model.joblib\n",
      "‚úÖ Final Metrics saved to: models\\ps5_match_result_metrics.json\n",
      "\n",
      "--- Saved Metrics ---\n",
      "{\n",
      "    \"model_name\": \"LogisticRegression\",\n",
      "    \"accuracy\": 0.5197368421052632,\n",
      "    \"f1_macro\": 0.41903158203900454,\n",
      "    \"precision_macro\": 0.449946891389925,\n",
      "    \"recall_macro\": 0.445303003987002,\n",
      "    \"best_cv_params\": {\n",
      "        \"classifier__C\": 0.1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Save Model and Metrics ---\n",
    "# Define paths for artifacts\n",
    "model_path = MODELS_DIR / 'ps5_match_result_model.joblib'\n",
    "metrics_path = MODELS_DIR / 'ps5_match_result_metrics.json'\n",
    "\n",
    "# Save the best model pipeline\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Save the final metrics\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(f\"--- Artifacts Saved ---\")\n",
    "print(f\"‚úÖ Best Model saved to: {model_path}\")\n",
    "print(f\"‚úÖ Final Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Display the saved metrics\n",
    "print(\"\\n--- Saved Metrics ---\")\n",
    "print(json.dumps(final_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Final Model Performance Comparison ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths to all metric files\n",
    "METRICS_PATHS = {\n",
    "    'PS1: League Winner': Path('models/ps1_league_winner_metadata.json'),\n",
    "    'PS2: Match Winner': Path('models/ps2_match_winner_metadata_v4_ultimate.json'),\n",
    "    'PS4: Total Points': Path('../models/ps4_total_points_metadata.json'), # Note the different path\n",
    "    'PS5: Match Result': Path('models/ps5_match_result_metrics.json')\n",
    "}\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Check which files exist to avoid errors\n",
    "existing_paths = {name: path for name, path in METRICS_PATHS.items() if path.exists()}\n",
    "print(\"Found the following metric files:\")\n",
    "for name, path in existing_paths.items():\n",
    "    print(f\"- {name}: {path}\")\n",
    "\n",
    "# --- Metric Extraction ---\n",
    "summary_data = []\n",
    "for name, path in existing_paths.items():\n",
    "    with open(path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    if 'PS4' in name:\n",
    "        # Regression model\n",
    "        score = metrics.get('r2_score')\n",
    "        metric_name = 'R-Squared'\n",
    "    else:\n",
    "        # Classification models\n",
    "        # Use the primary metric from the dictionary, could be 'f1_macro' or 'accuracy'\n",
    "        score = metrics.get('f1_macro', metrics.get('accuracy'))\n",
    "        metric_name = 'F1-Macro / Accuracy'\n",
    "        \n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Primary Metric': metric_name,\n",
    "        'Score': score\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Create the bar plot\n",
    "sns.barplot(x='Score', y='Model', data=summary_df, ax=ax, palette='viridis')\n",
    "\n",
    "# Add score labels to the bars\n",
    "for index, row in summary_df.iterrows():\n",
    "    ax.text(row['Score'] + 0.01, index, f\"{row['Score']:.3f}\", \n",
    "            color='black', ha=\"left\", va='center', fontsize=12)\n",
    "\n",
    "# Set titles and labels\n",
    "ax.set_title('Final Retrained Model Performance Comparison', fontsize=18, pad=20)\n",
    "ax.set_xlabel('Performance Score (F1-Macro or R-Squared)', fontsize=12)\n",
    "ax.set_ylabel('Problem Statement', fontsize=12)\n",
    "ax.set_xlim(0, 1.05) # R-squared can be up to 1.0\n",
    "\n",
    "# --- Save the Figure ---\n",
    "VIZ_DIR = Path('../visualizations')\n",
    "VIZ_DIR.mkdir(exist_ok=True)\n",
    "output_path = VIZ_DIR / 'final_model_performance_comparison.png'\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\n--- Comparison Chart Generated ---\")\n",
    "print(summary_df)\n",
    "print(f\"\\n‚úÖ Chart saved to: {output_path}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
