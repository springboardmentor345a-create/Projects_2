{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bcb941",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 1: Setup and Configuration\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    StratifiedKFold, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"IMPORTING LIBRARIES...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Try importing XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed\")\n",
    "\n",
    "# Global Configuration\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CONFIGURING PATHS...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "os.chdir('d:\\\\ScoreSight')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Create directory structure\n",
    "MODELS_DIR = Path('models')\n",
    "VIZ_DIR = Path('visualizations/ps4_match_result')\n",
    "DATA_DIR = Path('data')\n",
    "DATASETS_DIR = Path('datasets')\n",
    "FINAL_DATA_PATH = DATA_DIR / 'match_result' / 'match_result_data.csv'\n",
    "\n",
    "for dir_path in [MODELS_DIR, VIZ_DIR, FINAL_DATA_PATH.parent]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    print(f\"Created/verified: {dir_path}\")\n",
    "\n",
    "# Raw data path\n",
    "RAW_MATCH_DATA_PATH = DATASETS_DIR / 'Match Winner.csv'\n",
    "\n",
    "# Display options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PS4 MATCH RESULT PIPELINE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713e655",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ab6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 2: Data Loading and Advanced Feature Engineering\n",
    "# ==================================================\n",
    "print(f\"Loading raw match data from: {RAW_MATCH_DATA_PATH}\")\n",
    "try:\n",
    "    df_mw = pd.read_csv(RAW_MATCH_DATA_PATH)\n",
    "    df_mw['Date'] = pd.to_datetime(df_mw['Date'], format='mixed')\n",
    "    df_mw = df_mw.sort_values('Date').reset_index(drop=True)\n",
    "    df_mw = df_mw[['Date','HomeTeam','AwayTeam','FTHG','FTAG','FTR']].dropna()\n",
    "    print(f\"Loaded and sorted raw match data: {df_mw.shape}\")\n",
    "\n",
    "    # --- Advanced Feature Engineering ---\n",
    "    WINDOWS=[5,10]; ALPHAS=[0.1,0.2]\n",
    "    teams = pd.concat([df_mw['HomeTeam'], df_mw['AwayTeam']]).unique()\n",
    "    team_hist={t:[] for t in teams}; h2h_hist={}\n",
    "    rows=[]\n",
    "    for _,r in df_mw.iterrows():\n",
    "        h,a=r['HomeTeam'], r['AwayTeam']\n",
    "        key=tuple(sorted((h,a)))\n",
    "        if key not in h2h_hist: h2h_hist[key]=[]\n",
    "        def feats(hist, opp_hist, h2h):\n",
    "            out={}\n",
    "            for W in WINDOWS:\n",
    "                if len(hist)>=W:\n",
    "                    w=pd.DataFrame(hist[-W:])\n",
    "                    out[f'avg_gs_{W}']=w['gs'].mean(); out[f'avg_gc_{W}']=w['gc'].mean(); out[f'avg_gd_{W}']=w['gd'].mean(); out[f'avg_pts_{W}']=w['pts'].mean()\n",
    "                else:\n",
    "                    out[f'avg_gs_{W}']=np.nan; out[f'avg_gc_{W}']=np.nan; out[f'avg_gd_{W}']=np.nan; out[f'avg_pts_{W}']=np.nan\n",
    "            if len(hist)>1:\n",
    "                hist_df=pd.DataFrame(hist)\n",
    "                for a_ in ALPHAS:\n",
    "                    out[f'ewma_gs_{a_}']=hist_df['gs'].ewm(alpha=a_).mean().iloc[-1]\n",
    "                    out[f'ewma_gc_{a_}']=hist_df['gc'].ewm(alpha=a_).mean().iloc[-1]\n",
    "                    out[f'ewma_gd_{a_}']=hist_df['gd'].ewm(alpha=a_).mean().iloc[-1]\n",
    "            else:\n",
    "                for a_ in ALPHAS:\n",
    "                    out[f'ewma_gs_{a_}']=np.nan; out[f'ewma_gc_{a_}']=np.nan; out[f'ewma_gd_{a_}']=np.nan\n",
    "            if len(h2h)>0:\n",
    "                d=pd.DataFrame(h2h)\n",
    "                out['h2h_avg_gs']=d['gs'].mean(); out['h2h_avg_gc']=d['gc'].mean(); out['h2h_win_rate']=(d['pts']==3).mean()\n",
    "            else:\n",
    "                out['h2h_avg_gs']=np.nan; out['h2h_avg_gc']=np.nan; out['h2h_win_rate']=np.nan\n",
    "            return out\n",
    "        hf=feats(team_hist[h], team_hist[a], h2h_hist[key]); af=feats(team_hist[a], team_hist[h], h2h_hist[key])\n",
    "        row={f'H_{k}':v for k,v in hf.items()}\n",
    "        row.update({f'A_{k}':v for k,v in af.items()})\n",
    "        for W in WINDOWS: row[f'diff_avg_gd_{W}']=hf.get(f'avg_gd_{W}')-af.get(f'avg_gd_{W}')\n",
    "        for a_ in ALPHAS: row[f'diff_ewma_gd_{a_}']=hf.get(f'ewma_gd_{a_}')-af.get(f'ewma_gd_{a_}')\n",
    "        rows.append(row)\n",
    "        # update histories\n",
    "        hg,ag=int(r['FTHG']), int(r['FTAG'])\n",
    "        if r['FTR']=='H': hp,ap=3,0\n",
    "        elif r['FTR']=='A': hp,ap=0,3\n",
    "        else: hp,ap=1,1\n",
    "        team_hist[h].append({'gs':hg,'gc':ag,'gd':hg-ag,'pts':hp}); team_hist[a].append({'gs':ag,'gc':hg,'gd':ag-hg,'pts':ap}); h2h_hist[key].append({'team':h,'gs':hg,'gc':ag,'pts':hp})\n",
    "\n",
    "    feat_df=pd.DataFrame(rows, index=df_mw.index).dropna()\n",
    "    df_mw2=df_mw.loc[feat_df.index].copy()\n",
    "    \n",
    "    # Encode target: H/D/A as 0/1/2\n",
    "    class_map={'H':0,'D':1,'A':2}\n",
    "    df_ps4=pd.concat([feat_df.reset_index(drop=True), df_mw2['FTR'].map(class_map).rename('target')], axis=1)\n",
    "    \n",
    "    # Save the engineered data\n",
    "    df_ps4.to_csv(FINAL_DATA_PATH, index=False)\n",
    "    \n",
    "    print(f\"Engineered dataset created with {df_ps4.shape[1]-1} features.\")\n",
    "    print(f\"Saved final modeling data to: {FINAL_DATA_PATH}\")\n",
    "    print(\"\\nFinal DataFrame head:\")\n",
    "    display(df_ps4.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Raw data file not found at {RAW_MATCH_DATA_PATH}\")\n",
    "    df_ps4 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518d9b8",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 3: Model Training\n",
    "# ==================================================\n",
    "if df_ps4 is not None:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PS4: MATCH RESULT PREDICTION - TRAINING\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Define target and features\n",
    "    TARGET_COL_PS4 = 'target'\n",
    "    feature_cols_ps4 = [c for c in df_ps4.columns if c != TARGET_COL_PS4]\n",
    "    \n",
    "    X_ps4 = df_ps4[feature_cols_ps4].copy()\n",
    "    y_ps4 = df_ps4[TARGET_COL_PS4]\n",
    "    \n",
    "    print(f\"Features ({len(feature_cols_ps4)}): {feature_cols_ps4[:5]}...\")\n",
    "    print(f\"Target: {TARGET_COL_PS4}\")\n",
    "\n",
    "    # Stratified split\n",
    "    X_train_ps4, X_test_ps4, y_train_ps4, y_test_ps4 = train_test_split(\n",
    "        X_ps4, y_ps4, test_size=0.30, random_state=42, stratify=y_ps4\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split: {X_train_ps4.shape[0]} train / {X_test_ps4.shape[0]} test\")\n",
    "    \n",
    "    # Model configurations\n",
    "    models_ps4 = {\n",
    "        'LogisticRegression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_ps4['XGBoost'] = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    \n",
    "    # Parameter grids\n",
    "    param_grids_ps4 = {\n",
    "        'LogisticRegression': {'model__C': [0.01, 0.1, 1.0]},\n",
    "        'RandomForest': {'model__n_estimators': [50, 100], 'model__max_depth': [8, 15]},\n",
    "        'GradientBoosting': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]},\n",
    "        'XGBoost': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]}\n",
    "    }\n",
    "    \n",
    "    # --- Execute Training ---\n",
    "    best_model_data = None\n",
    "    best_f1_score = -1\n",
    "\n",
    "    for model_name, model in models_ps4.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline, param_distributions=param_grids_ps4.get(model_name, {}),\n",
    "            n_iter=10, cv=StratifiedKFold(n_splits=5), \n",
    "            scoring='f1_macro', n_jobs=-1, random_state=42\n",
    "        )\n",
    "        search.fit(X_train_ps4, y_train_ps4)\n",
    "        y_pred = search.predict(X_test_ps4)\n",
    "        f1 = f1_score(y_test_ps4, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"  Test F1-Macro: {f1:.4f}\")\n",
    "        print(f\"  Best CV Score: {search.best_score_:.4f}\")\n",
    "\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            best_model_data = {\n",
    "                'name': model_name,\n",
    "                'model': search.best_estimator_,\n",
    "                'f1_score': f1,\n",
    "                'report': classification_report(y_test_ps4, y_pred, output_dict=True)\n",
    "            }\n",
    "\n",
    "    print(f\"\\n--- Best Model: {best_model_data['name']} with F1-Macro: {best_model_data['f1_score']:.4f} ---\")\n",
    "\n",
    "    # --- Save Artifacts ---\n",
    "    model_path = MODELS_DIR / 'ps4_match_result_model.joblib'\n",
    "    metadata_path = MODELS_DIR / 'ps4_match_result_metrics.json'\n",
    "    \n",
    "    joblib.dump(best_model_data['model'], model_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'problem_name': 'PS4_Match_Result',\n",
    "        'best_model': best_model_data['name'],\n",
    "        'task_type': 'classification',\n",
    "        'test_metrics': {'f1_macro': best_model_data['f1_score']},\n",
    "        'classification_report': best_model_data['report']\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping training because data loading failed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
