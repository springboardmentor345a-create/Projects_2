{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b381eb",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 1: Setup and Configuration\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    TimeSeriesSplit, KFold, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"IMPORTING LIBRARIES...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Try importing XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"LightGBM not installed\")\n",
    "\n",
    "# Global Configuration\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CONFIGURING PATHS...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "os.chdir('d:\\\\ScoreSight')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Create directory structure\n",
    "MODELS_DIR = Path('models')\n",
    "VIZ_DIR = Path('visualizations/ps3_total_points')\n",
    "DATA_DIR = Path('data')\n",
    "DATASETS_DIR = Path('datasets')\n",
    "FINAL_DATA_PATH = DATA_DIR / 'points_tally' / 'points_tally_data.csv'\n",
    "\n",
    "for dir_path in [MODELS_DIR, VIZ_DIR, FINAL_DATA_PATH.parent]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    print(f\"Created/verified: {dir_path}\")\n",
    "\n",
    "# Raw data path\n",
    "RAW_LEAGUE_DATA_PATH = DATASETS_DIR / 'ScoreSight_ML_Season_LeagueWinner_Champion.csv'\n",
    "\n",
    "# Display options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PS3 TOTAL POINTS PIPELINE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3bed5",
   "metadata": {},
   "source": [
    "## 2. Data Loading, Cleaning, and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 2: Data Loading and Engineering\n",
    "# ==================================================\n",
    "print(f\"Loading raw league data from: {RAW_LEAGUE_DATA_PATH}\")\n",
    "try:\n",
    "    df_league_raw = pd.read_csv(RAW_LEAGUE_DATA_PATH)\n",
    "    print(f\"Loaded raw league data: {df_league_raw.shape}\")\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    df_league_raw.columns = df_league_raw.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    df_league_raw.drop_duplicates(inplace=True)\n",
    "    print(f\"Cleaned data shape: {df_league_raw.shape}\")\n",
    "\n",
    "    # --- Feature Engineering & Selection ---\n",
    "    # These columns are either direct calculations of points or other targets\n",
    "    leak_cols = ['points_per_game', 'target_league_position', 'target_champion', 'wins', 'draws', 'losses', 'points']\n",
    "    \n",
    "    # Define feature columns\n",
    "    feature_cols = [c for c in df_league_raw.columns if c not in leak_cols + ['team', 'season_encoded', 'team_encoded'] and df_league_raw[c].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    # Create the final DataFrame for modeling\n",
    "    # The target is 'target_total_points'\n",
    "    df_ps3 = df_league_raw[feature_cols + ['target_total_points', 'season']].copy()\n",
    "    df_ps3 = df_ps3.rename(columns={'target_total_points': 'total_points'})\n",
    "    \n",
    "    # Save the engineered data\n",
    "    df_ps3.to_csv(FINAL_DATA_PATH, index=False)\n",
    "    \n",
    "    print(f\"Engineered dataset created with {len(feature_cols)} features.\")\n",
    "    print(f\"Saved final modeling data to: {FINAL_DATA_PATH}\")\n",
    "    print(\"\\nFinal DataFrame head:\")\n",
    "    display(df_ps3.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Raw data file not found at {RAW_LEAGUE_DATA_PATH}\")\n",
    "    df_ps3 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48e394",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 3: Model Training\n",
    "# ==================================================\n",
    "if df_ps3 is not None:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PS3: TOTAL POINTS PREDICTION - TRAINING\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Define target and features\n",
    "    TARGET_COL_PS3 = 'total_points'\n",
    "    feature_cols_ps3 = [c for c in df_ps3.columns if c not in [TARGET_COL_PS3, 'season']]\n",
    "    \n",
    "    X_ps3 = df_ps3[feature_cols_ps3].copy()\n",
    "    y_ps3 = df_ps3[TARGET_COL_PS3]\n",
    "    \n",
    "    print(f\"Features ({len(feature_cols_ps3)}): {feature_cols_ps3}\")\n",
    "    print(f\"Target: {TARGET_COL_PS3}\")\n",
    "\n",
    "    # Temporal split by season\n",
    "    seasons = sorted(df_ps3['season'].dropna().unique())\n",
    "    if len(seasons) > 1:\n",
    "        test_season = seasons[-1]\n",
    "        train_mask = df_ps3['season'].isin(seasons[:-1])\n",
    "        test_mask = df_ps3['season'] == test_season\n",
    "        X_train_ps3, y_train_ps3 = X_ps3[train_mask], y_ps3[train_mask]\n",
    "        X_test_ps3, y_test_ps3 = X_ps3[test_mask], y_ps3[test_mask]\n",
    "        print(f\"\\nTemporal split: Training on {seasons[:-1]}, Testing on {test_season}\")\n",
    "    else:\n",
    "        X_train_ps3, X_test_ps3, y_train_ps3, y_test_ps3 = train_test_split(X_ps3, y_ps3, test_size=0.25, random_state=42)\n",
    "        print(\"\\nRandom split (only one season of data)\")\n",
    "\n",
    "    print(f\"Data split: {X_train_ps3.shape[0]} train / {X_test_ps3.shape[0]} test\")\n",
    "    \n",
    "    # Model configurations\n",
    "    models_ps3 = {\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_ps3['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    if LGB_AVAILABLE:\n",
    "        models_ps3['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    \n",
    "    # Parameter grids\n",
    "    param_grids_ps3 = {\n",
    "        'Ridge': {'model__alpha': [0.1, 1.0, 10.0]},\n",
    "        'RandomForest': {'model__n_estimators': [50, 100], 'model__max_depth': [10, 15]},\n",
    "        'GradientBoosting': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]},\n",
    "        'XGBoost': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]},\n",
    "        'LightGBM': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]}\n",
    "    }\n",
    "    \n",
    "    # --- Execute Training ---\n",
    "    best_model_data = None\n",
    "    best_mae = float('inf')\n",
    "\n",
    "    for model_name, model in models_ps3.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline, param_distributions=param_grids_ps3.get(model_name, {}),\n",
    "            n_iter=10, cv=TimeSeriesSplit(n_splits=5), \n",
    "            scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "        )\n",
    "        search.fit(X_train_ps3, y_train_ps3)\n",
    "        y_pred = search.predict(X_test_ps3)\n",
    "        mae = mean_absolute_error(y_test_ps3, y_pred)\n",
    "        \n",
    "        print(f\"  Test MAE: {mae:.4f}\")\n",
    "        print(f\"  Best CV Score (neg MAE): {search.best_score_:.4f}\")\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_model_data = {\n",
    "                'name': model_name,\n",
    "                'model': search.best_estimator_,\n",
    "                'mae': mae,\n",
    "                'r2': r2_score(y_test_ps3, y_pred)\n",
    "            }\n",
    "\n",
    "    print(f\"\\n--- Best Model: {best_model_data['name']} with MAE: {best_model_data['mae']:.4f} ---\")\n",
    "\n",
    "    # --- Save Artifacts ---\n",
    "    model_path = MODELS_DIR / 'ps3_total_points_best_model.joblib'\n",
    "    metadata_path = MODELS_DIR / 'ps3_total_points_metadata.json'\n",
    "    \n",
    "    joblib.dump(best_model_data['model'], model_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'problem_name': 'PS3_Total_Points',\n",
    "        'best_model': best_model_data['name'],\n",
    "        'task_type': 'regression',\n",
    "        'test_metrics': {'mae': best_model_data['mae'], 'r2': best_model_data['r2']}\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping training because data loading failed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
