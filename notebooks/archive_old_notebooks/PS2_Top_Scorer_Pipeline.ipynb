{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6be864",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816297f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 1: Setup and Configuration\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    KFold, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"IMPORTING LIBRARIES...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Try importing XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"LightGBM available\")\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"LightGBM not installed\")\n",
    "\n",
    "# Global Configuration\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CONFIGURING PATHS...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "os.chdir('d:\\\\ScoreSight')\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Create directory structure\n",
    "MODELS_DIR = Path('models')\n",
    "VIZ_DIR = Path('visualizations/ps2_top_scorer')\n",
    "DATA_DIR = Path('data')\n",
    "DATASETS_DIR = Path('datasets')\n",
    "FINAL_DATA_PATH = DATA_DIR / 'top_scorer' / 'top_scorer_data.csv'\n",
    "\n",
    "for dir_path in [MODELS_DIR, VIZ_DIR, FINAL_DATA_PATH.parent]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    print(f\"Created/verified: {dir_path}\")\n",
    "\n",
    "# Raw data path\n",
    "RAW_PLAYER_DATA_PATH = DATASETS_DIR / 'Goals & Assist.xlsx'\n",
    "\n",
    "# Display options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PS2 TOP SCORER PIPELINE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba185b",
   "metadata": {},
   "source": [
    "## 2. Data Loading, Cleaning, and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fab7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 2: Data Loading and Engineering\n",
    "# ==================================================\n",
    "print(f\"Loading raw player data from: {RAW_PLAYER_DATA_PATH}\")\n",
    "try:\n",
    "    df_player_raw = pd.read_excel(RAW_PLAYER_DATA_PATH)\n",
    "    print(f\"Loaded raw player data: {df_player_raw.shape}\")\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    df_player_raw.columns = df_player_raw.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    df_player_raw.drop_duplicates(inplace=True)\n",
    "    \n",
    "    for col in df_player_raw.select_dtypes(include=[np.number]).columns:\n",
    "        if df_player_raw[col].isnull().sum() > 0:\n",
    "            df_player_raw[col].fillna(df_player_raw[col].median(), inplace=True)\n",
    "            \n",
    "    for col in df_player_raw.select_dtypes(include=['object']).columns:\n",
    "        if df_player_raw[col].isnull().sum() > 0:\n",
    "            df_player_raw[col].fillna(df_player_raw[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(f\"Cleaned data shape: {df_player_raw.shape}\")\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "    df_player_eng = df_player_raw.copy()\n",
    "    df_player_eng['90s'] = df_player_eng['matches_played']\n",
    "    df_player_eng['goals_per_90'] = (df_player_eng['goals'] / df_player_eng['90s'].replace(0, 1)).round(2)\n",
    "    df_player_eng['assists_per_90'] = (df_player_eng['assists'] / df_player_eng['90s'].replace(0, 1)).round(2)\n",
    "    df_player_eng['xg_overperformance'] = df_player_eng['goals'] - df_player_eng['xg']\n",
    "    \n",
    "    age_bins = [0, 24, 29, 34, 100]\n",
    "    age_labels = ['Prospect', 'Peak', 'Experienced', 'Veteran']\n",
    "    df_player_eng['age_band'] = pd.cut(df_player_eng['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "    \n",
    "    # --- Feature Selection ---\n",
    "    exclude = ['player','team','season','nationality','position','player_encoded','nation_encoded','position_encoded','goals','non_penalty_goals','penalty_goals_made', 'age_band']\n",
    "    feature_cols = [c for c in df_player_eng.columns if c not in exclude and df_player_eng[c].dtype in ['float64','int64']]\n",
    "    \n",
    "    df_ps2 = df_player_eng[feature_cols + ['goals']].copy()\n",
    "    \n",
    "    # Save the engineered data\n",
    "    df_ps2.to_csv(FINAL_DATA_PATH, index=False)\n",
    "    \n",
    "    print(f\"Engineered dataset created with {len(feature_cols)} features.\")\n",
    "    print(f\"Saved final modeling data to: {FINAL_DATA_PATH}\")\n",
    "    print(\"\\nFinal DataFrame head:\")\n",
    "    display(df_ps2.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Raw data file not found at {RAW_PLAYER_DATA_PATH}\")\n",
    "    df_ps2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7afd2",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 3: Model Training\n",
    "# ==================================================\n",
    "if df_ps2 is not None:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PS2: TOP SCORER PREDICTION - TRAINING\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Define target and features\n",
    "    TARGET_COL_PS2 = 'goals'\n",
    "    feature_cols_ps2 = [c for c in df_ps2.columns if c != TARGET_COL_PS2]\n",
    "    \n",
    "    X_ps2 = df_ps2[feature_cols_ps2].copy()\n",
    "    y_ps2 = df_ps2[TARGET_COL_PS2]\n",
    "    \n",
    "    print(f\"Features ({len(feature_cols_ps2)}): {feature_cols_ps2}\")\n",
    "    print(f\"Target: {TARGET_COL_PS2}\")\n",
    "\n",
    "    # Temporal split (80/20)\n",
    "    split_idx = int(len(df_ps2) * 0.8)\n",
    "    X_train_ps2, y_train_ps2 = X_ps2.iloc[:split_idx], y_ps2.iloc[:split_idx]\n",
    "    X_test_ps2, y_test_ps2 = X_ps2.iloc[split_idx:], y_ps2.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"\\nData split (temporal): {X_train_ps2.shape[0]} train / {X_test_ps2.shape[0]} test\")\n",
    "    \n",
    "    # Model configurations\n",
    "    models_ps2 = {\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_ps2['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    if LGB_AVAILABLE:\n",
    "        models_ps2['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    \n",
    "    # Parameter grids\n",
    "    param_grids_ps2 = {\n",
    "        'Ridge': {'model__alpha': [0.1, 1.0, 10.0]},\n",
    "        'RandomForest': {'model__n_estimators': [50, 100], 'model__max_depth': [10, 15]},\n",
    "        'GradientBoosting': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]},\n",
    "        'XGBoost': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]},\n",
    "        'LightGBM': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05]}\n",
    "    }\n",
    "    \n",
    "    # --- Execute Training ---\n",
    "    best_model_data = None\n",
    "    best_mae = float('inf')\n",
    "\n",
    "    for model_name, model in models_ps2.items():\n",
    "        print(f\"\\n--- Training {model_name} ---\")\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline, param_distributions=param_grids_ps2.get(model_name, {}),\n",
    "            n_iter=10, cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "            scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "        )\n",
    "        search.fit(X_train_ps2, y_train_ps2)\n",
    "        y_pred = search.predict(X_test_ps2)\n",
    "        mae = mean_absolute_error(y_test_ps2, y_pred)\n",
    "        \n",
    "        print(f\"  Test MAE: {mae:.4f}\")\n",
    "        print(f\"  Best CV Score (neg MAE): {search.best_score_:.4f}\")\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_model_data = {\n",
    "                'name': model_name,\n",
    "                'model': search.best_estimator_,\n",
    "                'mae': mae,\n",
    "                'r2': r2_score(y_test_ps2, y_pred)\n",
    "            }\n",
    "\n",
    "    print(f\"\\n--- Best Model: {best_model_data['name']} with MAE: {best_model_data['mae']:.4f} ---\")\n",
    "\n",
    "    # --- Save Artifacts ---\n",
    "    model_path = MODELS_DIR / 'ps2_top_scorer_best_model.joblib'\n",
    "    metadata_path = MODELS_DIR / 'ps2_top_scorer_metadata.json'\n",
    "    \n",
    "    joblib.dump(best_model_data['model'], model_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'problem_name': 'PS2_Top_Scorer',\n",
    "        'best_model': best_model_data['name'],\n",
    "        'task_type': 'regression',\n",
    "        'test_metrics': {'mae': best_model_data['mae'], 'r2': best_model_data['r2']}\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping training because data loading failed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
