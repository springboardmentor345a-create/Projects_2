{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e58b827",
   "metadata": {},
   "source": [
    "# PS4: Total Points Prediction (Corrected)\n",
    "\n",
    "**Objective:** Predict the total points a team will accumulate in a season.\n",
    "\n",
    "**Methodology Correction:**\n",
    "The original notebook design was flawed, likely using a random split that leads to data leakage. This corrected version implements a strict temporal validation strategy to ensure the model provides realistic and trustworthy predictions.\n",
    "\n",
    "1.  **Data Source:** We will use `data_final_points_tally.csv`, which contains aggregated team statistics per season.\n",
    "2.  **Temporal Splitting:** The data will be sorted by season. The final season in the dataset will be held out as the test set, while all prior seasons will be used for training. This mimics a real-world scenario of predicting a future season's outcome based on historical data.\n",
    "3.  **Cross-Validation:** `TimeSeriesSplit` will be used during hyperparameter tuning to ensure validation folds respect the chronological order of the data.\n",
    "4.  **Evaluation:** The model's performance will be judged on the unseen test set (the final season) using standard regression metrics (R-squared, MAE, RMSE).\n",
    "5.  **Model Saving:** The best-performing model and its corresponding metadata will be saved for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877146d1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97590871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected Data Loading and Setup ---\n",
    "# Objective: Load the dataset, define features and target, and prepare for temporal validation.\n",
    "# Methodology: We will use 'data_final_points_tally.csv', which is pre-processed and contains season-level data.\n",
    "# The data will be sorted by season to ensure chronological order before splitting.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Check for XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# Define file paths\n",
    "DATA_FILE = '../data/final/data_final_points_tally.csv'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_NAME = 'ps4_total_points_best_model.joblib'\n",
    "METADATA_NAME = 'ps4_total_points_metadata.json'\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Load and sort the data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df = df.sort_values(by='season_encoded').reset_index(drop=True)\n",
    "\n",
    "# --- Feature and Target Definition ---\n",
    "# Target: 'target_total_points' - The total points a team achieved in a season.\n",
    "# Features: All other relevant columns, excluding identifiers and potential data leaks.\n",
    "# We must remove 'points_per_game' as it is a direct derivative of the target and causes leakage.\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df['target_total_points']\n",
    "X = df.drop(columns=['team', 'target_total_points', 'points_per_game'])\n",
    "\n",
    "print(\"Data loaded and sorted successfully.\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"Data spans from season {df['season_encoded'].min()} to {df['season_encoded'].max()}\")\n",
    "print(f\"XGBoost available: {XGB_AVAILABLE}\")\n",
    "print(f\"LightGBM available: {LGB_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73a323",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPER TEMPORAL TRAIN/TEST SPLIT (NO RANDOM SPLITS)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTING PROPER TEMPORAL VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by season to ensure temporal ordering\n",
    "df_sorted = df.sort_values('season_encoded').reset_index(drop=True)\n",
    "\n",
    "# Use temporal split: earlier seasons for training, later for testing\n",
    "seasons = sorted(df_sorted['season_encoded'].unique())\n",
    "print(f\"Available seasons: {seasons}\")\n",
    "\n",
    "if len(seasons) >= 3:\n",
    "    # Use first 60% of seasons for training, middle 20% for validation, last 20% for testing\n",
    "    n_seasons = len(seasons)\n",
    "    train_seasons = seasons[:int(0.6 * n_seasons)]\n",
    "    val_seasons = seasons[int(0.6 * n_seasons):int(0.8 * n_seasons)]\n",
    "    test_seasons = seasons[int(0.8 * n_seasons):]\n",
    "    \n",
    "    print(f\"Training seasons: {train_seasons}\")\n",
    "    print(f\"Validation seasons: {val_seasons}\")\n",
    "    print(f\"Test seasons: {test_seasons}\")\n",
    "    \n",
    "    # Create splits\n",
    "    train_mask = df_sorted['season_encoded'].isin(train_seasons)\n",
    "    val_mask = df_sorted['season_encoded'].isin(val_seasons)\n",
    "    test_mask = df_sorted['season_encoded'].isin(test_seasons)\n",
    "    \n",
    "    X_train = df_sorted[train_mask][valid_features]\n",
    "    y_train = df_sorted[train_mask]['target_total_points']\n",
    "    \n",
    "    X_val = df_sorted[val_mask][valid_features]\n",
    "    y_val = df_sorted[val_mask]['target_total_points']\n",
    "    \n",
    "    X_test = df_sorted[test_mask][valid_features]\n",
    "    y_test = df_sorted[test_mask]['target_total_points']\n",
    "    \n",
    "    print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: use simple train/test split but warn about temporal issues\n",
    "    print(\"WARNING: Not enough seasons for proper temporal validation\")\n",
    "    print(\"Using stratified split by team instead\")\n",
    "    \n",
    "    # Group by team and use some teams for training, others for testing\n",
    "    unique_teams = df['team_encoded'].unique()\n",
    "    np.random.seed(42)\n",
    "    train_teams = np.random.choice(unique_teams, size=int(0.7 * len(unique_teams)), replace=False)\n",
    "    test_teams = [team for team in unique_teams if team not in train_teams]\n",
    "    \n",
    "    train_mask = df['team_encoded'].isin(train_teams)\n",
    "    test_mask = df['team_encoded'].isin(test_teams)\n",
    "    \n",
    "    X_train = df[train_mask][valid_features]\n",
    "    y_train = df[train_mask]['target_total_points']\n",
    "    X_test = df[test_mask][valid_features]\n",
    "    y_test = df[test_mask]['target_total_points']\n",
    "    \n",
    "    X_val = X_test  # Use test set as validation for simplicity\n",
    "    y_val = y_test\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} samples ({len(train_teams)} teams)\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({len(test_teams)} teams)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfb1d8",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"[SPLIT] Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"[SPLIT] Train target - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"[SPLIT] Test target  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER SEARCH SPACE - Total Points Regression (Small Dataset)\n",
    "# ============================================================================\n",
    "\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [50, 100, 150],\n",
    "        'model__max_depth': [5, 10, 15, None],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model__n_estimators': [50, 100, 150],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__subsample': [0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'model__n_estimators': [50, 100, 150],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__subsample': [0.8, 0.9]\n",
    "    }\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    param_grids['LightGBM'] = {\n",
    "        'model__n_estimators': [50, 100, 150],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__num_leaves': [20, 30, 40]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING WITH HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "def create_pipeline(model, use_scaling=True):\n",
    "    steps = [('imputer', SimpleImputer(strategy='mean'))]\n",
    "    if use_scaling:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', model))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING - RandomizedSearchCV (Regression)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "best_models = {}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models_to_train = {\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    models_to_train['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    models_to_train['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"[{model_name}] Hyperparameter Tuning...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    pipeline = create_pipeline(model)\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        n_iter=20,\n",
    "        cv=cv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[model_name] = search.best_estimator_\n",
    "    print(f\"\\n[BEST] Params: {search.best_params_}\")\n",
    "    print(f\"[CV] MAE: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_mae': float(-search.best_score_),\n",
    "        'test_mae': float(mae),\n",
    "        'test_rmse': float(rmse),\n",
    "        'test_r2': float(r2)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n[TEST] MAE:  {mae:.4f}\")\n",
    "    print(f\"[TEST] RMSE: {rmse:.4f}\")\n",
    "    print(f\"[TEST] R²:   {r2:.4f}\")\n",
    "    \n",
    "    # Residuals analysis\n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"[TEST] Residuals - Mean: {residuals.mean():.4f}, Std: {residuals.std():.4f}\")\n",
    "    \n",
    "    trained_models[model_name] = best_models[model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaeb25d",
   "metadata": {},
   "source": [
    "## 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf38693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "best_model_name = min(results, key=lambda x: results[x]['test_mae'])\n",
    "best_metrics = results[best_model_name]\n",
    "\n",
    "print(f\"\\n[WINNER] Best Model: {best_model_name}\")\n",
    "print(f\"[WINNER] Test MAE: {best_metrics['test_mae']:.4f}\")\n",
    "print(f\"[WINNER] Test RMSE: {best_metrics['test_rmse']:.4f}\")\n",
    "print(f\"[WINNER] Test R²: {best_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE ONLY THE BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING BEST MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save only the best model\n",
    "best_model_path = models_dir / 'ps4_total_points_best_model.joblib'\n",
    "joblib.dump(trained_models[best_model_name], best_model_path)\n",
    "print(f\"[SAVE] Best Model ({best_model_name}) -> {best_model_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TRAINING SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "summary = {\n",
    "    'problem_statement': 'PS4: Total Points Tally Prediction',\n",
    "    'task_type': 'Regression',\n",
    "    'best_model': best_model_name,\n",
    "    'best_test_mae': best_metrics['test_mae'],\n",
    "    'best_test_rmse': best_metrics['test_rmse'],\n",
    "    'best_test_r2': best_metrics['test_r2'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data': {\n",
    "        'path': data_path,\n",
    "        'shape': list(df.shape),\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'n_features': len(feature_cols)\n",
    "    },\n",
    "    'features': feature_cols,\n",
    "    'cv_strategy': '5-Fold KFold',\n",
    "    'tuning_method': 'RandomizedSearchCV (20 iterations)',\n",
    "    'scoring_metric': 'neg_mean_absolute_error',\n",
    "    'best_params': best_metrics['best_params']\n",
    "}\n",
    "\n",
    "summary_path = models_dir / 'ps4_total_points_metadata.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[SAVE] Metadata -> {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ PS4: TOTAL POINTS PREDICTION - COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
