{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e991c0a",
   "metadata": {},
   "source": [
    "# ScoreSight: Complete Data Engineering Pipeline\n",
    "\n",
    "**Author:** Prathamesh Fuke  \n",
    "**Project:** EPL Football Analytics & Prediction  \n",
    "**Branch:** Prathamesh_Fuke  \n",
    "**Date:** November 13, 2025\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook consolidates all data engineering tasks from the original 7 notebooks (01-07) into a single, streamlined pipeline.\n",
    "\n",
    "### Pipeline Components:\n",
    "1. **Data Loading & Inspection** - Load raw datasets (Match, Player, League)\n",
    "2. **Data Cleaning** - Handle missing values, duplicates, standardize columns\n",
    "3. **Basic Feature Engineering** - Create fundamental features\n",
    "4. **Advanced Feature Engineering** - Tier 1-3 features (Statistical, Contextual, Interactions)\n",
    "5. **Data Leakage Validation** - Identify and remove leakage columns\n",
    "6. **Encoding & Feature Selection** - Prepare final modeling datasets\n",
    "7. **Visualization** - EDA and feature analysis\n",
    "\n",
    "### Expected Outputs:\n",
    "- `../data/cleaned/` - Cleaned datasets\n",
    "- `../data/features/` - Basic engineered features\n",
    "- `../data/engineered/` - Advanced v3 features\n",
    "- `../data/corrected/` - Leakage-corrected datasets\n",
    "- `../data/final/` - Final modeling-ready data\n",
    "- `../visualizations/` - EDA charts\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65692762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, percentileofscore\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "\n",
    "# Create directory structure if not exists\n",
    "dirs = ['../data/raw', '../data/cleaned', '../data/features', '../data/engineered', \n",
    "        '../data/final', '../data/corrected', '../data/encoded', '../visualizations']\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "print(\"✓ Directory structure verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PART 1: DATA LOADING & INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Raw Datasets\n",
    "# Note: Replace paths with your actual file paths if different\n",
    "try:\n",
    "    print(\"Loading datasets...\")\n",
    "    match_data = pd.read_csv('../datasets/Match Winner.csv')\n",
    "    player_data = pd.read_excel('../datasets/Goals & Assist.xlsx')\n",
    "    league_data = pd.read_csv('../datasets/ScoreSight_ML_Season_LeagueWinner_Champion.csv')\n",
    "    \n",
    "    print(f\"✓ Match Data: {match_data.shape}\")\n",
    "    print(f\"✓ Player Data: {player_data.shape}\")\n",
    "    print(f\"✓ League Data: {league_data.shape}\")\n",
    "    \n",
    "    # Save raw copies for reference\n",
    "    match_data.to_csv('../data/raw/data_raw_match.csv', index=False)\n",
    "    player_data.to_csv('../data/raw/data_raw_player.csv', index=False)\n",
    "    league_data.to_csv('../data/raw/data_raw_league.csv', index=False)\n",
    "    print(\"✓ Raw datasets saved to ../data/raw/\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Source datasets not found. Please check paths.\")\n",
    "\n",
    "# Initial Inspection - Match Data\n",
    "print(\"\\nMatch Data - Missing Values:\")\n",
    "missing = match_data.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if not missing.empty:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e38223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Clean Match Data ---\n",
    "match_clean = match_data.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "dups = match_clean.duplicated().sum()\n",
    "if dups > 0:\n",
    "    match_clean = match_clean.drop_duplicates()\n",
    "    print(f\"Removed {dups} duplicates from Match Data.\")\n",
    "\n",
    "# Handle Missing Values\n",
    "numeric_cols = match_clean.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = match_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if match_clean[col].isnull().sum() > 0:\n",
    "        match_clean[col].fillna(match_clean[col].median(), inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if match_clean[col].isnull().sum() > 0:\n",
    "        match_clean[col].fillna(match_clean[col].mode()[0] if not match_clean[col].mode().empty else 'Unknown', inplace=True)\n",
    "\n",
    "# Standardize Columns\n",
    "match_clean.columns = match_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "\n",
    "# --- Clean Player Data ---\n",
    "player_clean = player_data.copy()\n",
    "if player_clean.duplicated().sum() > 0:\n",
    "    player_clean = player_clean.drop_duplicates()\n",
    "\n",
    "# Fill numeric missing in player data (assumed 0 for stats)\n",
    "num_cols_player = player_clean.select_dtypes(include=[np.number]).columns\n",
    "player_clean[num_cols_player] = player_clean[num_cols_player].fillna(0)\n",
    "\n",
    "player_clean.columns = player_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "\n",
    "# --- Clean League Data ---\n",
    "league_clean = league_data.copy()\n",
    "if league_clean.duplicated().sum() > 0:\n",
    "    league_clean = league_clean.drop_duplicates()\n",
    "league_clean.columns = league_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "\n",
    "# Save Cleaned Data\n",
    "match_clean.to_csv('../data/cleaned/data_cleaned_match.csv', index=False)\n",
    "player_clean.to_csv('../data/cleaned/data_cleaned_player.csv', index=False)\n",
    "league_clean.to_csv('../data/cleaned/data_cleaned_league.csv', index=False)\n",
    "\n",
    "print(f\"✓ Match Data Cleaned: {match_clean.shape}\")\n",
    "print(f\"✓ Player Data Cleaned: {player_clean.shape}\")\n",
    "print(f\"✓ League Data Cleaned: {league_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: BASIC FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Match Features ---\n",
    "match_features = match_clean.copy()\n",
    "\n",
    "# Example Basic Engineering (Customize based on specific column availability)\n",
    "# Creating simple goal difference if columns exist\n",
    "if 'htgs' in match_features.columns and 'htgc' in match_features.columns:\n",
    "    match_features['htgd'] = match_features['htgs'] - match_features['htgc']\n",
    "    print(\"Created 'htgd' (Home Team Goal Difference)\")\n",
    "\n",
    "if 'atgs' in match_features.columns and 'atgc' in match_features.columns:\n",
    "    match_features['atgd'] = match_features['atgs'] - match_features['atgc']\n",
    "    print(\"Created 'atgd' (Away Team Goal Difference)\")\n",
    "\n",
    "# --- Player Features ---\n",
    "player_features = player_clean.copy()\n",
    "# Basic ratio creation example\n",
    "if 'goals' in player_features.columns and 'matches_played' in player_features.columns:\n",
    "    # Avoid division by zero\n",
    "    player_features['goals_per_game'] = player_features['goals'] / player_features['matches_played'].replace(0, 1)\n",
    "    print(\"Created 'goals_per_game'\")\n",
    "\n",
    "# --- League Features ---\n",
    "league_features = league_clean.copy()\n",
    "\n",
    "# Save Feature Data\n",
    "match_features.to_csv('../data/features/data_features_match.csv', index=False)\n",
    "player_features.to_csv('../data/features/data_features_player.csv', index=False)\n",
    "league_features.to_csv('../data/features/data_features_league.csv', index=False)\n",
    "\n",
    "print(\"✓ Basic feature engineering completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c58b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: ADVANCED FEATURE ENGINEERING (Tier 1-3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We use the match_features dataframe from the previous step as input\n",
    "df_match_adv = match_features.copy()\n",
    "\n",
    "class AdvancedFeatureEngineering:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.feature_descriptions = {}\n",
    "        # Ensure date column exists and is sorted for rolling calculations\n",
    "        if 'date' in self.df.columns:\n",
    "            self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "            self.df = self.df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    # --- TIER 1: Statistical & Distributional ---\n",
    "    def add_tier1_features(self):\n",
    "        print(\"  > Generating Tier 1 (Statistical) features...\")\n",
    "        \n",
    "        # Rolling Window settings\n",
    "        window = 10\n",
    "        \n",
    "        # Percentile Features\n",
    "        if 'htgs' in self.df.columns and 'atgs' in self.df.columns:\n",
    "            all_goals = pd.concat([self.df['htgs'], self.df['atgs']])\n",
    "            self.df['home_goals_percentile'] = self.df['htgs'].apply(lambda x: percentileofscore(all_goals, x, nan_policy='omit'))\n",
    "            self.df['away_goals_percentile'] = self.df['atgs'].apply(lambda x: percentileofscore(all_goals, x, nan_policy='omit'))\n",
    "            \n",
    "        # Rolling Statistics (Skewness, Kurtosis)\n",
    "        # Note: Using 'hometeam' as grouping key. Adjust if column name differs.\n",
    "        if 'hometeam' in self.df.columns:\n",
    "            # Skewness\n",
    "            self.df['home_goals_skewness'] = self.df.groupby('hometeam')['htgs'].transform(\n",
    "                lambda x: x.rolling(window=window, min_periods=3).apply(skew, raw=False)\n",
    "            )\n",
    "            # Coefficient of Variation\n",
    "            home_cv = self.df.groupby('hometeam')['htgs'].transform(\n",
    "                lambda x: x.rolling(window=window, min_periods=3).std() / (x.rolling(window=window, min_periods=3).mean() + 0.1)\n",
    "            )\n",
    "            self.df['home_scoring_cv'] = home_cv.fillna(0)\n",
    "            \n",
    "            # Quantiles\n",
    "            for q in [0.25, 0.75]:\n",
    "                self.df[f'home_goals_q{int(q*100)}'] = self.df.groupby('hometeam')['htgs'].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=3).quantile(q)\n",
    "                )\n",
    "            self.df['home_goals_iqr'] = self.df['home_goals_q75'] - self.df['home_goals_q25']\n",
    "\n",
    "    # --- TIER 2: Market & Context ---\n",
    "    def add_tier2_features(self):\n",
    "        print(\"  > Generating Tier 2 (Contextual) features...\")\n",
    "        \n",
    "        # Team Quality Proxy (Historical Performance)\n",
    "        if 'hometeam' in self.df.columns and 'htp' in self.df.columns: # htp = home team points\n",
    "            team_quality = self.df.groupby('hometeam')['htp'].mean().to_dict()\n",
    "            quality_vals = list(team_quality.values())\n",
    "            if quality_vals:\n",
    "                q_min, q_max = min(quality_vals), max(quality_vals)\n",
    "                self.df['home_quality_score'] = self.df['hometeam'].map(\n",
    "                    lambda x: (team_quality.get(x, 0) - q_min) / (q_max - q_min + 1e-5)\n",
    "                )\n",
    "        \n",
    "        # Scheduling (Days Rest)\n",
    "        if 'date' in self.df.columns:\n",
    "            home_days = self.df.groupby('hometeam')['date'].transform(lambda x: x.diff().dt.days)\n",
    "            self.df['home_days_rest'] = home_days.fillna(7)\n",
    "            self.df['is_midweek'] = self.df['date'].dt.weekday.isin([0, 1, 2, 3]).astype(int)\n",
    "\n",
    "    # --- TIER 3: Interactions & Non-Linear ---\n",
    "    def add_tier3_features(self):\n",
    "        print(\"  > Generating Tier 3 (Interaction) features...\")\n",
    "        \n",
    "        # Log Transforms\n",
    "        if 'htgs' in self.df.columns:\n",
    "            self.df['home_goals_log'] = np.log1p(self.df['htgs'].fillna(0))\n",
    "            \n",
    "        # Efficiency Ratios\n",
    "        if 'htgs' in self.df.columns and 'htgc' in self.df.columns:\n",
    "            self.df['home_offensive_efficiency'] = self.df['htgs'] / (self.df['htgs'] + self.df['htgc'] + 1)\n",
    "            \n",
    "        # Composite Index (Weighted average of form, goals, defense)\n",
    "        # Simplified version for stability\n",
    "        if {'htformpts', 'htgs', 'htgc'}.issubset(self.df.columns):\n",
    "            scaler = MinMaxScaler()\n",
    "            # Check if columns have data before scaling\n",
    "            if not self.df[['htformpts', 'htgs', 'htgc']].isnull().all().any():\n",
    "                try:\n",
    "                    comps = np.column_stack([\n",
    "                        self.df['htformpts'].fillna(0),\n",
    "                        self.df['htgs'].fillna(0),\n",
    "                        -self.df['htgc'].fillna(0) # Negative because lower conceded is better\n",
    "                    ])\n",
    "                    comps_norm = scaler.fit_transform(comps)\n",
    "                    weights = np.array([0.4, 0.3, 0.3])\n",
    "                    self.df['home_strength_index'] = np.dot(comps_norm, weights)\n",
    "                except Exception as e:\n",
    "                    print(f\"    ! Could not calculate strength index: {e}\")\n",
    "\n",
    "    def execute(self):\n",
    "        self.add_tier1_features()\n",
    "        self.add_tier2_features()\n",
    "        self.add_tier3_features()\n",
    "        return self.df\n",
    "\n",
    "# Execute Advanced Engineering\n",
    "# We check if sufficient columns exist to run this; otherwise we skip to avoid errors in empty demo datasets\n",
    "required_cols = ['htgs', 'atgs', 'hometeam', 'date']\n",
    "if set(required_cols).issubset(df_match_adv.columns):\n",
    "    engineer = AdvancedFeatureEngineering(df_match_adv)\n",
    "    match_engineered = engineer.execute()\n",
    "    \n",
    "    # Save\n",
    "    match_engineered.to_csv('../data/engineered/data_engineered_match_v3.csv', index=False)\n",
    "    print(f\"✓ Advanced features created. New Shape: {match_engineered.shape}\")\n",
    "else:\n",
    "    print(\"! Skipping Advanced Engineering: Required columns (htgs, atgs, hometeam, date) not found.\")\n",
    "    match_engineered = match_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec25743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: DATA LEAKAGE VALIDATION & CORRECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define Safe Features (Pre-match available only)\n",
    "# Matches shouldn't use Full Time Results (FTR) or Goals to predict themselves unless they are lag/rolling features\n",
    "SAFE_MATCH_FEATURES = [\n",
    "    'htgs', 'atgs', 'htgc', 'atgc', 'htp', 'atp', 'mw', \n",
    "    'htformpts', 'atformpts', 'htstreak', 'atstreak',\n",
    "    'home_days_rest', 'home_quality_score', 'home_strength_index',\n",
    "    'home_goals_log', 'home_offensive_efficiency' # Tier 3 features\n",
    "]\n",
    "\n",
    "# Define Targets\n",
    "TARGET_COLS = ['ftr', 'match_result', 'fthg', 'ftag'] # Full Time Result, Goals\n",
    "\n",
    "def validate_and_clean_leakage(df, dataset_name, safe_list, target_list):\n",
    "    print(f\"\\nValidating {dataset_name}...\")\n",
    "    \n",
    "    df_cols = df.columns.tolist()\n",
    "    leakage_candidates = []\n",
    "    \n",
    "    # Check for columns that look like future info but aren't in safe list or target list\n",
    "    # (Simple heuristic based on column names)\n",
    "    for col in df_cols:\n",
    "        col_lower = col.lower()\n",
    "        # If it looks like a result but isn't a historical/rolling stat\n",
    "        if ('result' in col_lower or 'winner' in col_lower or 'points_per_game' in col_lower) \\\n",
    "           and col not in safe_list and col not in target_list:\n",
    "             # Exclude rolling/lagged features usually denoted by prefixes or suffixes\n",
    "             if not any(x in col_lower for x in ['last', 'prev', 'rolling', 'mean', 'lag']):\n",
    "                 leakage_candidates.append(col)\n",
    "    \n",
    "    if leakage_candidates:\n",
    "        print(f\"  ! Potential leakage columns detected: {leakage_candidates}\")\n",
    "    else:\n",
    "        print(\"  ✓ No obvious leakage columns found based on heuristics.\")\n",
    "        \n",
    "    # Create Corrected Dataset (Keep Safe + Target)\n",
    "    # We keep all columns, but explicit \"Keep\" list ensures we strictly separate X and y later\n",
    "    # Here we just remove blatant leakage if identified\n",
    "    \n",
    "    # Specific Fix for League Data (from NB 06 logic)\n",
    "    if dataset_name == \"League Data\" and 'points_per_game' in df.columns:\n",
    "        print(\"  ! Removing 'points_per_game' (Derived from target)\")\n",
    "        df = df.drop(columns=['points_per_game'])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply Validation\n",
    "match_corrected = validate_and_clean_leakage(match_engineered, \"Match Data\", SAFE_MATCH_FEATURES, TARGET_COLS)\n",
    "league_corrected = validate_and_clean_leakage(league_features, \"League Data\", [], ['target_total_points', 'target_league_position'])\n",
    "player_corrected = validate_and_clean_leakage(player_features, \"Player Data\", [], ['goals', 'assists'])\n",
    "\n",
    "# Save Corrected Data\n",
    "match_corrected.to_csv('../data/corrected/match_prediction_corrected.csv', index=False)\n",
    "league_corrected.to_csv('../data/corrected/league_winner_corrected.csv', index=False)\n",
    "player_corrected.to_csv('../data/corrected/top_scorer_corrected.csv', index=False)\n",
    "\n",
    "print(\"✓ Datasets validated and saved to ../data/corrected/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: ENCODING & FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Process Match Data\n",
    "match_final = match_corrected.copy()\n",
    "\n",
    "# 1. Encode Categorical Variables\n",
    "cat_cols = match_final.select_dtypes(include=['object']).columns.tolist()\n",
    "# Exclude Date\n",
    "if 'date' in cat_cols: cat_cols.remove('date')\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    # Only encode if low cardinality, otherwise likely identifiers\n",
    "    if match_final[col].nunique() < 50:\n",
    "        le = LabelEncoder()\n",
    "        match_final[f'{col}_encoded'] = le.fit_transform(match_final[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  Encoded {col}\")\n",
    "\n",
    "# 2. Select Numeric Features for Modeling\n",
    "# We exclude the raw categorical columns and identifiers\n",
    "numeric_final = match_final.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# 3. Drop Target from Features (if it exists in numeric form) but keep for y\n",
    "if 'fthg' in numeric_final.columns:\n",
    "    # Assuming fthg (Full Time Home Goals) is a target, we shouldn't use it as input\n",
    "    # But we might need it for correlation analysis in Part 7\n",
    "    pass \n",
    "\n",
    "# Save Final Modeling Datasets\n",
    "match_final.to_csv('../data/final/data_final_match_prediction.csv', index=False)\n",
    "# (Repeat for player/league if needed, usually match is the primary modeling target)\n",
    "\n",
    "print(f\"✓ Final Match Dataset Ready: {match_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 7: VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Correlation Heatmap\n",
    "numeric_cols = match_final.select_dtypes(include=[np.number]).columns\n",
    "# Filter to interesting columns to avoid clutter\n",
    "interesting_cols = [c for c in numeric_cols if 'encoded' in c or c in SAFE_MATCH_FEATURES or c in ['fthg', 'ftag']]\n",
    "\n",
    "if len(interesting_cols) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = match_final[interesting_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "    plt.title('Match Data - Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/viz_match_correlation.png')\n",
    "    print(\"✓ Saved Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Feature Distributions (Tier 1-3 Features)\n",
    "viz_features = ['home_goals_percentile', 'home_scoring_cv', 'home_strength_index', 'home_quality_score']\n",
    "valid_viz = [f for f in viz_features if f in match_final.columns]\n",
    "\n",
    "if valid_viz:\n",
    "    fig, axes = plt.subplots(1, len(valid_viz), figsize=(16, 5))\n",
    "    if len(valid_viz) == 1: axes = [axes]\n",
    "    \n",
    "    for idx, feature in enumerate(valid_viz):\n",
    "        data = match_final[feature].dropna()\n",
    "        axes[idx].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        axes[idx].set_title(feature, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Value')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/viz_feature_distributions.png')\n",
    "    print(\"✓ Saved Feature Distributions\")\n",
    "    plt.show()\n",
    "\n",
    "# 3. Summary Dashboard\n",
    "print(\"\\nData Pipeline Summary:\")\n",
    "print(f\"  Total Records Processed: {match_final.shape[0]}\")\n",
    "print(f\"  Total Features Generated: {match_final.shape[1]}\")\n",
    "print(f\"  Final data saved to: ../data/final/data_final_match_prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
