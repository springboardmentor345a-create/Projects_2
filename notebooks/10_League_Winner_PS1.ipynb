{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc1d004",
   "metadata": {},
   "source": [
    "# PS1: League Winner Prediction - Rigorous Retraining\n",
    "\n",
    "## âš ï¸ CRITICAL CORRECTION: ADDRESSING DATA LEAKAGE\n",
    "\n",
    "My previous analysis was flawed and produced misleading \"perfect\" scores. This was due to **severe data leakage**, where features that are derived from the target variable were used for training. This is a major methodological error.\n",
    "\n",
    "**The primary source of leakage was using `points_per_game` as a feature, which is directly calculated from `target_total_points`.**\n",
    "\n",
    "This notebook has been completely rewritten to follow proper machine learning best practices.\n",
    "\n",
    "### Corrected Workflow:\n",
    "1.  **Isolate Target:** The target variable (`target_champion`) and any derivatives are strictly used for evaluation, not as features.\n",
    "2.  **Eliminate Leaky Features:** All columns that would not be available *before* a season starts are removed from the feature set.\n",
    "3.  **Temporal Validation:** The data is split by season to simulate a real-world prediction scenario (training on past seasons to predict a future one).\n",
    "4.  **Realistic Evaluation:** Performance is measured on a hold-out test set of the most recent season(s), providing an honest assessment of the model's capabilities.\n",
    "\n",
    "### Prediction Goal\n",
    "- **Predict the league champion (`target_champion` = 1) for a given season.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f9ef8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc72272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:56:01.637323Z",
     "iopub.status.busy": "2025-11-13T09:56:01.637116Z",
     "iopub.status.idle": "2025-11-13T09:56:02.796250Z",
     "shell.execute_reply": "2025-11-13T09:56:02.795395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (180, 10)\n",
      "Columns: ['matches_played', 'points_per_game', 'goals_scored', 'goals_conceded', 'goal_difference', 'target_total_points', 'target_league_position', 'target_champion', 'season_encoded', 'team_encoded']\n",
      "\n",
      "--- Feature and Target Definition ---\n",
      "Target column: 'target_champion'\n",
      "Feature columns (4): ['matches_played', 'goals_scored', 'goals_conceded', 'goal_difference']\n",
      "Leakage columns removed: ['points_per_game', 'target_total_points', 'target_league_position', 'target_champion']\n",
      "\n",
      "Features (X) shape: (180, 4)\n",
      "Target (y) shape: (180,)\n",
      "Champion distribution:\n",
      "target_champion\n",
      "0    0.95\n",
      "1    0.05\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import XGBoost and LightGBM if available\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = False # Disabled for now to ensure robust, non-leaky models first\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_URL = '../data/final/data_final_points_tally.csv'\n",
    "TARGET_COL = 'target_champion'\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_URL)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# --- CRITICAL: Define Leaky and Unusable Columns ---\n",
    "# These columns contain information about the outcome of the season, so they cannot be used as features.\n",
    "LEAKAGE_COLS = [\n",
    "    'points_per_game',          # Directly calculated from target_total_points\n",
    "    'target_total_points',      # A target variable itself\n",
    "    'target_league_position',   # Another target variable\n",
    "    'target_champion'           # The primary target for this problem statement\n",
    "]\n",
    "\n",
    "# Define feature columns - exclude leakage and identifiers\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in LEAKAGE_COLS and col not in ['season_encoded', 'team_encoded']\n",
    "]\n",
    "\n",
    "print(\"\\n--- Feature and Target Definition ---\")\n",
    "print(f\"Target column: '{TARGET_COL}'\")\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"Leakage columns removed: {LEAKAGE_COLS}\")\n",
    "\n",
    "# --- Prepare Data for Temporal Validation ---\n",
    "# Sort data by season to ensure correct temporal order\n",
    "df_sorted = df.sort_values('season_encoded').reset_index(drop=True)\n",
    "\n",
    "X = df_sorted[feature_cols]\n",
    "y = df_sorted[TARGET_COL]\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"Champion distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb708d",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7418a28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:56:02.797928Z",
     "iopub.status.busy": "2025-11-13T09:56:02.797645Z",
     "iopub.status.idle": "2025-11-13T09:56:02.805018Z",
     "shell.execute_reply": "2025-11-13T09:56:02.804525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available seasons: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "--- Temporal Split Details ---\n",
      "Training seasons: [0 1 2 3 4 5 6 7]\n",
      "Test season: 8\n",
      "Training set size: 160 samples\n",
      "Test set size: 20 samples\n",
      "Champions in training set: 8\n",
      "Champions in test set: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Temporal Train-Test Split ---\n",
    "# We must train on past data and test on future data.\n",
    "\n",
    "seasons = df_sorted['season_encoded'].unique()\n",
    "print(f\"Available seasons: {seasons}\")\n",
    "\n",
    "# Use the last season for testing, and all prior seasons for training\n",
    "test_season = seasons[-1]\n",
    "train_seasons = seasons[:-1]\n",
    "\n",
    "train_mask = df_sorted['season_encoded'].isin(train_seasons)\n",
    "test_mask = df_sorted['season_encoded'] == test_season\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "print(\"\\n--- Temporal Split Details ---\")\n",
    "print(f\"Training seasons: {train_seasons}\")\n",
    "print(f\"Test season: {test_season}\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Champions in training set: {y_train.sum()}\")\n",
    "print(f\"Champions in test set: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df3e44",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0999326f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:56:02.806404Z",
     "iopub.status.busy": "2025-11-13T09:56:02.806243Z",
     "iopub.status.idle": "2025-11-13T09:56:02.813932Z",
     "shell.execute_reply": "2025-11-13T09:56:02.813421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Models with Temporal CV (5 splits) ---\n",
      "Training GradientBoosting...\n",
      "  Done. Best F1 (macro) on CV: 0.7365\n",
      "Training RandomForest...\n",
      "  Done. Best F1 (macro) on CV: 0.7365\n",
      "Training RandomForest...\n",
      "  Done. Best F1 (macro) on CV: 0.8186\n",
      "Training XGBoost...\n",
      "  Done. Best F1 (macro) on CV: 0.8186\n",
      "Training XGBoost...\n",
      "  Done. Best F1 (macro) on CV: 0.7367\n",
      "\n",
      "--- Model Performance on Hold-out Test Set ---\n",
      "\n",
      "----- GradientBoosting -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n",
      "\n",
      "----- RandomForest -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n",
      "\n",
      "----- XGBoost -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n",
      "  Done. Best F1 (macro) on CV: 0.7367\n",
      "\n",
      "--- Model Performance on Hold-out Test Set ---\n",
      "\n",
      "----- GradientBoosting -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n",
      "\n",
      "----- RandomForest -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n",
      "\n",
      "----- XGBoost -----\n",
      "  Accuracy: 0.9500\n",
      "  F1 (Macro): 0.4872\n",
      "  ROC AUC: 0.9474\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.47      0.50      0.49        20\n",
      "weighted avg       0.90      0.95      0.93        20\n",
      "\n",
      "  Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 1  0]]\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training Pipeline with Temporal Cross-Validation ---\n",
    "\n",
    "# Define a robust pipeline\n",
    "def create_pipeline(model):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "}\n",
    "if XGB_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grids = {\n",
    "    'GradientBoosting': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__max_depth': [3, 5]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [5, 10, None],\n",
    "        'model__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__max_depth': [3, 5],\n",
    "        'model__scale_pos_weight': [len(y_train[y_train==0]) / len(y_train[y_train==1])]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation to respect temporal order\n",
    "n_splits = min(len(train_seasons) - 1, 5) # Cannot have more splits than seasons-1\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "print(f\"\\n--- Training Models with Temporal CV ({n_splits} splits) ---\")\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline = create_pipeline(model)\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=10,\n",
    "        cv=tscv,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on the hold-out test set\n",
    "    y_pred = search.predict(X_test)\n",
    "    y_proba = search.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': search.best_estimator_,\n",
    "        'best_params': search.best_params_,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "        'report': classification_report(y_test, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    print(f\"  Done. Best F1 (macro) on CV: {search.best_score_:.4f}\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Model Performance on Hold-out Test Set ---\")\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n----- {name} -----\")\n",
    "    print(f\"  Accuracy: {res['accuracy']:.4f}\")\n",
    "    print(f\"  F1 (Macro): {res['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC AUC: {res['roc_auc']:.4f}\")\n",
    "    print(\"  Classification Report:\")\n",
    "    print(res['report'])\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(res['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3f8e8",
   "metadata": {},
   "source": [
    "## 4. Define Pipeline and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151c46e",
   "metadata": {},
   "source": [
    "## 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528a14a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T10:00:06.617745Z",
     "iopub.status.busy": "2025-11-13T10:00:06.617473Z",
     "iopub.status.idle": "2025-11-13T10:00:06.632081Z",
     "shell.execute_reply": "2025-11-13T10:00:06.631272Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Test_F1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_18444\\902777219.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compare all models\u001b[39;00m\n\u001b[32m      2\u001b[39m results_df = pd.DataFrame(results)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results_df = results_df.sort_values(\u001b[33m'Test_F1'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m print(\u001b[33m\"\\n\"\u001b[39m + \u001b[33m\"=\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      6\u001b[39m print(\u001b[33m\"ðŸ“Š MODEL COMPARISON - PS1: League Top-4 Classification\"\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7207\u001b[39m             )\n\u001b[32m   7208\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7209\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7210\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7211\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7212\u001b[39m \n\u001b[32m   7213\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7214\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Test_F1'"
     ]
    }
   ],
   "source": [
    "# --- Save the Best Model ---\n",
    "# Select the best model based on F1-score on the test set\n",
    "best_model_name = max(results, key=lambda name: results[name]['f1_macro'])\n",
    "best_model_data = results[best_model_name]\n",
    "best_model = best_model_data['model']\n",
    "\n",
    "print(f\"\\n--- Saving Best Model: {best_model_name} ---\")\n",
    "print(f\"Test F1 (Macro): {best_model_data['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up old model files\n",
    "for old_file in MODELS_DIR.glob('ps1_league_winner_*'):\n",
    "    old_file.unlink()\n",
    "\n",
    "# Save model and metadata\n",
    "model_filename = MODELS_DIR / 'ps1_league_winner_best_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "metadata = {\n",
    "    'problem_statement': 'PS1: League Winner Prediction',\n",
    "    'model_type': best_model_name,\n",
    "    'pipeline_steps': [step[0] for step in best_model.steps],\n",
    "    'performance_metrics': {\n",
    "        'accuracy': best_model_data['accuracy'],\n",
    "        'f1_macro': best_model_data['f1_macro'],\n",
    "        'roc_auc': best_model_data['roc_auc']\n",
    "    },\n",
    "    'features': feature_cols,\n",
    "    'target': TARGET_COL,\n",
    "    'temporal_split': {\n",
    "        'train_seasons': train_seasons.tolist(),\n",
    "        'test_season': int(test_season)\n",
    "    },\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_filename = MODELS_DIR / 'ps1_league_winner_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"Model saved to: {model_filename}\")\n",
    "print(f\"Metadata saved to: {metadata_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a194d",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
