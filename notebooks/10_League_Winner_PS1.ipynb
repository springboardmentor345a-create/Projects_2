{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc1d004",
   "metadata": {},
   "source": [
    "# Problem Statement 1: League Winner Prediction\n",
    "## Predict the League Champion/Winner\n",
    "\n",
    "**Author:** ScoreSight ML Team  \n",
    "**Date:** 2025-11-12  \n",
    "**Problem Type:** Classification (League Champion/Position Prediction)\n",
    "\n",
    "### Dataset\n",
    "- **File:** `data/data_engineered_league_points.csv`\n",
    "- **Task:** Predict league winner and final positions\n",
    "- **Features:** Team statistics, performance metrics (22+ engineered features)\n",
    "- **Target:** League champion indicator or final position\n",
    "\n",
    "### Pipeline\n",
    "1. Load and explore dataset\n",
    "2. Preprocess features and target\n",
    "3. Train classification models with hyperparameter tuning\n",
    "4. Evaluate with cross-validation\n",
    "5. Save best models to `models/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f9ef8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc72272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# For Colab: Base directory is /content/drive/MyDrive/ScoreSight\n",
    "SCORESIGHT_DIR = '/content/drive/MyDrive/ScoreSight'\n",
    "DATA_DIR = os.path.join(SCORESIGHT_DIR, 'data')\n",
    "MODELS_DIR = os.path.join(SCORESIGHT_DIR, 'models')\n",
    "\n",
    "# Create models directory\n",
    "Path(MODELS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"[OK] All libraries imported\")\n",
    "print(f\"XGBoost: {XGB_AVAILABLE} | LightGBM: {LGB_AVAILABLE}\")\n",
    "print(f\"ScoreSight directory: {SCORESIGHT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb708d",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what's actually in your Google Drive\n",
    "print(\"Checking Google Drive structure...\")\n",
    "print(\"\\n1. Contents of /content/drive/MyDrive/:\")\n",
    "try:\n",
    "    mydrive_contents = os.listdir('/content/drive/MyDrive/')\n",
    "    for item in sorted(mydrive_contents):\n",
    "        item_path = os.path.join('/content/drive/MyDrive/', item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  ðŸ“ {item}/\")\n",
    "        else:\n",
    "            print(f\"  ðŸ“„ {item}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print(\"\\n2. Looking for ScoreSight folder...\")\n",
    "scoresight_path = '/content/drive/MyDrive/ScoreSight'\n",
    "if os.path.exists(scoresight_path):\n",
    "    print(f\"âœ… Found: {scoresight_path}\")\n",
    "    print(\"\\n3. Contents of ScoreSight/:\")\n",
    "    for item in sorted(os.listdir(scoresight_path)):\n",
    "        item_path = os.path.join(scoresight_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  ðŸ“ {item}/\")\n",
    "        else:\n",
    "            print(f\"  ðŸ“„ {item}\")\n",
    "else:\n",
    "    print(f\"âŒ NOT FOUND: {scoresight_path}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. Is your Google Drive mounted? (You should see the drive icon)\")\n",
    "    print(\"2. What is the EXACT name of your ScoreSight folder in Google Drive?\")\n",
    "    print(\"3. Did you upload to 'My Drive' or somewhere else?\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Once we find the correct path, we'll load the data.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df3e44",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column (usually champion or winner)\n",
    "target_col = None\n",
    "for col in df.columns:\n",
    "    if any(x in col.lower() for x in ['champion', 'winner', 'position', 'rank']):\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    target_col = df.columns[-1]  # Last column as fallback\n",
    "\n",
    "print(f\"[TARGET] Using column: {target_col}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_cols = [target_col, 'unnamed: 0', 'team', 'season', 'team_name', 'league']\n",
    "feature_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols and df[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Handle missing values in features\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Encode target if necessary\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    print(f\"Target classes: {list(le.classes_)}\")\n",
    "else:\n",
    "    y_encoded = y.values\n",
    "    le = None\n",
    "\n",
    "print(f\"\\n[OK] Features: {len(feature_cols)}\")\n",
    "print(f\"[OK] Samples: {len(X)}\")\n",
    "print(f\"[OK] Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3f8e8",
   "metadata": {},
   "source": [
    "## 4. Define Pipeline and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6df725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model, use_scaling=True):\n",
    "    steps = [('imputer', SimpleImputer(strategy='mean'))]\n",
    "    if use_scaling:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', model))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    models['LightGBM'] = lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "print(f\"[OK] {len(models)} models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151c46e",
   "metadata": {},
   "source": [
    "## 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n[{name}] Training...\")\n",
    "    \n",
    "    pipeline = create_pipeline(model, use_scaling=(name == 'LogisticRegression'))\n",
    "    pipeline.fit(X, y_encoded)\n",
    "    \n",
    "    # Evaluate\n",
    "    scores = []\n",
    "    for train_idx, test_idx in cv.split(X, y_encoded):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "        \n",
    "        temp_pipeline = create_pipeline(model.__class__(**model.get_params()), use_scaling=(name == 'LogisticRegression'))\n",
    "        temp_pipeline.fit(X_train, y_train)\n",
    "        y_pred = temp_pipeline.predict(X_test)\n",
    "        scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    results[name] = {'f1': np.mean(scores), 'f1_std': np.std(scores)}\n",
    "    trained_models[name] = pipeline\n",
    "    \n",
    "    print(f\"  F1-weighted: {results[name]['f1']:.4f} (+/- {results[name]['f1_std']:.4f})\")\n",
    "\n",
    "print(\"\\n[OK] All models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a194d",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53041f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(results).T.sort_values('f1', ascending=False)\n",
    "comparison_df.columns = ['F1-Score', 'Std Dev']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[RESULTS] Model Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df)\n",
    "\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
    "print(f\"\\n[BEST] {best_model_name} (F1: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2409400",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(MODELS_DIR)\n",
    "models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    model_path = models_dir / f\"league_winner_{name.lower().replace(' ', '_')}_ps1.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"[SAVE] {name} -> {model_path}\")\n",
    "\n",
    "if le is not None:\n",
    "    le_path = models_dir / \"league_winner_encoder_ps1.joblib\"\n",
    "    joblib.dump(le, le_path)\n",
    "    print(f\"[SAVE] Label Encoder -> {le_path}\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'problem_statement': 'League Winner Prediction',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_model': best_model_name,\n",
    "    'best_f1': float(best_f1),\n",
    "    'n_features': len(feature_cols),\n",
    "    'n_samples': len(X),\n",
    "    'models': {name: {'f1': float(results[name]['f1']), 'model_path': str(models_dir / f\"league_winner_{name.lower().replace(' ', '_')}_ps1.joblib\")} for name in results.keys()}\n",
    "}\n",
    "\n",
    "summary_path = models_dir / \"league_winner_training_summary_ps1.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n[SAVE] Summary -> {summary_path}\")\n",
    "print(\"\\n[COMPLETE] League Winner Prediction Training Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
