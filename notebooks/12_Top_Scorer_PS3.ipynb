{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3dadfac",
   "metadata": {},
   "source": [
    "# PS3: Top Scorer Prediction (Corrected)\n",
    "\n",
    "**Objective:** Predict the number of goals a player will score in a season.\n",
    "\n",
    "**Methodology Correction:**\n",
    "The previous version of this model used a standard K-Fold cross-validation, which is inappropriate for time-series data and leads to data leakage and overly optimistic results. This corrected version implements a strict temporal validation strategy.\n",
    "\n",
    "1.  **Data Source:** We will use the pre-processed `data_final_top_scorer.csv` file.\n",
    "2.  **Temporal Splitting:** The data is sorted by season. The final season in the dataset will be used as a hold-out test set, while all prior seasons will be used for training. This simulates a real-world scenario where we predict a future season based on past data.\n",
    "3.  **Cross-Validation:** `TimeSeriesSplit` will be used for hyperparameter tuning to ensure that the validation folds respect the chronological order of the data.\n",
    "4.  **Evaluation:** The model will be evaluated on the unseen test set (the final season) using regression metrics like R-squared, MAE, and RMSE.\n",
    "5.  **Model Saving:** The best-performing model and its metadata, including the realistic performance metrics, will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7b77f",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e8f2d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'season_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_15456\\3723720573.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m os.makedirs(MODEL_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Load and sort the data\u001b[39;00m\n\u001b[32m     42\u001b[39m df = pd.read_csv(DATA_FILE)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df = df.sort_values(by=[\u001b[33m'season_encoded'\u001b[39m, \u001b[33m'player_encoded'\u001b[39m]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# --- Feature and Target Definition ---\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Target: 'goals_scored' - The number of goals a player scored in a season.\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7190\u001b[39m                 f\"Length of ascending ({len(ascending)})\"  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   7191\u001b[39m                 f\" != length of by ({len(by)})\"\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(by) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7194\u001b[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m by]\n\u001b[32m   7195\u001b[39m \n\u001b[32m   7196\u001b[39m             \u001b[38;5;66;03m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[32m   7197\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'season_encoded'"
     ]
    }
   ],
   "source": [
    "# --- Corrected Data Loading and Setup ---\n",
    "# Objective: Load the dataset, define features and target, and prepare for temporal validation.\n",
    "# Methodology: We will use 'data_final_top_scorer.csv', which is pre-processed.\n",
    "# The data will be sorted by season to ensure chronological order before splitting.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Check for XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# Define file paths\n",
    "DATA_FILE = '../data/final/data_final_top_scorer.csv'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_NAME = 'ps3_top_scorer_best_model.joblib'\n",
    "METADATA_NAME = 'ps3_top_scorer_metadata.json'\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Load and sort the data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df = df.sort_values(by=['season_encoded', 'player_encoded']).reset_index(drop=True)\n",
    "\n",
    "# --- Feature and Target Definition ---\n",
    "# Target: 'goals_scored' - The number of goals a player scored in a season.\n",
    "# Features: All other relevant columns, excluding identifiers and the target.\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df['goals_scored']\n",
    "X = df.drop(columns=['player', 'goals_scored'])\n",
    "\n",
    "print(\"Data loaded and sorted successfully.\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"Data spans from season {df['season_encoded'].min()} to {df['season_encoded'].max()}\")\n",
    "print(f\"XGBoost available: {XGB_AVAILABLE}\")\n",
    "print(f\"LightGBM available: {LGB_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bdd95",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1db94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loading from local file: ../data/corrected/top_scorer_corrected.csv\n",
      "[OK] Shape: (2070, 21)\n",
      "\n",
      "Columns: ['age', 'matches_played', 'goals', 'assists', 'non_penalty_goals', 'penalty_goals_made', 'penalty_attempts', 'xg', 'npxg', 'xag', 'npxg_+_xag', 'goals_per_90', 'assists_per_90', 'goals_+_assists_per_90', 'non_penalty_goals_per_90', 'xg_per_90', 'xag_per_90', 'npxg_per_90', 'player_encoded', 'nation_encoded', 'position_encoded']\n",
      "\n",
      "ðŸ“Š Target (goals) Statistics:\n",
      "  Mean: 1.47 goals\n",
      "  Std: 2.95\n",
      "  Min: 0, Max: 29\n",
      "  Median: 0.0\n",
      "\n",
      "[OK] Features (16): ['age', 'matches_played', 'assists', 'penalty_attempts', 'xg', 'npxg', 'xag', 'npxg_+_xag', 'goals_per_90', 'assists_per_90', 'goals_+_assists_per_90', 'non_penalty_goals_per_90', 'xg_per_90', 'xag_per_90', 'npxg_per_90', 'position_encoded']\n",
      "[OK] Samples: 2070\n",
      "\n",
      "[SPLIT] Train: 1656, Test: 414\n",
      "[SPLIT] Train target stats: Mean=1.54, Std=3.07\n",
      "[SPLIT] Test target stats: Mean=1.22, Std=2.43\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Use GitHub URL if on Colab, local path otherwise\n",
    "import os\n",
    "if os.path.exists('../data/corrected/top_scorer_corrected.csv'):\n",
    "    data_path = '../data/corrected/top_scorer_corrected.csv'\n",
    "    print(f\"[LOAD] Loading from local file: {data_path}\")\n",
    "else:\n",
    "    data_path = 'https://raw.githubusercontent.com/springboardmentor345a-create/Projects_2/Prathamesh_Fuke/data/corrected/top_scorer_corrected.csv'\n",
    "    print(f\"[LOAD] Loading from GitHub: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "print(f\"[OK] Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nðŸ“Š Target (goals) Statistics:\")\n",
    "print(f\"  Mean: {df['goals'].mean():.2f} goals\")\n",
    "print(f\"  Std: {df['goals'].std():.2f}\")\n",
    "print(f\"  Min: {df['goals'].min()}, Max: {df['goals'].max()}\")\n",
    "print(f\"  Median: {df['goals'].median():.1f}\")\n",
    "\n",
    "# Prepare features - exclude targets and identifiers\n",
    "exclude_cols = ['goals', 'non_penalty_goals', 'penalty_goals_made',\n",
    "                'player_encoded', 'nation_encoded', 'pos_encoded', \n",
    "                'unnamed:_0', 'player', 'nation', 'position']\n",
    "feature_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols and df[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['goals'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"\\nâš ï¸  Found {X.isnull().sum().sum()} missing values - filling with column means\")\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "print(f\"\\n[OK] Features ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"[OK] Samples: {len(X)}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n[SPLIT] Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"[SPLIT] Train target stats: Mean={y_train.mean():.2f}, Std={y_train.std():.2f}\")\n",
    "print(f\"[SPLIT] Test target stats: Mean={y_test.mean():.2f}, Std={y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f40339",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876702f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPLIT] Train: 1656, Test: 414\n",
      "[SPLIT] Train target - Mean: 1.54, Std: 3.07\n",
      "[SPLIT] Test target  - Mean: 1.22, Std: 2.43\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING - RandomizedSearchCV (Regression)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[Ridge] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "[BEST] Params: {'model__alpha': 1}\n",
      "[CV] MAE: 0.5273\n",
      "\n",
      "[TEST] MAE:  0.4856\n",
      "[TEST] RMSE: 0.7956\n",
      "[TEST] RÂ²:   0.8922\n",
      "[TEST] Residuals - Mean: -0.0842, Std: 0.7921\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[Lasso] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "[BEST] Params: {'model__alpha': 0.001}\n",
      "[CV] MAE: 0.5283\n",
      "\n",
      "[TEST] MAE:  0.4851\n",
      "[TEST] RMSE: 0.7942\n",
      "[TEST] RÂ²:   0.8926\n",
      "[TEST] Residuals - Mean: -0.0841, Std: 0.7907\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[RandomForest] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[BEST] Params: {'model__n_estimators': 100, 'model__min_samples_split': 2, 'model__min_samples_leaf': 1, 'model__max_depth': 15}\n",
      "[CV] MAE: 0.2574\n",
      "\n",
      "[TEST] MAE:  0.1864\n",
      "[TEST] RMSE: 0.5017\n",
      "[TEST] RÂ²:   0.9571\n",
      "[TEST] Residuals - Mean: -0.0067, Std: 0.5023\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[GradientBoosting] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[BEST] Params: {'model__subsample': 0.9, 'model__n_estimators': 100, 'model__max_depth': 5, 'model__learning_rate': 0.1}\n",
      "[CV] MAE: 0.2017\n",
      "\n",
      "[TEST] MAE:  0.1443\n",
      "[TEST] RMSE: 0.3685\n",
      "[TEST] RÂ²:   0.9769\n",
      "[TEST] Residuals - Mean: -0.0032, Std: 0.3690\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[XGBoost] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[BEST] Params: {'model__subsample': 0.8, 'model__n_estimators': 200, 'model__max_depth': 7, 'model__learning_rate': 0.05}\n",
      "[CV] MAE: 0.1909\n",
      "\n",
      "[TEST] MAE:  0.1363\n",
      "[TEST] RMSE: 0.3656\n",
      "[TEST] RÂ²:   0.9772\n",
      "[TEST] Residuals - Mean: -0.0121, Std: 0.3658\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[LightGBM] Hyperparameter Tuning...\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "[BEST] Params: {'model__num_leaves': 50, 'model__n_estimators': 200, 'model__learning_rate': 0.05}\n",
      "[CV] MAE: 0.2121\n",
      "\n",
      "[TEST] MAE:  0.1540\n",
      "[TEST] RMSE: 0.3887\n",
      "[TEST] RÂ²:   0.9743\n",
      "[TEST] Residuals - Mean: -0.0313, Std: 0.3879\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"[SPLIT] Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"[SPLIT] Train target - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"[SPLIT] Test target  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER SEARCH SPACE - Top Scorer Regression\n",
    "# ============================================================================\n",
    "\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model__alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__max_depth': [10, 15, 20, None],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__max_depth': [5, 7, 9],\n",
    "        'model__subsample': [0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__max_depth': [5, 7, 9],\n",
    "        'model__subsample': [0.8, 0.9]\n",
    "    }\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    param_grids['LightGBM'] = {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__num_leaves': [30, 50, 70]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING WITH HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "def create_pipeline(model, use_scaling=True):\n",
    "    steps = [('imputer', SimpleImputer(strategy='mean'))]\n",
    "    if use_scaling:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', model))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING - RandomizedSearchCV (Regression)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "best_models = {}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models_to_train = {\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    models_to_train['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "if LGB_AVAILABLE:\n",
    "    models_to_train['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "for model_name, model in models_to_train.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"[{model_name}] Hyperparameter Tuning...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    pipeline = create_pipeline(model)\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        n_iter=20,\n",
    "        cv=cv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[model_name] = search.best_estimator_\n",
    "    print(f\"\\n[BEST] Params: {search.best_params_}\")\n",
    "    print(f\"[CV] MAE: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_mae': float(-search.best_score_),\n",
    "        'test_mae': float(mae),\n",
    "        'test_rmse': float(rmse),\n",
    "        'test_r2': float(r2)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n[TEST] MAE:  {mae:.4f}\")\n",
    "    print(f\"[TEST] RMSE: {rmse:.4f}\")\n",
    "    print(f\"[TEST] RÂ²:   {r2:.4f}\")\n",
    "    \n",
    "    # Residuals analysis\n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"[TEST] Residuals - Mean: {residuals.mean():.4f}, Std: {residuals.std():.4f}\")\n",
    "    \n",
    "    trained_models[model_name] = best_models[model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca76dfb",
   "metadata": {},
   "source": [
    "## 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c1a1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[WINNER] Best Model: XGBoost\n",
      "[WINNER] Test MAE: 0.1363\n",
      "[WINNER] Test RMSE: 0.3656\n",
      "[WINNER] Test RÂ²: 0.9772\n",
      "\n",
      "================================================================================\n",
      "SAVING BEST MODEL\n",
      "================================================================================\n",
      "[SAVE] Best Model (XGBoost) -> models\\ps3_top_scorer_best_model.joblib\n",
      "[SAVE] Metadata -> models\\ps3_top_scorer_metadata.json\n",
      "\n",
      "================================================================================\n",
      "âœ… PS3: TOP SCORER PREDICTION - COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "best_model_name = min(results, key=lambda x: results[x]['test_mae'])\n",
    "best_metrics = results[best_model_name]\n",
    "\n",
    "print(f\"\\n[WINNER] Best Model: {best_model_name}\")\n",
    "print(f\"[WINNER] Test MAE: {best_metrics['test_mae']:.4f}\")\n",
    "print(f\"[WINNER] Test RMSE: {best_metrics['test_rmse']:.4f}\")\n",
    "print(f\"[WINNER] Test RÂ²: {best_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE ONLY THE BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING BEST MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save only the best model\n",
    "best_model_path = models_dir / 'ps3_top_scorer_best_model.joblib'\n",
    "joblib.dump(trained_models[best_model_name], best_model_path)\n",
    "print(f\"[SAVE] Best Model ({best_model_name}) -> {best_model_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TRAINING SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "summary = {\n",
    "    'problem_statement': 'PS3: Top Scorer Goals Prediction',\n",
    "    'task_type': 'Regression',\n",
    "    'best_model': best_model_name,\n",
    "    'best_test_mae': best_metrics['test_mae'],\n",
    "    'best_test_rmse': best_metrics['test_rmse'],\n",
    "    'best_test_r2': best_metrics['test_r2'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data': {\n",
    "        'path': data_path,\n",
    "        'shape': list(df.shape),\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'n_features': len(feature_cols)\n",
    "    },\n",
    "    'features': feature_cols,\n",
    "    'cv_strategy': '5-Fold KFold',\n",
    "    'tuning_method': 'RandomizedSearchCV (20 iterations)',\n",
    "    'scoring_metric': 'neg_mean_absolute_error',\n",
    "    'best_params': best_metrics['best_params']\n",
    "}\n",
    "\n",
    "summary_path = models_dir / 'ps3_top_scorer_metadata.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[SAVE] Metadata -> {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PS3: TOP SCORER PREDICTION - COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
