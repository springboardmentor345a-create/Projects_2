{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851d0edc",
   "metadata": {},
   "source": [
    "# ScoreSight: Unified Model Training Pipeline\n",
    "\n",
    "**Author:** Prathamesh Fuke  \n",
    "**Project:** EPL Football Analytics & Prediction  \n",
    "**Branch:** Prathamesh_Fuke  \n",
    "**Date:** November 13, 2025\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook consolidates all model training tasks from the original 7 problem-specific notebooks (08, 10-14) into a single, comprehensive training pipeline.\n",
    "\n",
    "### Problem Statements Covered:\n",
    "\n",
    "| Problem | Type | Target | Dataset | Metric |\n",
    "|---------|------|--------|---------|--------|\n",
    "| **PS1** | Classification | League Champion | Points Tally | F1-Macro |\n",
    "| **PS2** | Classification | Match Winner (H/NH) | Match Winner | F1-Macro |\n",
    "| **PS3** | Regression | Top Scorer Goals | Top Scorer | MAE, RÂ² |\n",
    "| **PS4** | Regression | Total Points | Points Tally | MAE, RÂ² |\n",
    "| **PS5** | Classification | Match Result (H/D/A) | Raw Match | F1-Macro |\n",
    "| **v1** | Regression | Home Goals (FTHG) | Engineered Match v3 | MAE |\n",
    "\n",
    "### Key Features:\n",
    "- âœ… **Temporal Cross-Validation** - Respects time series nature of sports data\n",
    "- âœ… **Data Leakage Prevention** - Removes future information (e.g., `points_per_game`)\n",
    "- âœ… **Hyperparameter Tuning** - RandomizedSearchCV / GridSearchCV for optimization\n",
    "- âœ… **Model Persistence** - Saves best models and metadata for deployment\n",
    "- âœ… **Comprehensive Evaluation** - Final comparison across all problem statements\n",
    "\n",
    "### Expected Outputs:\n",
    "- `models/ps1_league_winner_*.joblib` & `*.json`\n",
    "- `models/ps2_match_winner_*.joblib` & `*.json`\n",
    "- `models/ps3_top_scorer_*.joblib` & `*.json`\n",
    "- `models/ps4_total_points_*.joblib` & `*.json`\n",
    "- `models/ps5_match_result_*.joblib` & `*.json`\n",
    "- `data/engineered/model_training_summary_v1.json`\n",
    "- `visualizations/final_model_performance_comparison.png`\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35131e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    TimeSeriesSplit, StratifiedKFold, train_test_split\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor,\n",
    "    RandomForestClassifier, GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imbalanced Learn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    print(\"âš ï¸  Imbalanced-learn (SMOTE) not installed. Will proceed without it.\")\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"âš ï¸  XGBoost not installed.\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"âš ï¸  LightGBM not installed.\")\n",
    "\n",
    "# --- Global Configuration ---\n",
    "\n",
    "# Set working directory\n",
    "# All paths will be relative to this\n",
    "try:\n",
    "    os.chdir('d:\\\\ScoreSight')\n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸  Could not change directory. Assuming paths are relative to current location.\")\n",
    "\n",
    "# Define common paths\n",
    "MODELS_DIR = Path('models')\n",
    "DATA_RAW_DIR = Path('../data/raw') # From 14_\n",
    "DATA_FINAL_DIR = Path('data/final')\n",
    "DATA_CORRECTED_DIR = Path('data/corrected')\n",
    "DATA_ENGINEERED_DIR = Path('data/engineered')\n",
    "DATASETS_DIR = Path('../datasets') # From 11_\n",
    "VIZ_DIR = Path('../visualizations')\n",
    "\n",
    "# Create directories\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "DATA_CORRECTED_DIR.mkdir(exist_ok=True)\n",
    "VIZ_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set display options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\\nâœ“ All libraries imported and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.1: PS1 - Load Data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: PS1 - League Winner Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_PS1 = DATA_FINAL_DIR / 'data_final_points_tally.csv'\n",
    "TARGET_COL_PS1 = 'target_champion'\n",
    "\n",
    "print(f\"Loading dataset: {DATA_FILE_PS1}\")\n",
    "try:\n",
    "    df_ps1 = pd.read_csv(DATA_FILE_PS1)\n",
    "    print(f\"Dataset shape: {df_ps1.shape}\")\n",
    "    print(f\"Columns: {df_ps1.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_PS1}. Skipping this section.\")\n",
    "    df_ps1 = None\n",
    "\n",
    "if df_ps1 is not None:\n",
    "    # --- 1.2: PS1 - Feature and Target Definition (Leakage Corrected) ---\n",
    "    LEAKAGE_COLS_PS1 = [\n",
    "        'points_per_game',          # Directly calculated from target_total_points\n",
    "        'target_total_points',      # A target variable itself\n",
    "        'target_league_position',   # Another target variable\n",
    "        'target_champion'           # The primary target for this problem statement\n",
    "    ]\n",
    "    \n",
    "    feature_cols_ps1 = [\n",
    "        col for col in df_ps1.columns\n",
    "        if col not in LEAKAGE_COLS_PS1 and col not in ['season_encoded', 'team_encoded']\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- PS1: Feature and Target Definition ---\")\n",
    "    print(f\"Target column: '{TARGET_COL_PS1}'\")\n",
    "    print(f\"Feature columns ({len(feature_cols_ps1)}): {feature_cols_ps1}\")\n",
    "    print(f\"Leakage columns removed: {LEAKAGE_COLS_PS1}\")\n",
    "\n",
    "    # --- 1.3: PS1 - Temporal Train-Test Split ---\n",
    "    df_sorted_ps1 = df_ps1.sort_values('season_encoded').reset_index(drop=True)\n",
    "    \n",
    "    X_ps1 = df_sorted_ps1[feature_cols_ps1]\n",
    "    y_ps1 = df_sorted_ps1[TARGET_COL_PS1]\n",
    "    \n",
    "    seasons_ps1 = df_sorted_ps1['season_encoded'].unique()\n",
    "    \n",
    "    # Use the last season for testing, and all prior seasons for training\n",
    "    test_season_ps1 = seasons_ps1[-1]\n",
    "    train_seasons_ps1 = seasons_ps1[:-1]\n",
    "    \n",
    "    train_mask_ps1 = df_sorted_ps1['season_encoded'].isin(train_seasons_ps1)\n",
    "    test_mask_ps1 = df_sorted_ps1['season_encoded'] == test_season_ps1\n",
    "    \n",
    "    X_train_ps1, y_train_ps1 = X_ps1[train_mask_ps1], y_ps1[train_mask_ps1]\n",
    "    X_test_ps1, y_test_ps1 = X_ps1[test_mask_ps1], y_ps1[test_mask_ps1]\n",
    "    \n",
    "    print(\"\\n--- PS1: Temporal Split Details ---\")\n",
    "    print(f\"Training seasons: {train_seasons_ps1.tolist()}\")\n",
    "    print(f\"Test season: {test_season_ps1}\")\n",
    "    print(f\"Training set size: {X_train_ps1.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test_ps1.shape[0]} samples\")\n",
    "    print(f\"Champions in training set: {y_train_ps1.sum()}\")\n",
    "    print(f\"Champions in test set: {y_test_ps1.sum()}\")\n",
    "\n",
    "    # --- 1.4: PS1 - Model Training Pipeline ---\n",
    "    def create_pipeline_ps1(model):\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "    models_ps1 = {\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_ps1['XGBoost'] = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    \n",
    "    param_grids_ps1 = {\n",
    "        'GradientBoosting': {\n",
    "            'model__n_estimators': [100, 200], 'model__learning_rate': [0.05, 0.1], 'model__max_depth': [3, 5]\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'model__n_estimators': [100, 200], 'model__max_depth': [5, 10, None], 'model__min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model__n_estimators': [100, 200], 'model__learning_rate': [0.05, 0.1], 'model__max_depth': [3, 5],\n",
    "            'model__scale_pos_weight': [len(y_train_ps1[y_train_ps1==0]) / len(y_train_ps1[y_train_ps1==1])]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    n_splits_ps1 = min(len(train_seasons_ps1) - 1, 5)\n",
    "    tscv_ps1 = TimeSeriesSplit(n_splits=n_splits_ps1)\n",
    "    \n",
    "    print(f\"\\n--- PS1: Training Models with Temporal CV ({n_splits_ps1} splits) ---\")\n",
    "    \n",
    "    results_ps1 = {}\n",
    "    for name, model in models_ps1.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        pipeline = create_pipeline_ps1(model)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline, param_distributions=param_grids_ps1[name],\n",
    "            n_iter=10, cv=tscv_ps1, scoring='f1_macro', n_jobs=-1, random_state=42\n",
    "        )\n",
    "        search.fit(X_train_ps1, y_train_ps1)\n",
    "        y_pred = search.predict(X_test_ps1)\n",
    "        y_proba = search.predict_proba(X_test_ps1)[:, 1]\n",
    "        \n",
    "        results_ps1[name] = {\n",
    "            'model': search.best_estimator_, 'best_params': search.best_params_,\n",
    "            'accuracy': accuracy_score(y_test_ps1, y_pred),\n",
    "            'f1_macro': f1_score(y_test_ps1, y_pred, average='macro'),\n",
    "            'roc_auc': roc_auc_score(y_test_ps1, y_proba),\n",
    "            'report': classification_report(y_test_ps1, y_pred, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_test_ps1, y_pred)\n",
    "        }\n",
    "        print(f\"  Done. Best F1 (macro) on CV: {search.best_score_:.4f}\")\n",
    "\n",
    "    # --- 1.5: PS1 - Results and Model Saving ---\n",
    "    best_model_name_ps1 = max(results_ps1, key=lambda name: results_ps1[name]['f1_macro'])\n",
    "    best_model_data_ps1 = results_ps1[best_model_name_ps1]\n",
    "    \n",
    "    print(\"\\n--- PS1: Model Performance on Hold-out Test Set ---\")\n",
    "    print(f\"ðŸ† Best Model: {best_model_name_ps1}\")\n",
    "    print(f\"  Accuracy: {best_model_data_ps1['accuracy']:.4f}\")\n",
    "    print(f\"  F1 (Macro): {best_model_data_ps1['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC AUC: {best_model_data_ps1['roc_auc']:.4f}\")\n",
    "    print(\"  Confusion Matrix:\\n\", best_model_data_ps1['confusion_matrix'])\n",
    "    \n",
    "    model_filename_ps1 = MODELS_DIR / 'ps1_league_winner_best_model.joblib'\n",
    "    joblib.dump(best_model_data_ps1['model'], model_filename_ps1)\n",
    "    \n",
    "    metadata_ps1 = {\n",
    "        'problem_statement': 'PS1: League Winner Prediction',\n",
    "        'model_type': best_model_name_ps1,\n",
    "        'performance_metrics': {\n",
    "            'accuracy': best_model_data_ps1['accuracy'],\n",
    "            'f1_macro': best_model_data_ps1['f1_macro'],\n",
    "            'roc_auc': best_model_data_ps1['roc_auc']\n",
    "        },\n",
    "        'features': feature_cols_ps1,\n",
    "        'target': TARGET_COL_PS1,\n",
    "        'temporal_split': {\n",
    "            'train_seasons': train_seasons_ps1.tolist(),\n",
    "            'test_season': int(test_season_ps1)\n",
    "        }\n",
    "    }\n",
    "    metadata_filename_ps1 = MODELS_DIR / 'ps1_league_winner_metadata.json'\n",
    "    with open(metadata_filename_ps1, 'w') as f:\n",
    "        json.dump(metadata_ps1, f, indent=4)\n",
    "        \n",
    "    print(f\"\\nModel saved to: {model_filename_ps1}\")\n",
    "    print(f\"Metadata saved to: {metadata_filename_ps1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c00634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1: PS3 - Load Data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: PS3 - Top Scorer Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_PS3 = DATA_FINAL_DIR / 'data_final_top_scorer.csv'\n",
    "TARGET_COL_PS3 = 'goals' # The original file uses 'goals_scored', this one uses 'goals'\n",
    "MODEL_NAME_PS3 = 'ps3_top_scorer_best_model.joblib'\n",
    "METADATA_NAME_PS3 = 'ps3_top_scorer_metadata.json'\n",
    "\n",
    "print(f\"Loading dataset: {DATA_FILE_PS3}\")\n",
    "try:\n",
    "    df_ps3 = pd.read_csv(DATA_FILE_PS3)\n",
    "    # The file 'data_final_top_scorer.csv' lacks 'season_encoded', but '10_League_Winner_PS1.ipynb' implies\n",
    "    # 'data_final_points_tally.csv' has it. We must join to get the season.\n",
    "    # This is a critical step to enable temporal splitting.\n",
    "    \n",
    "    # Load league data just for the season mapping\n",
    "    df_league_ps3 = pd.read_csv(DATA_FINAL_DIR / 'data_final_points_tally.csv')\n",
    "    \n",
    "    # Check if 'player_encoded' and 'team_encoded' exist for a merge\n",
    "    if 'player_encoded' in df_ps3.columns and 'team_encoded' in df_league_ps3.columns:\n",
    "        # This is a weak link. Assuming 'player_encoded' can map to a team, or we must use another file.\n",
    "        # Re-reading notebook 12, it seems 'season_encoded' was *intended* to be in this file.\n",
    "        # The file loaded in cell 2 of 12_Top_Scorer_PS3.ipynb is 'data/corrected/top_scorer_corrected.csv'\n",
    "        # Let's pivot and use that file instead, as it was the one that worked.\n",
    "        \n",
    "        DATA_FILE_PS3 = DATA_CORRECTED_DIR / 'top_scorer_corrected.csv'\n",
    "        print(f\"Pivoting to corrected file: {DATA_FILE_PS3}\")\n",
    "        df_ps3 = pd.read_csv(DATA_FILE_PS3)\n",
    "        df_ps3.columns = df_ps3.columns.str.lower().str.strip()\n",
    "\n",
    "        # This corrected file *still* lacks a season identifier.\n",
    "        # The original notebook 12 is critically flawed. It says temporal but uses random split.\n",
    "        # I will IMPLEMENT the temporal split logic from PS1/PS4 on the *wrong* file, to show how it's done.\n",
    "        # The file 'data_final_top_scorer.csv' from '04_Encoding' *should* have 'season_encoded'.\n",
    "        # Let's try that one again.\n",
    "        \n",
    "        DATA_FILE_PS3 = DATA_FINAL_DIR / 'data_final_top_scorer.csv'\n",
    "        print(f\"Re-trying with final data: {DATA_FILE_PS3}\")\n",
    "        df_ps3 = pd.read_csv(DATA_FILE_PS3)\n",
    "        \n",
    "        if 'season_encoded' not in df_ps3.columns:\n",
    "            print(\"âŒ CRITICAL ERROR: 'season_encoded' not found in 'data_final_top_scorer.csv'.\")\n",
    "            print(\"Cannot perform temporal split for PS3. Skipping this section.\")\n",
    "            df_ps3 = None\n",
    "        else:\n",
    "            df_ps3 = df_ps3.sort_values(by=['season_encoded', 'player_encoded']).reset_index(drop=True)\n",
    "            print(\"âœ“ Successfully loaded and sorted data for PS3.\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_PS3}. Skipping this section.\")\n",
    "    df_ps3 = None\n",
    "\n",
    "if df_ps3 is not None:\n",
    "    # --- 2.2: PS3 - Feature and Target Definition ---\n",
    "    y_ps3 = df_ps3[TARGET_COL_PS3]\n",
    "    # Exclude all identifiers and other potential targets\n",
    "    exclude_cols_ps3 = [\n",
    "        'goals', 'non_penalty_goals', 'penalty_goals_made', # Targets\n",
    "        'player', 'team', 'season', 'nationality', 'position', # Identifiers\n",
    "        'player_encoded', 'nation_encoded', 'position_encoded', 'season_encoded' # Encoded IDs\n",
    "    ]\n",
    "    feature_cols_ps3 = [col for col in df_ps3.columns if col not in exclude_cols_ps3 and df_ps3[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "    print(f\"\\n--- PS3: Feature and Target Definition ---\")\n",
    "    print(f\"Target column: '{TARGET_COL_PS3}'\")\n",
    "    print(f\"Feature columns ({len(feature_cols_ps3)}): {feature_cols_ps3[:5]}...\")\n",
    "\n",
    "    # --- 2.3: PS3 - Temporal Train-Test Split ---\n",
    "    df_sorted_ps3 = df_ps3.sort_values('season_encoded').reset_index(drop=True)\n",
    "    \n",
    "    X_ps3 = df_sorted_ps3[feature_cols_ps3]\n",
    "    y_ps3 = df_sorted_ps3[TARGET_COL_PS3]\n",
    "    \n",
    "    seasons_ps3 = df_sorted_ps3['season_encoded'].unique()\n",
    "    test_season_ps3 = seasons_ps3[-1]\n",
    "    train_seasons_ps3 = seasons_ps3[:-1]\n",
    "    \n",
    "    train_mask_ps3 = df_sorted_ps3['season_encoded'].isin(train_seasons_ps3)\n",
    "    test_mask_ps3 = df_sorted_ps3['season_encoded'] == test_season_ps3\n",
    "    \n",
    "    X_train_ps3, y_train_ps3 = X_ps3[train_mask_ps3], y_ps3[train_mask_ps3]\n",
    "    X_test_ps3, y_test_ps3 = X_ps3[test_mask_ps3], y_ps3[test_mask_ps3]\n",
    "    \n",
    "    print(\"\\n--- PS3: Temporal Split Details ---\")\n",
    "    print(f\"Training seasons: {train_seasons_ps3.tolist()}\")\n",
    "    print(f\"Test season: {test_season_ps3}\")\n",
    "    print(f\"Training set size: {X_train_ps3.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test_ps3.shape[0]} samples\")\n",
    "\n",
    "    # --- 2.4: PS3 - Model Training Pipeline ---\n",
    "    def create_pipeline_ps3(model):\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    models_to_train_ps3 = {\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_to_train_ps3['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    if LGB_AVAILABLE:\n",
    "        models_to_train_ps3['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "    # Use param grids from notebook 12\n",
    "    param_grids_ps3 = {\n",
    "        'Ridge': {'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'Lasso': {'model__alpha': [0.001, 0.01, 0.1, 1, 10]},\n",
    "        'RandomForest': {'model__n_estimators': [100, 200, 300], 'model__max_depth': [10, 15, 20, None], 'model__min_samples_split': [2, 5], 'model__min_samples_leaf': [1, 2]},\n",
    "        'GradientBoosting': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__max_depth': [5, 7, 9], 'model__subsample': [0.8, 0.9]},\n",
    "        'XGBoost': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__max_depth': [5, 7, 9], 'model__subsample': [0.8, 0.9]},\n",
    "        'LightGBM': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__num_leaves': [30, 50, 70]}\n",
    "    }\n",
    "\n",
    "    n_splits_ps3 = min(len(train_seasons_ps3) - 1, 5)\n",
    "    tscv_ps3 = TimeSeriesSplit(n_splits=n_splits_ps3)\n",
    "\n",
    "    print(f\"\\n--- PS3: Training Models with Temporal CV ({n_splits_ps3} splits) ---\")\n",
    "    \n",
    "    results_ps3 = {}\n",
    "    trained_models_ps3 = {}\n",
    "\n",
    "    for model_name, model in models_to_train_ps3.items():\n",
    "        if model_name not in param_grids_ps3: continue\n",
    "        print(f\"Training {model_name}...\")\n",
    "        pipeline = create_pipeline_ps3(model)\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, param_grids_ps3[model_name], n_iter=20, cv=tscv_ps3,\n",
    "            scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=0\n",
    "        )\n",
    "        search.fit(X_train_ps3, y_train_ps3)\n",
    "        y_pred = search.predict(X_test_ps3)\n",
    "        \n",
    "        results_ps3[model_name] = {\n",
    "            'best_params': search.best_params_,\n",
    "            'cv_mae': float(-search.best_score_),\n",
    "            'test_mae': float(mean_absolute_error(y_test_ps3, y_pred)),\n",
    "            'test_rmse': float(np.sqrt(mean_squared_error(y_test_ps3, y_pred))),\n",
    "            'test_r2': float(r2_score(y_test_ps3, y_pred))\n",
    "        }\n",
    "        trained_models_ps3[model_name] = search.best_estimator_\n",
    "        print(f\"  [CV] MAE: {-search.best_score_:.4f} | [TEST] MAE: {results_ps3[model_name]['test_mae']:.4f}\")\n",
    "\n",
    "    # --- 2.5: PS3 - Results and Model Saving ---\n",
    "    best_model_name_ps3 = min(results_ps3, key=lambda x: results_ps3[x]['test_mae'])\n",
    "    best_metrics_ps3 = results_ps3[best_model_name_ps3]\n",
    "\n",
    "    print(f\"\\n[WINNER] Best Model: {best_model_name_ps3}\")\n",
    "    print(f\"[WINNER] Test MAE: {best_metrics_ps3['test_mae']:.4f}\")\n",
    "    print(f\"[WINNER] Test RÂ²: {best_metrics_ps3['test_r2']:.4f}\")\n",
    "\n",
    "    best_model_path_ps3 = MODELS_DIR / MODEL_NAME_PS3\n",
    "    joblib.dump(trained_models_ps3[best_model_name_ps3], best_model_path_ps3)\n",
    "\n",
    "    summary_ps3 = {\n",
    "        'problem_statement': 'PS3: Top Scorer Goals Prediction',\n",
    "        'task_type': 'Regression',\n",
    "        'best_model': best_model_name_ps3,\n",
    "        'best_test_mae': best_metrics_ps3['test_mae'],\n",
    "        'best_test_r2': best_metrics_ps3['test_r2'],\n",
    "        'data': {\n",
    "            'path': str(DATA_FILE_PS3),\n",
    "            'train_size': len(X_train_ps3),\n",
    "            'test_size': len(X_test_ps3),\n",
    "            'n_features': len(feature_cols_ps3)\n",
    "        },\n",
    "        'features': feature_cols_ps3,\n",
    "        'cv_strategy': f'TimeSeriesSplit (n_splits={n_splits_ps3})',\n",
    "        'best_params': best_metrics_ps3['best_params']\n",
    "    }\n",
    "\n",
    "    summary_path_ps3 = MODELS_DIR / METADATA_NAME_PS3\n",
    "    with open(summary_path_ps3, 'w') as f:\n",
    "        json.dump(summary_ps3, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"[SAVE] Best Model ({best_model_name_ps3}) -> {best_model_path_ps3}\")\n",
    "    print(f\"[SAVE] Metadata -> {summary_path_ps3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1: PS4 - Load Data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: PS4 - Total Points Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_PS4 = DATA_FINAL_DIR / 'data_final_points_tally.csv'\n",
    "TARGET_COL_PS4 = 'target_total_points'\n",
    "MODEL_NAME_PS4 = 'ps4_total_points_best_model.joblib'\n",
    "METADATA_NAME_PS4 = 'ps4_total_points_metadata.json'\n",
    "\n",
    "print(f\"Loading dataset: {DATA_FILE_PS4}\")\n",
    "try:\n",
    "    df_ps4 = pd.read_csv(DATA_FILE_PS4)\n",
    "    df_ps4 = df_ps4.sort_values(by='season_encoded').reset_index(drop=True)\n",
    "    print(f\"Dataset shape: {df_ps4.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_PS4}. Skipping this section.\")\n",
    "    df_ps4 = None\n",
    "\n",
    "if df_ps4 is not None:\n",
    "    # --- 3.2: PS4 - Feature and Target Definition (Leakage Corrected) ---\n",
    "    y_ps4 = df_ps4[TARGET_COL_PS4]\n",
    "    \n",
    "    # CRITICAL: Remove 'points_per_game' and other targets\n",
    "    leakage_cols_ps4 = ['points_per_game', 'target_total_points', 'target_league_position', 'target_champion']\n",
    "    feature_cols_ps4 = [\n",
    "        col for col in df_ps4.columns \n",
    "        if col not in leakage_cols_ps4 and col not in ['team', 'season_encoded', 'team_encoded']\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- PS4: Feature and Target Definition ---\")\n",
    "    print(f\"Target column: '{TARGET_COL_PS4}'\")\n",
    "    print(f\"Feature columns ({len(feature_cols_ps4)}): {feature_cols_ps4}\")\n",
    "    \n",
    "    X_ps4 = df_ps4[feature_cols_ps4]\n",
    "\n",
    "    # --- 3.3: PS4 - Temporal Train-Test Split ---\n",
    "    seasons_ps4 = df_ps4['season_encoded'].unique()\n",
    "    test_season_ps4 = seasons_ps4[-1]\n",
    "    train_seasons_ps4 = seasons_ps4[:-1]\n",
    "    \n",
    "    train_mask_ps4 = df_ps4['season_encoded'].isin(train_seasons_ps4)\n",
    "    test_mask_ps4 = df_ps4['season_encoded'] == test_season_ps4\n",
    "    \n",
    "    X_train_ps4, y_train_ps4 = X_ps4[train_mask_ps4], y_ps4[train_mask_ps4]\n",
    "    X_test_ps4, y_test_ps4 = X_ps4[test_mask_ps4], y_ps4[test_mask_ps4]\n",
    "    \n",
    "    print(\"\\n--- PS4: Temporal Split Details ---\")\n",
    "    print(f\"Training seasons: {train_seasons_ps4.tolist()}\")\n",
    "    print(f\"Test season: {test_season_ps4}\")\n",
    "    print(f\"Training set size: {X_train_ps4.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test_ps4.shape[0]} samples\")\n",
    "\n",
    "    # --- 3.4: PS4 - Model Training Pipeline ---\n",
    "    def create_pipeline_ps4(model):\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    models_to_train_ps4 = {\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_to_train_ps4['XGBoost'] = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    if LGB_AVAILABLE:\n",
    "        models_to_train_ps4['LightGBM'] = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "    # Use param grids from notebook 13\n",
    "    param_grids_ps4 = {\n",
    "        'Ridge': {'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'Lasso': {'model__alpha': [0.001, 0.01, 0.1, 1, 10]},\n",
    "        'RandomForest': {'model__n_estimators': [50, 100, 150], 'model__max_depth': [5, 10, 15, None], 'model__min_samples_split': [2, 5], 'model__min_samples_leaf': [1, 2]},\n",
    "        'GradientBoosting': {'model__n_estimators': [50, 100, 150], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__max_depth': [3, 5, 7], 'model__subsample': [0.8, 0.9]},\n",
    "        'XGBoost': {'model__n_estimators': [50, 100, 150], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__max_depth': [3, 5, 7], 'model__subsample': [0.8, 0.9]},\n",
    "        'LightGBM': {'model__n_estimators': [50, 100, 150], 'model__learning_rate': [0.01, 0.05, 0.1], 'model__num_leaves': [20, 30, 40]}\n",
    "    }\n",
    "    \n",
    "    n_splits_ps4 = min(len(train_seasons_ps4) - 1, 5)\n",
    "    tscv_ps4 = TimeSeriesSplit(n_splits=n_splits_ps4)\n",
    "    \n",
    "    print(f\"\\n--- PS4: Training Models with Temporal CV ({n_splits_ps4} splits) ---\")\n",
    "    \n",
    "    results_ps4 = {}\n",
    "    trained_models_ps4 = {}\n",
    "    \n",
    "    for model_name, model in models_to_train_ps4.items():\n",
    "        if model_name not in param_grids_ps4: continue\n",
    "        print(f\"Training {model_name}...\")\n",
    "        pipeline = create_pipeline_ps4(model)\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, param_grids_ps4[model_name], n_iter=20, cv=tscv_ps4,\n",
    "            scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=0\n",
    "        )\n",
    "        search.fit(X_train_ps4, y_train_ps4)\n",
    "        y_pred = search.predict(X_test_ps4)\n",
    "        \n",
    "        results_ps4[model_name] = {\n",
    "            'best_params': search.best_params_,\n",
    "            'cv_mae': float(-search.best_score_),\n",
    "            'test_mae': float(mean_absolute_error(y_test_ps4, y_pred)),\n",
    "            'test_rmse': float(np.sqrt(mean_squared_error(y_test_ps4, y_pred))),\n",
    "            'test_r2': float(r2_score(y_test_ps4, y_pred))\n",
    "        }\n",
    "        trained_models_ps4[model_name] = search.best_estimator_\n",
    "        print(f\"  [CV] MAE: {-search.best_score_:.4f} | [TEST] MAE: {results_ps4[model_name]['test_mae']:.4f}\")\n",
    "\n",
    "    # --- 3.5: PS4 - Results and Model Saving ---\n",
    "    best_model_name_ps4 = min(results_ps4, key=lambda x: results_ps4[x]['test_mae'])\n",
    "    best_metrics_ps4 = results_ps4[best_model_name_ps4]\n",
    "\n",
    "    print(f\"\\n[WINNER] Best Model: {best_model_name_ps4}\")\n",
    "    print(f\"[WINNER] Test MAE: {best_metrics_ps4['test_mae']:.4f}\")\n",
    "    print(f\"[WINNER] Test RÂ²: {best_metrics_ps4['test_r2']:.4f}\")\n",
    "\n",
    "    best_model_path_ps4 = MODELS_DIR / MODEL_NAME_PS4\n",
    "    joblib.dump(trained_models_ps4[best_model_name_ps4], best_model_path_ps4)\n",
    "    \n",
    "    summary_ps4 = {\n",
    "        'problem_statement': 'PS4: Total Points Tally Prediction',\n",
    "        'task_type': 'Regression',\n",
    "        'best_model': best_model_name_ps4,\n",
    "        'best_test_mae': best_metrics_ps4['test_mae'],\n",
    "        'best_test_r2': best_metrics_ps4['test_r2'],\n",
    "        'data': {\n",
    "            'path': str(DATA_FILE_PS4),\n",
    "            'train_size': len(X_train_ps4),\n",
    "            'test_size': len(X_test_ps4),\n",
    "            'n_features': len(feature_cols_ps4)\n",
    "        },\n",
    "        'features': feature_cols_ps4,\n",
    "        'cv_strategy': f'TimeSeriesSplit (n_splits={n_splits_ps4})',\n",
    "        'best_params': best_metrics_ps4['best_params']\n",
    "    }\n",
    "\n",
    "    summary_path_ps4 = MODELS_DIR / METADATA_NAME_PS4\n",
    "    with open(summary_path_ps4, 'w') as f:\n",
    "        json.dump(summary_ps4, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"[SAVE] Best Model ({best_model_name_ps4}) -> {best_model_path_ps4}\")\n",
    "    print(f\"[SAVE] Metadata -> {summary_path_ps4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca540a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1.1: PS2 - Load Data & Clean ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4a: PS2 - Match Winner (H/NH) - Advanced FE Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_PS2 = DATASETS_DIR / 'Match Winner.csv'\n",
    "MODEL_NAME_PS2 = 'ps2_match_winner_best_model_v4_ultimate.joblib'\n",
    "METADATA_NAME_PS2 = 'ps2_match_winner_metadata_v4_ultimate.json'\n",
    "\n",
    "try:\n",
    "    df_ps2 = pd.read_csv(DATA_FILE_PS2)\n",
    "    df_ps2['Date'] = pd.to_datetime(df_ps2['Date'], format='mixed')\n",
    "    df_ps2 = df_ps2.sort_values('Date').reset_index(drop=True)\n",
    "    df_ps2 = df_ps2[['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']]\n",
    "    df_ps2.dropna(subset=['FTHG', 'FTAG', 'FTR'], inplace=True)\n",
    "    print(f\"PS2 data loaded and cleaned. Shape: {df_ps2.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_PS2}. Skipping this section.\")\n",
    "    df_ps2 = None\n",
    "\n",
    "if df_ps2 is not None:\n",
    "    # --- 4.1.2: PS2 - Advanced Feature Engineering ---\n",
    "    \n",
    "    # (Using the same function from Notebook 11)\n",
    "    def get_advanced_features_ps2(team_history, opponent_history, h2h_history, windows, alphas):\n",
    "        features = {}\n",
    "        for W in windows:\n",
    "            if len(team_history) >= W:\n",
    "                window_df = pd.DataFrame(team_history[-W:])\n",
    "                features[f'avg_gs_{W}'] = window_df['gs'].mean()\n",
    "                features[f'avg_gc_{W}'] = window_df['gc'].mean()\n",
    "                features[f'avg_gd_{W}'] = window_df['gd'].mean()\n",
    "                features[f'avg_pts_{W}'] = window_df['pts'].mean()\n",
    "            else:\n",
    "                features[f'avg_gs_{W}'] = np.nan\n",
    "                features[f'avg_gc_{W}'] = np.nan\n",
    "                features[f'avg_gd_{W}'] = np.nan\n",
    "                features[f'avg_pts_{W}'] = np.nan\n",
    "        if len(team_history) > 1:\n",
    "            history_df = pd.DataFrame(team_history)\n",
    "            for alpha in alphas:\n",
    "                features[f'ewma_gs_{alpha}'] = history_df['gs'].ewm(alpha=alpha).mean().iloc[-1]\n",
    "                features[f'ewma_gc_{alpha}'] = history_df['gc'].ewm(alpha=alpha).mean().iloc[-1]\n",
    "                features[f'ewma_gd_{alpha}'] = history_df['gd'].ewm(alpha=alpha).mean().iloc[-1]\n",
    "        else:\n",
    "            for alpha in alphas:\n",
    "                features[f'ewma_gs_{alpha}'] = np.nan\n",
    "                features[f'ewma_gc_{alpha}'] = np.nan\n",
    "                features[f'ewma_gd_{alpha}'] = np.nan\n",
    "        if len(h2h_history) > 0:\n",
    "            h2h_df = pd.DataFrame(h2h_history)\n",
    "            features['h2h_avg_gs'] = h2h_df['gs'].mean()\n",
    "            features['h2h_avg_gc'] = h2h_df['gc'].mean()\n",
    "            features['h2h_win_rate'] = (h2h_df['pts'] == 3).mean()\n",
    "        else:\n",
    "            features['h2h_avg_gs'] = np.nan\n",
    "            features['h2h_avg_gc'] = np.nan\n",
    "            features['h2h_win_rate'] = np.nan\n",
    "        return features\n",
    "\n",
    "    WINDOWS = [5, 10]\n",
    "    ALPHAS = [0.1, 0.2]\n",
    "    teams = pd.concat([df_ps2['HomeTeam'], df_ps2['AwayTeam']]).unique()\n",
    "    team_histories = {team: [] for team in teams}\n",
    "    h2h_histories = {} \n",
    "    \n",
    "    new_features_list = []\n",
    "    print(\"Starting PS2 feature engineering (EWMA, H2H)...\")\n",
    "    \n",
    "    for index, row in df_ps2.iterrows():\n",
    "        home_team, away_team = row['HomeTeam'], row['AwayTeam']\n",
    "        h2h_key = tuple(sorted((home_team, away_team)))\n",
    "        if h2h_key not in h2h_histories:\n",
    "            h2h_histories[h2h_key] = []\n",
    "\n",
    "        home_feats = get_advanced_features_ps2(team_histories[home_team], team_histories[away_team], h2h_histories[h2h_key], WINDOWS, ALPHAS)\n",
    "        away_feats = get_advanced_features_ps2(team_histories[away_team], team_histories[home_team], h2h_histories[h2h_key], WINDOWS, ALPHAS)\n",
    "        \n",
    "        match_features = {}\n",
    "        for key, value in home_feats.items(): match_features[f'H_{key}'] = value\n",
    "        for key, value in away_feats.items(): match_features[f'A_{key}'] = value\n",
    "        for W in WINDOWS: match_features[f'diff_avg_gd_{W}'] = home_feats.get(f'avg_gd_{W}') - away_feats.get(f'avg_gd_{W}')\n",
    "        for alpha in ALPHAS: match_features[f'diff_ewma_gd_{alpha}'] = home_feats.get(f'ewma_gd_{alpha}') - away_feats.get(f'ewma_gd_{alpha}')\n",
    "        new_features_list.append(match_features)\n",
    "\n",
    "        # Update histories\n",
    "        home_goals, away_goals = row['FTHG'], row['FTAG']\n",
    "        if row['FTR'] == 'H': home_pts, away_pts = 3, 0\n",
    "        elif row['FTR'] == 'A': home_pts, away_pts = 0, 3\n",
    "        else: home_pts, away_pts = 1, 1\n",
    "        team_histories[home_team].append({'gs': home_goals, 'gc': away_goals, 'gd': home_goals - away_goals, 'pts': home_pts})\n",
    "        team_histories[away_team].append({'gs': away_goals, 'gc': home_goals, 'gd': away_goals - home_goals, 'pts': away_pts})\n",
    "        h2h_histories[h2h_key].append({'team': home_team, 'gs': home_goals, 'gc': away_goals, 'pts': home_pts})\n",
    "    \n",
    "    features_df_ps2 = pd.DataFrame(new_features_list, index=df_ps2.index)\n",
    "    df_featured_ps2 = pd.concat([df_ps2, features_df_ps2], axis=1)\n",
    "    df_featured_ps2.dropna(inplace=True)\n",
    "    print(f\"PS2 feature engineering complete. New shape: {df_featured_ps2.shape}\")\n",
    "\n",
    "    # --- 4.1.3: PS2 - Target, Split, and Pipeline ---\n",
    "    le_ps2 = LabelEncoder()\n",
    "    # Note: This is BINARY classification (H vs NH)\n",
    "    df_featured_ps2['FTR_encoded'] = le_ps2.fit_transform(df_featured_ps2['FTR'])\n",
    "    target_mapping_ps2 = {i: l for i, l in enumerate(le_ps2.classes_)} # {0: 'H', 1: 'NH'}\n",
    "    \n",
    "    y_ps2 = df_featured_ps2['FTR_encoded']\n",
    "    X_ps2 = df_featured_ps2.drop(columns=['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'FTR_encoded'])\n",
    "    \n",
    "    split_index_ps2 = int(len(df_featured_ps2) * 0.85)\n",
    "    X_train_ps2, y_train_ps2 = X_ps2.iloc[:split_index_ps2], y_ps2.iloc[:split_index_ps2]\n",
    "    X_test_ps2, y_test_ps2 = X_ps2.iloc[split_index_ps2:], y_ps2.iloc[split_index_ps2:]\n",
    "    \n",
    "    print(f\"PS2 Temporal Split: Train={len(X_train_ps2)}, Test={len(X_test_ps2)}\")\n",
    "    print(f\"PS2 Target Mapping: {target_mapping_ps2}\")\n",
    "\n",
    "    # --- 4.1.4: PS2 - Model Training ---\n",
    "    def create_smote_pipeline_ps2(model):\n",
    "        if IMBLEARN_AVAILABLE:\n",
    "            return ImbPipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')), \n",
    "                ('scaler', StandardScaler()),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        else:\n",
    "             return Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')), \n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', model)\n",
    "            ])\n",
    "\n",
    "    models_ps2 = {'RandomForest': RandomForestClassifier(random_state=42)}\n",
    "    if XGB_AVAILABLE:\n",
    "        models_ps2['XGBoost'] = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss') # mlogloss for multi, logloss for binary\n",
    "    \n",
    "    param_grids_ps2 = {\n",
    "        'RandomForest': {\n",
    "            'model__n_estimators': [200, 400], 'model__max_depth': [10, 20, 30],\n",
    "            'model__min_samples_leaf': [1, 2], 'model__max_features': ['sqrt', 'log2']\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model__n_estimators': [200, 400], 'model__learning_rate': [0.05, 0.1],\n",
    "            'model__max_depth': [5, 7], 'model__subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tscv_ps2 = TimeSeriesSplit(n_splits=5)\n",
    "    results_ps2 = {}\n",
    "    \n",
    "    print(f\"\\n--- PS2: Training Models with GridSearchCV (Binary) ---\")\n",
    "    \n",
    "    for name, model in models_ps2.items():\n",
    "        if name not in param_grids_ps2: continue\n",
    "        print(f\"Training {name}...\")\n",
    "        pipeline = create_smote_pipeline_ps2(model)\n",
    "        search = GridSearchCV(\n",
    "            estimator=pipeline, param_grid=param_grids_ps2[name],\n",
    "            cv=tscv_ps2, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "        )\n",
    "        search.fit(X_train_ps2, y_train_ps2)\n",
    "        y_pred = search.predict(X_test_ps2)\n",
    "        \n",
    "        results_ps2[name] = {\n",
    "            'model': search.best_estimator_, 'best_params': search.best_params_,\n",
    "            'accuracy': accuracy_score(y_test_ps2, y_pred),\n",
    "            'f1_macro': f1_score(y_test_ps2, y_pred, average='macro'),\n",
    "            'report': classification_report(y_test_ps2, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_test_ps2, y_pred)\n",
    "        }\n",
    "        print(f\"  Done. Best Accuracy on CV: {search.best_score_:.4f}\")\n",
    "\n",
    "    # --- 4.1.5: PS2 - Results and Model Saving ---\n",
    "    best_model_name_ps2 = max(results_ps2, key=lambda name: results_ps2[name]['accuracy'])\n",
    "    best_model_data_ps2 = results_ps2[best_model_name_ps2]\n",
    "\n",
    "    print(f\"\\n--- PS2: Saving Best Model: {best_model_name_ps2} ---\")\n",
    "    print(f\"  Accuracy: {best_model_data_ps2['accuracy']:.4f}\")\n",
    "    print(f\"  F1 (Macro): {best_model_data_ps2['f1_macro']:.4f}\")\n",
    "\n",
    "    joblib.dump(best_model_data_ps2['model'], MODELS_DIR / MODEL_NAME_PS2)\n",
    "    \n",
    "    metadata_ps2 = {\n",
    "        \"problem_statement\": \"PS2: Match Winner Prediction (H/NH)\",\n",
    "        \"model_name\": best_model_name_ps2,\n",
    "        \"features\": list(X_ps2.columns),\n",
    "        \"target_mapping\": target_mapping_ps2,\n",
    "        \"performance_metrics\": {\n",
    "            \"accuracy\": best_model_data_ps2['accuracy'],\n",
    "            \"f1_macro\": best_model_data_ps2['f1_macro'],\n",
    "        },\n",
    "        \"hyperparameters\": best_model_data_ps2['best_params'],\n",
    "    }\n",
    "    \n",
    "    with open(MODELS_DIR / METADATA_NAME_PS2, 'w') as f:\n",
    "        json.dump(metadata_ps2, f, indent=4)\n",
    "        \n",
    "    print(f\"Model saved to: {MODELS_DIR / MODEL_NAME_PS2}\")\n",
    "    print(f\"Metadata saved to: {MODELS_DIR / METADATA_NAME_PS2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3841dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2.1: PS5 - Load Data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4b: PS5 - Match Result (H/D/A) - Raw Data Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_PS5 = DATA_RAW_DIR / 'data_raw_match.csv'\n",
    "MODEL_NAME_PS5 = 'ps5_match_result_model.joblib'\n",
    "METADATA_NAME_PS5 = 'ps5_match_result_metrics.json'\n",
    "\n",
    "try:\n",
    "    df_ps5 = pd.read_csv(DATA_FILE_PS5)\n",
    "    df_ps5.columns = df_ps5.columns.str.lower().str.strip()\n",
    "    df_ps5['date'] = pd.to_datetime(df_ps5['date'], format='mixed')\n",
    "    df_ps5 = df_ps5.sort_values('date').reset_index(drop=True)\n",
    "    print(f\"PS5 data loaded. Shape: {df_ps5.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_PS5}. Skipping this section.\")\n",
    "    df_ps5 = None\n",
    "\n",
    "if df_ps5 is not None:\n",
    "    # --- 4.2.2: PS5 - Target Engineering & Feature Selection ---\n",
    "    def create_target_ps5(row):\n",
    "        if row['fthg'] > row['ftag']: return 'H'\n",
    "        elif row['fthg'] == row['ftag']: return 'D'\n",
    "        else: return 'A'\n",
    "    \n",
    "    df_ps5['result'] = df_ps5.apply(create_target_ps5, axis=1)\n",
    "    target_map_ps5 = {'H': 0, 'D': 1, 'A': 2}\n",
    "    df_ps5['target'] = df_ps5['result'].map(target_map_ps5)\n",
    "    \n",
    "    exclude_cols_ps5 = [\n",
    "        'unnamed: 0', 'date', 'hometeam', 'awayteam', \n",
    "        'fthg', 'ftag', 'ftr', 'result', 'target'\n",
    "    ]\n",
    "    \n",
    "    numeric_features_ps5 = df_ps5.select_dtypes(include=np.number).columns.tolist()\n",
    "    numeric_features_ps5 = [col for col in numeric_features_ps5 if col not in exclude_cols_ps5]\n",
    "    \n",
    "    categorical_features_ps5 = [\n",
    "        'hm1', 'hm2', 'hm3', 'hm4', 'hm5', \n",
    "        'am1', 'am2', 'am3', 'am4', 'am5'\n",
    "    ]\n",
    "    features_ps5 = numeric_features_ps5 + categorical_features_ps5\n",
    "    X_ps5 = df_ps5[features_ps5].copy()\n",
    "    y_ps5 = df_ps5['target'].copy()\n",
    "\n",
    "    print(\"--- PS5: Target Variable ---\")\n",
    "    print(y_ps5.value_counts(normalize=True))\n",
    "\n",
    "    # --- 4.2.3: PS5 - Temporal Train-Test Split ---\n",
    "    split_index_ps5 = int(len(df_ps5) * 0.8)\n",
    "    X_train_ps5, y_train_ps5 = X_ps5.iloc[:split_index_ps5], y_ps5.iloc[:split_index_ps5]\n",
    "    X_test_ps5, y_test_ps5 = X_ps5.iloc[split_index_ps5:], y_ps5.iloc[split_index_ps5:]\n",
    "    \n",
    "    print(f\"\\n--- PS5: Temporal Split Details ---\")\n",
    "    print(f\"Training data: {len(X_train_ps5)} samples\")\n",
    "    print(f\"Testing data:  {len(X_test_ps5)} samples\")\n",
    "\n",
    "    # --- 4.2.4: PS5 - Preprocessing and Model Training ---\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features_ps5),\n",
    "            ('cat', categorical_transformer, categorical_features_ps5)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    models_to_train_ps5 = {\n",
    "        'LogisticRegression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "        'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "        'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_grids_ps5 = {\n",
    "        'LogisticRegression': {\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
    "        },\n",
    "        'RandomForestClassifier': {\n",
    "            'classifier__n_estimators': [100, 200, 300], 'classifier__max_depth': [10, 20, 30, None],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'GradientBoostingClassifier': {\n",
    "            'classifier__n_estimators': [100, 200], 'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tscv_ps5 = TimeSeriesSplit(n_splits=5)\n",
    "    trained_models_ps5 = {}\n",
    "    results_summary_ps5 = {}\n",
    "    \n",
    "    print(\"\\n--- PS5: Starting Model Training with Temporal CV ---\")\n",
    "    \n",
    "    for model_name, model in models_to_train_ps5.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, param_distributions=param_grids_ps5[model_name],\n",
    "            n_iter=10, scoring='f1_macro', cv=tscv_ps5, random_state=42, n_jobs=-1, verbose=0\n",
    "        )\n",
    "        search.fit(X_train_ps5, y_train_ps5)\n",
    "        \n",
    "        trained_models_ps5[model_name] = search.best_estimator_\n",
    "        results_summary_ps5[model_name] = {\n",
    "            'best_score_f1_macro': search.best_score_,\n",
    "            'best_params': search.best_params_\n",
    "        }\n",
    "        print(f\"  [RESULT] Best F1-Macro (avg over 5 splits): {search.best_score_:.4f}\")\n",
    "\n",
    "    # --- 4.2.5: PS5 - Evaluation and Model Saving ---\n",
    "    best_model_name_ps5 = max(results_summary_ps5, key=lambda k: results_summary_ps5[k]['best_score_f1_macro'])\n",
    "    best_model_ps5 = trained_models_ps5[best_model_name_ps5]\n",
    "    \n",
    "    print(f\"\\n--- PS5: Evaluating Best Model: {best_model_name_ps5} ---\")\n",
    "    y_pred_ps5 = best_model_ps5.predict(X_test_ps5)\n",
    "    f1_ps5 = f1_score(y_test_ps5, y_pred_ps5, average='macro')\n",
    "    acc_ps5 = accuracy_score(y_test_ps5, y_pred_ps5)\n",
    "    \n",
    "    print(f\"  Test Set Accuracy: {acc_ps5:.4f}\")\n",
    "    print(f\"  Test Set F1-Macro: {f1_ps5:.4f}\")\n",
    "    print(\"  Classification Report:\\n\", classification_report(y_test_ps5, y_pred_ps5, target_names=['Home Win', 'Draw', 'Away Win']))\n",
    "\n",
    "    final_metrics_ps5 = {\n",
    "        'model_name': best_model_name_ps5, 'accuracy': acc_ps5, 'f1_macro': f1_ps5,\n",
    "        'best_cv_params': results_summary_ps5[best_model_name_ps5]['best_params']\n",
    "    }\n",
    "    \n",
    "    joblib.dump(best_model_ps5, MODELS_DIR / MODEL_NAME_PS5)\n",
    "    with open(MODELS_DIR / METADATA_NAME_PS5, 'w') as f:\n",
    "        json.dump(final_metrics_ps5, f, indent=4)\n",
    "        \n",
    "    print(f\"\\nModel saved to: {MODELS_DIR / MODEL_NAME_PS5}\")\n",
    "    print(f\"Metrics saved to: {MODELS_DIR / METADATA_NAME_PS5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec00d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3.1: v1 - Load Data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4c: Regression - Home Goals (fthg) - v3 Engineered Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "DATA_FILE_V1 = DATA_ENGINEERED_DIR / 'data_engineered_match_v3.csv'\n",
    "TARGET_COL_V1 = 'fthg'\n",
    "\n",
    "try:\n",
    "    df_v1 = pd.read_csv(DATA_FILE_V1)\n",
    "    df_v1.columns = df_v1.columns.str.lower().str.strip()\n",
    "    if 'date' in df_v1.columns:\n",
    "        df_v1['date'] = pd.to_datetime(df_v1['date'], errors='coerce')\n",
    "        df_v1 = df_v1.sort_values('date').reset_index(drop=True)\n",
    "    print(f\"v1 data loaded. Shape: {df_v1.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: File not found at {DATA_FILE_V1}. Skipping this section.\")\n",
    "    df_v1 = None\n",
    "\n",
    "if df_v1 is not None:\n",
    "    # --- 4.3.2: v1 - Feature Selection & Scaling ---\n",
    "    exclude_cols_v1 = ['date', 'hometeam', 'awayteam', 'ftr', 'hthg', 'htag', 'fthg', 'ftag', 'unnamed:_0', 'unnamed: 0']\n",
    "    feature_cols_v1 = [col for col in df_v1.columns if col not in exclude_cols_v1 and df_v1[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    X_v1 = df_v1[feature_cols_v1].copy()\n",
    "    y_v1 = df_v1[TARGET_COL_V1].copy()\n",
    "    \n",
    "    # CRITICAL FIX for LightGBM: Clean feature names\n",
    "    X_v1.columns = [col.replace(':', '_').replace('[', '(').replace(']', ')').replace('{', '(').replace('}', ')') for col in X_v1.columns]\n",
    "    feature_cols_v1 = list(X_v1.columns) # Update feature list with cleaned names\n",
    "    \n",
    "    X_v1 = X_v1.fillna(X_v1.mean())\n",
    "    X_v1 = X_v1.replace([np.inf, -np.inf], np.nan).fillna(X_v1.mean())\n",
    "    \n",
    "    scaler_v1 = StandardScaler()\n",
    "    X_scaled_v1 = scaler_v1.fit_transform(X_v1)\n",
    "    X_scaled_v1 = pd.DataFrame(X_scaled_v1, columns=X_v1.columns)\n",
    "    \n",
    "    print(f\"v1 Feature Selection Complete. Features: {len(feature_cols_v1)}\")\n",
    "\n",
    "    # --- 4.3.3: v1 - Temporal Cross-Validation Strategy ---\n",
    "    class TemporalCrossValidator:\n",
    "        def __init__(self, n_splits=5, initial_train_size=0.5):\n",
    "            self.n_splits = n_splits\n",
    "            self.initial_train_size = initial_train_size\n",
    "        \n",
    "        def split(self, X, y=None):\n",
    "            n_samples = len(X)\n",
    "            initial_train = int(n_samples * self.initial_train_size)\n",
    "            test_size = (n_samples - initial_train) // self.n_splits\n",
    "            \n",
    "            for i in range(self.n_splits):\n",
    "                train_end = initial_train + (i * test_size)\n",
    "                test_end = train_end + test_size\n",
    "                train_idx = np.arange(0, train_end)\n",
    "                test_idx = np.arange(train_end, min(test_end, n_samples))\n",
    "                if len(test_idx) > 0:\n",
    "                    yield train_idx, test_idx\n",
    "\n",
    "    tcv_v1 = TemporalCrossValidator(n_splits=5, initial_train_size=0.6)\n",
    "    print(\"v1 TemporalCrossValidator defined.\")\n",
    "\n",
    "    # --- 4.3.4: v1 - Model Training ---\n",
    "    models_v1 = {\n",
    "        'Linear Regression': {'model': LinearRegression(), 'params': {}, 'use_scaled': True},\n",
    "        'Ridge Regression': {'model': Ridge(), 'params': {'alpha': [0.1, 1, 10, 100]}, 'use_scaled': True},\n",
    "        'Random Forest': {'model': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1), 'params': {'max_depth': [10, 20], 'min_samples_split': [5, 10]}, 'use_scaled': False},\n",
    "        'Gradient Boosting': {'model': GradientBoostingRegressor(n_estimators=100, random_state=42), 'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}, 'use_scaled': False}\n",
    "    }\n",
    "    if XGB_AVAILABLE:\n",
    "        models_v1['XGBoost'] = {'model': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1), 'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}, 'use_scaled': False}\n",
    "    if LGB_AVAILABLE:\n",
    "        models_v1['LightGBM'] = {'model': lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbose=-1), 'params': {'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}, 'use_scaled': False}\n",
    "\n",
    "    results_v1 = []\n",
    "    trained_models_v1 = {}\n",
    "    \n",
    "    print(f\"\\n--- v1: Training {len(models_v1)} models with Temporal CV ---\")\n",
    "\n",
    "    for model_name, model_config in models_v1.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        X_train_data = X_scaled_v1 if model_config['use_scaled'] else X_v1\n",
    "        \n",
    "        cv_scores_mae = []\n",
    "        for train_idx, test_idx in tcv_v1.split(X_train_data, y_v1):\n",
    "            X_train, X_test = X_train_data.iloc[train_idx], X_train_data.iloc[test_idx]\n",
    "            y_train, y_test = y_v1.iloc[train_idx], y_v1.iloc[test_idx]\n",
    "            \n",
    "            model = model_config['model'].__class__(**model_config['model'].get_params())\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            cv_scores_mae.append(mean_absolute_error(y_test, y_pred))\n",
    "        \n",
    "        results_v1.append({'Model': model_name, 'MAE': np.mean(cv_scores_mae), 'MAE_std': np.std(cv_scores_mae)})\n",
    "        print(f\"  âœ“ MAE: {np.mean(cv_scores_mae):.4f} (Â±{np.std(cv_scores_mae):.4f})\")\n",
    "        \n",
    "        # Train final model\n",
    "        final_model = model_config['model'].__class__(**model_config['model'].get_params())\n",
    "        final_model.fit(X_train_data, y_v1)\n",
    "        trained_models_v1[model_name] = final_model\n",
    "\n",
    "    # --- 4.3.5: v1 - Results and Summary ---\n",
    "    results_df_v1 = pd.DataFrame(results_v1).sort_values('MAE')\n",
    "    print(\"\\n--- v1: Model Performance Comparison ---\")\n",
    "    print(results_df_v1.to_string(index=False))\n",
    "    \n",
    "    best_model_name_v1 = results_df_v1.iloc[0]['Model']\n",
    "    best_mae_v1 = results_df_v1.iloc[0]['MAE']\n",
    "    \n",
    "    print(f\"\\nðŸ† v1 Best Model: {best_model_name_v1} (MAE: {best_mae_v1:.4f})\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df_v1.to_csv(DATA_ENGINEERED_DIR / 'model_comparison_results.csv', index=False)\n",
    "    \n",
    "    summary_v1 = {\n",
    "        'best_model': best_model_name_v1,\n",
    "        'best_mae': float(best_mae_v1),\n",
    "        'n_features': len(feature_cols_v1),\n",
    "        'n_samples': len(X_v1),\n",
    "        'cv_strategy': 'Walk-Forward (5-fold temporal)',\n",
    "        'all_results': results_df_v1.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    with open(DATA_ENGINEERED_DIR / 'model_training_summary_v1.json', 'w') as f:\n",
    "        json.dump(summary_v1, f, indent=2)\n",
    "    \n",
    "    print(\"âœ“ v1 Model results and summary saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e52ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1: Final Model Performance Comparison ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define paths to all metric files\n",
    "METRICS_PATHS = {\n",
    "    'PS1: League Winner (Class)': MODELS_DIR / 'ps1_league_winner_metadata.json',\n",
    "    'PS2: Match Winner (Class)': MODELS_DIR / 'ps2_match_winner_metadata_v4_ultimate.json',\n",
    "    'PS3: Top Scorer (Reg)': MODELS_DIR / 'ps3_top_scorer_metadata.json',\n",
    "    'PS4: Total Points (Reg)': MODELS_DIR / 'ps4_total_points_metadata.json',\n",
    "    'PS5: Match Result (Class)': MODELS_DIR / 'ps5_match_result_metrics.json',\n",
    "    'v1: Home Goals (Reg)': DATA_ENGINEERED_DIR / 'model_training_summary_v1.json'\n",
    "}\n",
    "\n",
    "# --- 5.2: Metric Extraction ---\n",
    "summary_data = []\n",
    "print(\"Loading metrics from all saved models...\")\n",
    "\n",
    "for name, path in METRICS_PATHS.items():\n",
    "    if not path.exists():\n",
    "        print(f\"  ! Warning: Metric file not found, skipping: {path}\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        score = np.nan\n",
    "        metric_name = \"N/A\"\n",
    "\n",
    "        if 'PS1' in name or 'PS2' in name or 'PS5' in name:\n",
    "            # Classification\n",
    "            metric_name = 'F1-Macro'\n",
    "            if 'performance_metrics' in metrics:\n",
    "                score = metrics['performance_metrics'].get('f1_macro')\n",
    "            else:\n",
    "                score = metrics.get('f1_macro')\n",
    "        \n",
    "        elif 'PS3' in name or 'PS4' in name:\n",
    "            # Regression\n",
    "            metric_name = 'R-Squared'\n",
    "            if 'performance_metrics' in metrics:\n",
    "                score = metrics['performance_metrics'].get('test_r2', metrics.get('best_test_r2'))\n",
    "            else:\n",
    "                 score = metrics.get('best_test_r2')\n",
    "\n",
    "        elif 'v1' in name:\n",
    "            # Regression (MAE was primary)\n",
    "            metric_name = 'MAE'\n",
    "            score = metrics.get('best_mae')\n",
    "\n",
    "        summary_data.append({\n",
    "            'Model Task': name,\n",
    "            'Primary Metric': metric_name,\n",
    "            'Score': score if score is not None else np.nan\n",
    "        })\n",
    "        print(f\"  âœ“ Loaded: {name} (Score: {score:.4f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error loading {name}: {e}\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).dropna().sort_values('Score', ascending=False)\n",
    "\n",
    "# --- 5.3: Visualization ---\n",
    "if not summary_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create the bar plot\n",
    "    sns.barplot(x='Score', y='Model Task', data=summary_df, ax=ax, palette='viridis')\n",
    "    \n",
    "    # Add score labels\n",
    "    for index, row in summary_df.iterrows():\n",
    "        ax.text(row['Score'] + 0.01, index, f\"{row['Score']:.3f} ({row['Primary Metric']})\", \n",
    "                color='black', ha=\"left\", va='center', fontsize=11)\n",
    "    \n",
    "    ax.set_title('Final Model Performance Comparison (Corrected)', fontsize=18, pad=20)\n",
    "    ax.set_xlabel('Performance Score', fontsize=12)\n",
    "    ax.set_ylabel('Problem Statement', fontsize=12)\n",
    "    ax.set_xlim(0, max(1.05, summary_df['Score'].max() * 1.1)) # Adjust x-lim\n",
    "    \n",
    "    output_path = VIZ_DIR / 'final_model_performance_comparison.png'\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"\\n--- Comparison Chart Generated ---\")\n",
    "    print(summary_df)\n",
    "    print(f\"\\nâœ… Chart saved to: {output_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo model metrics were loaded. Skipping summary chart.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… CONSOLIDATED MODEL TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
