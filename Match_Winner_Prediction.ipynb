{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPL Match Winner Prediction\n",
    "\n",
    "This notebook predicts the outcome of English Premier League matches using historical match data.\n",
    "\n",
    "**Prediction Target**: Full Time Result (FTR)\n",
    "- H: Home Win\n",
    "- D: Draw\n",
    "- A: Away Win\n",
    "\n",
    "**Dataset**: Match Winner.csv (6,840 matches with 40 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Libraries imported successfully!\n",
      "\u2713 Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully!\")\n",
    "print(f\"\u2713 Random seed set to {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " LOADING AND EXPLORING DATASET\n",
      "======================================================================\n",
      "\n",
      "\u2713 Dataset loaded successfully!\n",
      "  Shape: 6,840 rows \u00d7 40 columns\n",
      "\n",
      "   Unnamed: 0      Date  HomeTeam       AwayTeam  FTHG  FTAG FTR  HTGS  ATGS  \\\n",
      "0           0  19/08/00  Charlton       Man City     4     0   H     0     0   \n",
      "1           1  19/08/00   Chelsea       West Ham     4     2   H     0     0   \n",
      "2           2  19/08/00  Coventry  Middlesbrough     1     3  NH     0     0   \n",
      "3           3  19/08/00     Derby    Southampton     2     2  NH     0     0   \n",
      "4           4  19/08/00     Leeds        Everton     2     0   H     0     0   \n",
      "\n",
      "   HTGC  ...  HTLossStreak3  HTLossStreak5  ATWinStreak3 ATWinStreak5  \\\n",
      "0     0  ...              0              0             0            0   \n",
      "1     0  ...              0              0             0            0   \n",
      "2     0  ...              0              0             0            0   \n",
      "3     0  ...              0              0             0            0   \n",
      "4     0  ...              0              0             0            0   \n",
      "\n",
      "  ATLossStreak3 ATLossStreak5 HTGD ATGD DiffPts DiffFormPts  \n",
      "0             0             0  0.0  0.0     0.0         0.0  \n",
      "1             0             0  0.0  0.0     0.0         0.0  \n",
      "2             0             0  0.0  0.0     0.0         0.0  \n",
      "3             0             0  0.0  0.0     0.0         0.0  \n",
      "4             0             0  0.0  0.0     0.0         0.0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" LOADING AND EXPLORING DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../Data/Match Winner.csv')\n",
    "\n",
    "print(f\"\\n\u2713 Dataset loaded successfully!\")\n",
    "print(f\"  Shape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "print(f\"\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " DATASET INFORMATION\n",
      "======================================================================\n",
      "\n",
      "Columns (40):\n",
      "   1. Unnamed: 0\n",
      "   2. Date\n",
      "   3. HomeTeam\n",
      "   4. AwayTeam\n",
      "   5. FTHG\n",
      "   6. FTAG\n",
      "   7. FTR\n",
      "   8. HTGS\n",
      "   9. ATGS\n",
      "  10. HTGC\n",
      "  11. ATGC\n",
      "  12. HTP\n",
      "  13. ATP\n",
      "  14. HM1\n",
      "  15. HM2\n",
      "  16. HM3\n",
      "  17. HM4\n",
      "  18. HM5\n",
      "  19. AM1\n",
      "  20. AM2\n",
      "  21. AM3\n",
      "  22. AM4\n",
      "  23. AM5\n",
      "  24. MW\n",
      "  25. HTFormPtsStr\n",
      "  26. ATFormPtsStr\n",
      "  27. HTFormPts\n",
      "  28. ATFormPts\n",
      "  29. HTWinStreak3\n",
      "  30. HTWinStreak5\n",
      "  31. HTLossStreak3\n",
      "  32. HTLossStreak5\n",
      "  33. ATWinStreak3\n",
      "  34. ATWinStreak5\n",
      "  35. ATLossStreak3\n",
      "  36. ATLossStreak5\n",
      "  37. HTGD\n",
      "  38. ATGD\n",
      "  39. DiffPts\n",
      "  40. DiffFormPts\n",
      "\n",
      "\n",
      "Data Types:\n",
      "int64      17\n",
      "object     16\n",
      "float64     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Missing Values:\n",
      "0 total missing values\n"
     ]
    }
   ],
   "source": [
    "# Dataset info\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" DATASET INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n\\nData Types:\\n{df.dtypes.value_counts()}\")\n",
    "print(f\"\\n\\nMissing Values:\\n{df.isnull().sum().sum()} total missing values\")\n",
    "\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(f\"\\nColumns with missing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " STATISTICAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Numerical Features Summary:\n",
      "                count         mean          std       min          25%  \\\n",
      "Unnamed: 0     6840.0  3419.500000  1974.682253  0.000000  1709.750000   \n",
      "FTHG           6840.0     1.527485     1.297913  0.000000     1.000000   \n",
      "FTAG           6840.0     1.130263     1.124566  0.000000     0.000000   \n",
      "HTGS           6840.0    24.416667    17.178524  0.000000    11.000000   \n",
      "ATGS           6840.0    24.514327    17.136894  0.000000    11.000000   \n",
      "HTGC           6840.0    24.497807    16.401571  0.000000    11.000000   \n",
      "ATGC           6840.0    24.347515    16.341557  0.000000    11.000000   \n",
      "HTP            6840.0     1.209014     0.530186  0.000000     0.888889   \n",
      "ATP            6840.0     1.226768     0.523176  0.000000     0.906250   \n",
      "MW             6840.0    19.500000    10.966658  1.000000    10.000000   \n",
      "HTFormPts      6840.0     6.242690     3.582486  0.000000     4.000000   \n",
      "ATFormPts      6840.0     6.413596     3.549762  0.000000     4.000000   \n",
      "HTWinStreak3   6840.0     0.062281     0.241682  0.000000     0.000000   \n",
      "HTWinStreak5   6840.0     0.017982     0.132897  0.000000     0.000000   \n",
      "HTLossStreak3  6840.0     0.057602     0.233007  0.000000     0.000000   \n",
      "HTLossStreak5  6840.0     0.014327     0.118846  0.000000     0.000000   \n",
      "ATWinStreak3   6840.0     0.062865     0.242739  0.000000     0.000000   \n",
      "ATWinStreak5   6840.0     0.016520     0.127475  0.000000     0.000000   \n",
      "ATLossStreak3  6840.0     0.051023     0.220062  0.000000     0.000000   \n",
      "ATLossStreak5  6840.0     0.010234     0.100651  0.000000     0.000000   \n",
      "HTGD           6840.0    -0.009690     0.693593 -3.000000    -0.500000   \n",
      "ATGD           6840.0     0.014539     0.692937 -3.333333    -0.464286   \n",
      "DiffPts        6840.0    -0.017755     0.668523 -2.363636    -0.451613   \n",
      "DiffFormPts    6840.0    -0.018086     0.408488 -2.250000    -0.176471   \n",
      "\n",
      "                       50%          75%          max  \n",
      "Unnamed: 0     3419.500000  5129.250000  6839.000000  \n",
      "FTHG              1.000000     2.000000     9.000000  \n",
      "FTAG              1.000000     2.000000     7.000000  \n",
      "HTGS             23.000000    35.000000   102.000000  \n",
      "ATGS             23.000000    35.000000   105.000000  \n",
      "HTGC             23.000000    36.000000    85.000000  \n",
      "ATGC             23.000000    36.000000    82.000000  \n",
      "HTP               1.172414     1.555556     2.736842  \n",
      "ATP               1.192308     1.562500     2.761905  \n",
      "MW               19.500000    29.000000    38.000000  \n",
      "HTFormPts         6.000000     9.000000    15.000000  \n",
      "ATFormPts         6.000000     9.000000    15.000000  \n",
      "HTWinStreak3      0.000000     0.000000     1.000000  \n",
      "HTWinStreak5      0.000000     0.000000     1.000000  \n",
      "HTLossStreak3     0.000000     0.000000     1.000000  \n",
      "HTLossStreak5     0.000000     0.000000     1.000000  \n",
      "ATWinStreak3      0.000000     0.000000     1.000000  \n",
      "ATWinStreak5      0.000000     0.000000     1.000000  \n",
      "ATLossStreak3     0.000000     0.000000     1.000000  \n",
      "ATLossStreak5     0.000000     0.000000     1.000000  \n",
      "HTGD             -0.076923     0.400000     4.000000  \n",
      "ATGD             -0.050000     0.421053     3.500000  \n",
      "DiffPts           0.000000     0.428571     2.285714  \n",
      "DiffFormPts       0.000000     0.153846     2.250000  \n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nNumerical Features Summary:\")\n",
    "print(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" DATA CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    print(\"\\n\u2713 Removed 'Unnamed: 0' column\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n\u2713 Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"  Removed {duplicates} duplicate rows\")\n",
    "\n",
    "# Fill any missing values with 0 (assuming missing means no activity)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Reconstruct FTR from goals to ensure H, D, A classes (if needed)\n",
    "if 'FTHG' in df.columns and 'FTAG' in df.columns:\n",
    "    conditions = [\n",
    "        (df['FTHG'] > df['FTAG']),\n",
    "        (df['FTHG'] < df['FTAG'])\n",
    "    ]\n",
    "    choices = ['H', 'A']\n",
    "    df['FTR'] = np.select(conditions, choices, default='D')\n",
    "    print(f\"\\n\u2713 Reconstructed FTR target variable to ensure 3 classes (Home, Draw, Away)\")\n",
    "print(f\"\\n\u2713 Missing values handled\")\n",
    "\n",
    "print(f\"\\n\u2713 Clean dataset shape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target distribution\n",
    "target_counts = df['FTR'].value_counts()\n",
    "target_pct = df['FTR'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMatch Results Distribution:\")\n",
    "for result_code in target_counts.index:\n",
    "    label_map = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win', 'NH': 'Not Home'}\n",
    "    label = label_map.get(result_code, result_code)\n",
    "    count = target_counts[result_code]\n",
    "    pct = target_pct[result_code]\n",
    "    print(f\"  {label} ({result_code}): {count:,} matches ({pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "# Create dynamic labels based on actual results in data\n",
    "result_labels = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win', 'NH': 'Not Home'}\n",
    "plot_labels = [result_labels.get(result, result) for result in target_counts.index]\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=colors[:len(target_counts)], alpha=0.8)\n",
    "axes[0].set_title('Match Results Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Result', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Matches', fontsize=12)\n",
    "axes[0].set_xticklabels(plot_labels, rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=plot_labels, \n",
    "            autopct='%1.1f%%', colors=colors[:len(target_counts)], startangle=90)\n",
    "axes[1].set_title('Match Results Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print home advantage summary - safe access\n",
    "if 'H' in target_pct.index and 'A' in target_pct.index:\n",
    "    print(f\"\\n\u2713 Home advantage observed: {target_pct['H']:.1f}% home wins vs {target_pct['A']:.1f}% away wins\")\n",
    "elif 'H' in target_pct.index and 'NH' in target_pct.index:\n",
    "    print(f\"\\n\u2713 Home advantage observed: {target_pct['H']:.1f}% home wins vs {target_pct['NH']:.1f}% not home\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Distribution: {', '.join([f'{k}={v:.1f}%' for k, v in target_pct.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" FEATURE TYPES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}):\")\n",
    "print(f\"  {', '.join(numerical_cols[:15])}...\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key feature statistics by match result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" KEY METRICS BY MATCH RESULT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "key_features = ['FTHG', 'FTAG', 'HTGS', 'ATGS', 'HTGC', 'ATGC', 'HTP', 'ATP', 'DiffPts', 'DiffFormPts']\n",
    "available_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "print(\"\\nAverage statistics by match outcome:\")\n",
    "print(df.groupby('FTR')[available_features].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by outcome\n",
    "features_to_plot = ['FTHG', 'FTAG', 'DiffPts', 'DiffFormPts']\n",
    "available_plot_features = [f for f in features_to_plot if f in df.columns]\n",
    "\n",
    "if len(available_plot_features) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(available_plot_features[:4]):\n",
    "        if idx < 4:\n",
    "            sns.boxplot(data=df, x='FTR', y=feature, ax=axes[idx], palette=colors)\n",
    "            axes[idx].set_title(f'{feature} by Match Result', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel('Match Result', fontsize=10)\n",
    "            axes[idx].set_ylabel(feature, fontsize=10)\n",
    "            result_labels_box = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win'}\n",
    "            box_labels = [result_labels_box.get(r, r) for r in df['FTR'].unique()]\n",
    "            axes[idx].set_xticklabels(box_labels)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for key numerical features\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select important numerical features for correlation\n",
    "corr_features = ['FTHG', 'FTAG', 'HTGS', 'ATGS', 'HTGC', 'ATGC', 'HTP', 'ATP', \n",
    "                 'HTFormPts', 'ATFormPts', 'DiffPts', 'DiffFormPts']\n",
    "corr_features = [f for f in corr_features if f in df.columns]\n",
    "\n",
    "correlation_matrix = df[corr_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Correlation analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Team Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home team performance\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" TEAM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'HomeTeam' in df.columns:\n",
    "    home_wins = df[df['FTR'] == 'H'].groupby('HomeTeam').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Home Winners:\")\n",
    "    print(home_wins.head(10))\n",
    "    \n",
    "    # Visualize top teams\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    home_wins.head(15).plot(kind='barh', color='#2ecc71', alpha=0.8)\n",
    "    plt.title('Top 15 Teams by Home Wins', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Home Wins', fontsize=12)\n",
    "    plt.ylabel('Team', fontsize=12)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_model = df.copy()\n",
    "\n",
    "# Encode team names (if present)\n",
    "label_encoders = {}\n",
    "if 'HomeTeam' in df_model.columns:\n",
    "    le_home = LabelEncoder()\n",
    "    df_model['HomeTeam_Encoded'] = le_home.fit_transform(df_model['HomeTeam'])\n",
    "    label_encoders['HomeTeam'] = le_home\n",
    "    print(\"\\n\u2713 Encoded HomeTeam\")\n",
    "\n",
    "if 'AwayTeam' in df_model.columns:\n",
    "    le_away = LabelEncoder()\n",
    "    df_model['AwayTeam_Encoded'] = le_away.fit_transform(df_model['AwayTeam'])\n",
    "    label_encoders['AwayTeam'] = le_away\n",
    "    print(\"\u2713 Encoded AwayTeam\")\n",
    "\n",
    "# Create additional features\n",
    "if 'HTGS' in df_model.columns and 'ATGS' in df_model.columns:\n",
    "    df_model['GoalScoredDiff'] = df_model['HTGS'] - df_model['ATGS']\n",
    "    print(\"\u2713 Created GoalScoredDiff feature\")\n",
    "\n",
    "if 'HTGC' in df_model.columns and 'ATGC' in df_model.columns:\n",
    "    df_model['GoalConcededDiff'] = df_model['ATGC'] - df_model['HTGC']\n",
    "    print(\"\u2713 Created GoalConcededDiff feature\")\n",
    "\n",
    "if 'HTFormPts' in df_model.columns and 'ATFormPts' in df_model.columns:\n",
    "    df_model['FormDiff'] = df_model['HTFormPts'] - df_model['ATFormPts']\n",
    "    print(\"\u2713 Created FormDiff feature\")\n",
    "\n",
    "# Create interaction features\n",
    "if 'HTP' in df_model.columns and 'HTWinStreak3' in df_model.columns:\n",
    "    df_model['HomeStrength'] = df_model['HTP'] * (1 + df_model['HTWinStreak3'] * 0.1)\n",
    "    print(\"\u2713 Created HomeStrength feature\")\n",
    "\n",
    "if 'ATP' in df_model.columns and 'ATWinStreak3' in df_model.columns:\n",
    "    df_model['AwayStrength'] = df_model['ATP'] * (1 + df_model['ATWinStreak3'] * 0.1)\n",
    "    print(\"\u2713 Created AwayStrength feature\")\n",
    "\n",
    "print(f\"\\n\u2713 Feature engineering complete\")\n",
    "print(f\"  New dataset shape: {df_model.shape[0]:,} rows \u00d7 {df_model.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define feature columns (excluding target and non-predictive columns)\n",
    "exclude_cols = ['FTR', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG']\n",
    "feature_cols = [col for col in df_model.columns if col not in exclude_cols]\n",
    "\n",
    "# Ensure all feature columns are numerical\n",
    "feature_cols = [col for col in feature_cols if df_model[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"\\nSelected {len(feature_cols)} features for modeling:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = df_model[feature_cols].copy()\n",
    "y = df_model['FTR'].copy()\n",
    "\n",
    "print(f\"\\n\u2713 Feature matrix X shape: {X.shape}\")\n",
    "print(f\"\u2713 Target vector y shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Data split completed (80% train, 20% test)\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  y_train distribution:\\n{y_train.value_counts()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" FEATURE SCALING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n\u2713 Features scaled using StandardScaler\")\n",
    "print(f\"  Training data scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Test data scaled shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, multi_class='multinomial')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "print(f\"\\n\u2713 Model trained successfully\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Training: {lr_train_acc:.4f} ({lr_train_acc*100:.2f}%)\")\n",
    "print(f\"  Test:     {lr_test_acc:.4f} ({lr_test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "# Create dynamic target names based on actual classes\n",
    "unique_classes = sorted(y_test.unique())\n",
    "class_label_map = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win'}\n",
    "dynamic_target_names = [class_label_map.get(cls, cls) for cls in unique_classes]\n",
    "print(classification_report(y_test, lr_test_pred, \n",
    "                          target_names=dynamic_target_names))\n",
    "\n",
    "# Cross-validation\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_train_acc = accuracy_score(y_train, rf_train_pred)\n",
    "rf_test_acc = accuracy_score(y_test, rf_test_pred)\n",
    "\n",
    "print(f\"\\n\u2713 Model trained successfully\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Training: {rf_train_acc:.4f} ({rf_train_acc*100:.2f}%)\")\n",
    "print(f\"  Test:     {rf_test_acc:.4f} ({rf_test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "# Create dynamic target names based on actual classes\n",
    "unique_classes = sorted(y_test.unique())\n",
    "class_label_map = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win'}\n",
    "dynamic_target_names = [class_label_map.get(cls, cls) for cls in unique_classes]\n",
    "print(classification_report(y_test, rf_test_pred, \n",
    "                          target_names=dynamic_target_names))\n",
    "\n",
    "# Cross-validation\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='#3498db', alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 20 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" MODEL 3: GRADIENT BOOSTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "gb_train_pred = gb_model.predict(X_train)\n",
    "gb_test_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "gb_train_acc = accuracy_score(y_train, gb_train_pred)\n",
    "gb_test_acc = accuracy_score(y_test, gb_test_pred)\n",
    "\n",
    "print(f\"\\n\u2713 Model trained successfully\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Training: {gb_train_acc:.4f} ({gb_train_acc*100:.2f}%)\")\n",
    "print(f\"  Test:     {gb_test_acc:.4f} ({gb_test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "# Create dynamic target names based on actual classes\n",
    "unique_classes = sorted(y_test.unique())\n",
    "class_label_map = {'H': 'Home Win', 'D': 'Draw', 'A': 'Away Win'}\n",
    "dynamic_target_names = [class_label_map.get(cls, cls) for cls in unique_classes]\n",
    "print(classification_report(y_test, gb_test_pred, \n",
    "                          target_names=dynamic_target_names))\n",
    "\n",
    "# Cross-validation\n",
    "gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {gb_cv_scores.mean():.4f} (+/- {gb_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 XGBoost Classifier (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgb_available:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" MODEL 4: XGBOOST\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Encode target for XGBoost\n",
    "    le_target = LabelEncoder()\n",
    "    y_train_encoded = le_target.fit_transform(y_train)\n",
    "    y_test_encoded = le_target.transform(y_test)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "    \n",
    "    # Predictions\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    xgb_test_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Decode predictions\n",
    "    xgb_train_pred_labels = le_target.inverse_transform(xgb_train_pred)\n",
    "    xgb_test_pred_labels = le_target.inverse_transform(xgb_test_pred)\n",
    "    \n",
    "    # Evaluate\n",
    "    xgb_train_acc = accuracy_score(y_train, xgb_train_pred_labels)\n",
    "    xgb_test_acc = accuracy_score(y_test, xgb_test_pred_labels)\n",
    "    \n",
    "    print(f\"\\n\u2713 Model trained successfully\")\n",
    "    print(f\"\\nAccuracy:\")\n",
    "    print(f\"  Training: {xgb_train_acc:.4f} ({xgb_train_acc*100:.2f}%)\")\n",
    "    print(f\"  Test:     {xgb_test_acc:.4f} ({xgb_test_acc*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nClassification Report (Test Set):\")\n",
    "    \n",
    "    # Robust check for target names\n",
    "    unique_classes = sorted(y_test.unique())\n",
    "    if len(unique_classes) == 3:\n",
    "        # Expected case: 3 classes\n",
    "        print(classification_report(y_test, xgb_test_pred_labels, \n",
    "                                  target_names=['Away Win', 'Draw', 'Home Win']))\n",
    "    else:\n",
    "        # Fallback case: Mismatch detected (likely stale data)\n",
    "        print(f\"\\n\u26a0 WARNING: Found {len(unique_classes)} classes instead of 3.\")\n",
    "        print(\"  Please restart the kernel and run all cells to apply the data fix.\")\n",
    "        print(\"  Using dynamic target names for now to prevent crash:\\n\")\n",
    "        print(classification_report(y_test, xgb_test_pred_labels))\n",
    "else:\n",
    "    print(\"\\n\u26a0 XGBoost not available. Skipping XGBoost model.\")\n",
    "    xgb_test_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison dataframe\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost'],\n",
    "    'Test Accuracy': [lr_test_acc, rf_test_acc, gb_test_acc, xgb_test_acc if xgb_available else 0],\n",
    "    'CV Score': [lr_cv_scores.mean(), rf_cv_scores.mean(), gb_cv_scores.mean(), 0]\n",
    "}).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "if not xgb_available:\n",
    "    models_comparison = models_comparison[models_comparison['Model'] != 'XGBoost']\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(models_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(models_comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, models_comparison['Test Accuracy'], width, label='Test Accuracy', color='#3498db', alpha=0.8)\n",
    "ax.bar(x + width/2, models_comparison['CV Score'], width, label='CV Score', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_comparison['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model_name = models_comparison.iloc[0]['Model']\n",
    "best_accuracy = models_comparison.iloc[0]['Test Accuracy']\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {best_model_name} with {best_accuracy*100:.2f}% test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" CONFUSION MATRIX - RANDOM FOREST (Best Performer)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, rf_test_pred, labels=['A', 'D', 'H'])\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Away Win', 'Draw', 'Home Win'],\n",
    "            yticklabels=['Away Win', 'Draw', 'Home Win'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "for i, label in enumerate(['Away Win', 'Draw', 'Home Win']):\n",
    "    class_acc = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"  {label}: {class_acc*100:.2f}% ({cm[i, i]}/{cm[i, :].sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get prediction probabilities on test set\n",
    "rf_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Create a sample prediction dataframe\n",
    "sample_predictions = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': rf_test_pred[:10],\n",
    "    'Prob_Away': rf_proba[:10, 0],\n",
    "    'Prob_Draw': rf_proba[:10, 1],\n",
    "    'Prob_Home': rf_proba[:10, 2]\n",
    "})\n",
    "\n",
    "sample_predictions['Correct'] = sample_predictions['Actual'] == sample_predictions['Predicted']\n",
    "\n",
    "print(\"\\nSample Predictions (First 10 test matches):\")\n",
    "print(sample_predictions.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\u2713 Predictions generated successfully!\")\n",
    "print(f\"  Correct predictions in sample: {sample_predictions['Correct'].sum()}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Dataset Overview:\")\n",
    "print(f\"  - Total matches: {len(df):,}\")\n",
    "print(f\"  - Features used: {len(feature_cols)}\")\n",
    "print(f\"  - Training samples: {len(X_train):,}\")\n",
    "print(f\"  - Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Target Distribution:\")\n",
    "print(f\"  - Home Wins: {(y=='H').sum()/len(y)*100:.1f}%\")\n",
    "print(f\"  - Draws: {(y=='D').sum()/len(y)*100:.1f}%\")\n",
    "print(f\"  - Away Wins: {(y=='A').sum()/len(y)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\ud83c\udfc6 Best Model Performance:\")\n",
    "print(f\"  - Model: {best_model_name}\")\n",
    "print(f\"  - Test Accuracy: {best_accuracy*100:.2f}%\")\n",
    "print(f\"  - Improvement over baseline (random): {(best_accuracy - 0.33)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insights:\")\n",
    "if 'DiffPts' in feature_importance.head(5)['feature'].values:\n",
    "    print(\"  - Point difference is a strong predictor of match outcome\")\n",
    "if 'DiffFormPts' in feature_importance.head(5)['feature'].values:\n",
    "    print(\"  - Recent form significantly impacts match results\")\n",
    "print(f\"  - Home advantage exists: {(y_test=='H').sum()/len(y_test)*100:.1f}% home wins in test set\")\n",
    "print(\"  - Tree-based models outperform traditional logistic regression\")\n",
    "\n",
    "print(\"\\n\u2705 Notebook execution completed successfully!\")\n",
    "print(f\"\\nThis model can now be used to predict EPL match outcomes with {best_accuracy*100:.1f}% accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}